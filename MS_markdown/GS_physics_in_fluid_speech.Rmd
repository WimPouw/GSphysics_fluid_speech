---
title             : "Gesture-Speech Physics in Fluent Speech and Rhythmic Upper Limb Movements"
shorttitle        : "Gesture-Speech Physics in Fluent Speech"

author: 
  - name          : "Wim Pouw"
    affiliation   : "1,2,3"
    corresponding : yes    # Define only one corresponding author
    address       : "Donders Institute for Brain, Cognition and Behaviour, Heyendaalseweg 135, 6525 AJ Nijmegen"
    email         : "w.pouw@psych.ru.nl"
  - name          : "Lisette de Jonge-Hoekstra"
    affiliation   : "1,4"
  - name          : "Steven J. Harrison"
    affiliation   : "1"
  - name          : "Alexandra Paxton"
    affiliation   : "1,5"
  - name          : "James A. Dixon"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Center for the Ecological Study of Perception and Action, University of Connecticut"
  - id            : "2"
    institution   : "Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen"
  - id            : "3"
    institution   : "Institute for Psycholinguistics, Max Planck Nijmegen"
  - id            : "4"
    institution   : "University of Groningen"
  - id            : "5"
    institution   : "Department of Psychological Sciences, University of Connecticut"


authornote: |
  All anonymised data and analysis code are available at the Open Science Framework (https://osf.io/tgbmw/). This manuscript has been written with Rmarkdown - for the code-embedded reproducible version of this manuscript please see the Rmarkdown (.Rmd) file available at the OSF page.
  This research has been funded by The Netherlands Organisation of Scientific Research (NWO; Rubicon grant “Acting on Enacted Kinematics”, Grant Nr. 446-16-012; PI Wim Pouw).  
  Some sections of this paper has been submitted (in verbatim) as 2-page abstract to GESPIN2020.
  Acknowledgement: We would like to thank Jenny Michlich for pointing us to relevant bioacoustic literature. We thank Susanne Fuchs for valuable comments on this work.

abstract: |
  Communicative hand gestures are often coordinated with prosodic aspects of speech. Therein, salient moments of gestural movement (e.g., quick changes in speed) often co-occur with salient moments in speech (e.g., near peaks in fundamental frequency and intensity). A common understanding is that such gesture and speech coordination is culturally and cognitively acquired rather than having a biological basis. Recently, however, the physical coupling of arm movements to speech movements via myofascial tissue biomechanics has been investigated as a potentially important factor in understanding the emergence of gesture-speech coordination. Specifically, in the case of steady-state vocalization and mono-syllable utterances, it has been shown that forces produced during gesturing are transfered onto the tensioned body leading to changes in respiratory-related activity and thereby affecting vocalization F0 and intensity during steady-state vocalization and mono-syllable utterances. In the current experiment (N = 34), we show that gesture-speech physics is relevant for fluent speech, too. Compared with non-movement, participants who are producing fluent self-formulated speech while rhythmically moving their limbs demonstrate heightened F0 and amplitude envelope, and such effects are more pronounced for higher-impulse arm versus lower-impulse wrist movement. We replicate that acoustic peaks arise especially during moments of peak-impulse (i.e., the beat) of the movement, namely around deceleration phases of the movement. Finally, higher deceleration rates of higher-mass arm movements were related to higher peaks in acoustics, supporting a role for physical impulse transmissions of gesture onto the tensioned body, affecting speech. These results further support a radically embodied account of the origins of human gesture.

  
keywords          : "hand gesture, speech production, speech acoustics, biomechanics, entrainment"
wordcount         : "X"

bibliography      : ["mybib.bib", "r-references.bib"]

fig_caption       : no
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man, noextraspace"
output            : papaja::apa6_docx


---

```{r setup, include = FALSE}
library("papaja") #papaja::apa6_pdf
knitr::opts_chunk$set(fig.cap = "")
knitr::opts_chunk$set(dpi=600)
```


```{r analysis-preferences_packages_functions_etc, warning = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
#load libraries
library(dplyr)   #data formatting
library(ggplot2) #plotting 3d density plots
library(ggbeeswarm) #plotting of density jitter distributions
library(gridExtra)  #plotting mulitple pannels
library(nlme)       #mixed regression
library(ggplot2)    #plotting 3d density plots
library(gam)        #generalizized additive models
library(mgcv)       #plotting generalized additive models
library(itsadug)    #plotting generalized additive models
library(scales)     #for rescaling variables
```
```{r functions_themes, echo = FALSE, message = FALSE}
#save blue theme for plotting later on
bluetheme <- theme(
  panel.background = element_rect(fill = "#BFD5E3", colour = "#6D9EC1",
                                size = 2, linetype = "solid"),
  panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                colour = "white"), 
  panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                colour = "white"),
  strip.background =element_rect(fill="#BFD5E3"))
```
```{r load_in_dataetc, echo = FALSE, message=FALSE, warning=FALSE,cache = TRUE}
#set directories, load main data, and order factors
parentfolder <- "D:/Research_Projects/Respiration, Gesture, Speech/MainStudy_fluidspeech/MS_markdown" 
basefolder <- dirname(parentfolder)
meta <- read.csv(paste0(basefolder, "/DATA/META/META.csv"))
fd <- read.csv(paste0(basefolder, "/DATA/Main/MERGED_DATA/fd.csv"))
fd$condition <- factor(fd$condition, levels = c("PASSIVE", "WRIST", "ARM"))
cartoon_duration_s <- c(50, 33, 39, 75, 23, 38, 77, 21, 89, 120, 100, 48) #seconds for each video clip


#save some general info task
  #####average time per trial
ch <- ave(fd$time_ms_rec, fd$unique_trial,FUN=function(x){max(x)-min(x)} )
ch <- ch[!duplicated(fd$unique_trial)]
trialtime <- mean(ch, na.rm=TRUE)/1000
sdtrialtime <- sd(ch, na.rm=TRUE)/1000
```


  Communicative hand gestures are ubiquitous across human cultures. Gestures aid communication by seamlessly interweaving relevant pragmatic, iconic and symbolic expressions of the hands together with speech [@hollerMultimodalLanguageProcessing2019; @streeckDepictingGesture2008; @feyereisenCognitivePsychologySpeechRelated2017]. For such multi-articulatory utterances to do their communicative work, gesture and speech must be tightly temporally coupled to form a sensible speech-gesture whole. And in fact, gestures' salient moments are often timed with emphatic stress made in speech, no matter what the hands depict [@wagnerGestureSpeechInteraction2014; @shattuck-hufnagelDimensionalizingCospeechGestures2019]. For such exquisite gesture-speech coordination to get off the ground, the system must functionally constrain its degrees of freedom [@turveyCoordination1990], and in doing so it will have to utilize (or otherwise account for) intrinsic dynamics arising from the bio-physics of speaking and moving at the same time.  
  In this report we provide evidence that movement of the upper limbs constrain fluent self-generated speech acoustics through biomechanics. This replicates and extends results obtained with rudimentary steady-state vocalization and mono-syllabic utterances showing that physical impulses generated by upper limb movements reach the respiratory system, affecting vocalization acoustics [@pouwGesturespeechPhysicsBiomechanical2019; @pouwEnergyFlowsGesturespeech2019a; @pouwAcousticInformationUpper2020a]. We thereby show that there is a further complexity to the human voice in that it is directly constrained by gestural movement. 
  
## The gesture-speech prosody link
  The tight coordination of prosodic aspects of speech with the kinematics of gesture has been long appreciated and is classically referred to as the beat-like quality of co-speech gesture [@mcneillHandMindWhat1992]. This phenomenon has been studied in several ways. Human coders have been trained to identify salient moments called the apex in gestures, together with prosodically meaningful moments in fluent speech. That research tradition has found that gesture apices often align with *pitch accents* in speech—accents which are acoustically predominately defined by positive excursions in the fundamental frequency (F0), lowering of the second formant, longer vowel duration, and increased intensity [@loehrTemporalStructuralPragmatic2012; @mendoza-dentonSemioticLayeringGesture2011; @mcclavePitchManualGestures1998]. Pitch accents can be perceptually differentiated by sudden lowering of F0 as well, but gestures do not seem to align with those events quite as much [@imProbabilisticRelationCospeech2020].  
  In more recent motion-tracking studies, these gesture-speech prosody correlations have been obtained as well, showing—for example—that gestures’ peak velocity often co-occurs near peaks in F0, even when such gestures are depicting something [@dannerQuantitativeAnalysisMultimodal2018; @pouwEntrainmentModulationGesture2019; @pouwQuantifyingGesturespeechSynchrony2019; @leonardTemporalRelationBeat2011; @krivokapicGesturalCoordinationProsodic2014]. In pointing gestures, stressed syllables align neatly with the maximum extension of the pointing movement, such that the hand movement terminates at the first syllable utterance in strong-weak stressed “PApa” and terminates later during the second syllable utterance in the weak-strong “paPA”  [@esteve-gibertProsodicStructureShapes2013; @rochet-capellanSpeechFocusPosition2008]. During finger-tapping and mono-syllabic utterances, when participants are instructed to alternate prominence in their utterances (“pa, PA, pa, PA”), the tapping action spontaneously aligns with the syllable pattern, such that larger movements are made during stressed syllables  [@parrellSpatiotemporalCouplingSpeech2014]. Conversely, if participants are instructed to alternate stress in finger tapping (STRONG-weak-STRONG-weak force production), speech will follow, with larger oral-labial apertures for stressed vs. unstressed tapping movements.   
  Even when people do not intend to change the stress patterning of an uttered sentence, gesturing concurrently affects speech acoustics in a way that makes it seem intentionally stressed, inducing an increase in vocalization duration and a lowering of the second formant of co-occurrent speech [@krahmerEffectsVisualBeats2007]. Further, gesture and speech cycle rates seem to be attracted towards particular (polyrhythmic) stabilities: In-phase speech-tapping is preferred over anti-phase coordination, and 2:1 speech-to-tapping ratios are preferred over more complex integer ratios such as 5:2 [@stoltmannSyllablepointingGestureCoordination2017;@zelicArticulatoryConstraintsSpontaneous2015; @kelsoConvergingEvidenceSupport1984; @treffnerIntentionalAttentionalDynamics2002]. All these previous results indicate that gesture and speech naturally couple their activity, raising questions about origins of its pervasiveness.  
  
## Gesture-speech physics 
  The current mainstream understanding of the gesture-prosody link is that is not "biologically mandated" [p.69 ; @mcclavePitchManualGestures1998] but culturally acquired, and requires neural-cognitive timing mechanisms [@ruiterProductionGestureSpeech2000] that appear only after 16 months of age [@iversonHandMouthBrain2005].   
  Recently, however, the physical coupling of arm movements with speech via myofascial tissue biomechanics has been investigated, where it was found that hand gesturing physically impacts steady-state vocalizations and mono-syllabic consonant-vowel utterances [@pouwGesturespeechPhysicsBiomechanical2019; @pouwEnergyFlowsGesturespeech2019a; @pouwAcousticInformationUpper2020a; @pouwAcousticSpecificationUpper2019]. Specifically, hand and arm movements can transfer a force (a physical impulse) onto the musco-skeletal system, thereby modulating respiration-related muscle activity, leading to changes in vocalization's intensity. If gesture-induced impulses are not accommodated for by vocal fold adjustments, the Fundamental Frequency (F0) of vocalizations is affected as well. It has been found that higher-impulse arm- versus wrist movements, or dual- versus one handed movements, will induce more pronounced effects on F0 and intensity. This is because the mass of the "object" in motion is higher for arm(s) versus wrist movements thereby changing the momentum of the effector, everything else—such as effector speed—being equal (as momentum effector = effctor mass x effector velocity). The change in momentum is the physical impulse, and the moment where physical impulse is highest, is when the change in velocity (i.e., acceleration) is highest (everything else—such as effector mass—being constant).  
    The way in which physical impulses are absorbed by the respiratory system is likely complex and not a simple linear function [@levinTensegrityNewBiomechanics2006]. However, a complete understanding will involve an appreciation of the body as a pre-stressed system [@bernsteinCoordinationRegulationsMovements1967; @profetaBernsteinLevelsMovement2018], forming an interconnected tensioned network of compressive (e,g., bones) and tensile elements (e.g., fascia, muscles) through which forces may reverberate nonlinearly [@turveyMediumHapticPerception2014; @silvaSteadystateStressOne2007]. Specifically, the upper limb movements are controlled by stabilizing musculo-skeletal actions of the scapula and shoulder joint, which directly implicate accessory expiratory muscles that also stabilize scapula and shoulder joint actions [e.g., the serratus anterior inferior; see  @pouwEnergyFlowsGesturespeech2019a for an overview].  
    Peripheral actions also play a role, as performing an upper limb movement recruits a whole kinetic chain of muscle activity around the trunk (e.g., rector abdominus) to maintain posture [@hodgesFeedforwardContractionTransversus1997]. Indeed, when people are standing vs. sitting for example, it has been found that the effects of peak physical impulse of gestures onto vocalization acoustics is more pronounced [@pouwGesturespeechPhysicsBiomechanical2019]. We reasoned that this is because standing involves more forceful anticipatory postural counter adjustments [@cordoPropertiesPosturalAdjustments1982] which reach the respiratory system via accessory expiratory muscles also implicated in keeping postural integrity. Recently, more direct evidence has been found for the gesture-respiratory-speech link: Respiratory-related activity (measured with a respiratory belt) was enhanced during moments of peak-impetus of gesture as opposed to other phases in the gesture movement, and respiratory-related activity itself was related to gesture-related intensity modulations of mono-syllable utterances [@pouwEnergyFlowsGesturespeech2019a].    
  The evidence reviewed so far has been based on experiments on continuous vocalizations or monosyllabic utterances. Such results cannot directly generalize to fluent, self-generated, full-sentenced speech, although promising indications suggest that gesture-speech physics does generalize to fluent speech. A recent study found that encouraging participants to gesture during cartoon-narration versus giving no instructions lead to 22Hz increase in observation of max F0 and to greater F0 ranges of speech and intensity [@cravottaEffectsEncouragingUse2019]. Furthermore, computational modelers have reported on interesting successes of synthesizing gesture kinematics based on speech acoustics alone [@ginosarLearningIndividualStyles2019; @kucherenkoAnalyzingInputOutput2019], indicating that information about body movements inhabits the speech signal. Although promising, such results do not necessitate a role for biomechanics but only suggests a strong connection between gesture and speech.  

## Current experiment
  The current experiment was conducted as a simple test of the constraints of upper limb movement on fluent speech speech acoustics. Participants were asked to retell a cartoon scene that they had just watched, while either not moving, vertically moving their wrist, or vertically moving their arm at a tempo of 80 beats per minute (1.33Hz). Participants were asked to give a stress or beat in the downward motion with a sudden stop at maximum extension (i.e., sudden deceleration). Participants were asked to not allow movements to affect their speaking performance in any way. Similar to previous experiments [e.g., @pouwEnergyFlowsGesturespeech2019], we assessed the following to conclude that gesture-speech physics is present:
* 1) Does rhythmic co-speech movement change acoustic markers of prosody (i.e., F0 and amplitude envelope)?  
* 2) At what moments of co-speech movement is change in acoustics observed?
* 3) Does degree of physical impulse (as measured by effector mass or changes in speed) predict acoustic variation?

\pagebreak 
# Method

## Participants & Design
  We tested a total of 37 participants `r printnum(length(meta$PPN))` participants (*M* age = `r printnum(mean(meta$AGE))`, *SD* age = `r printnum(sd(meta$AGE))`, %cis-gender female = `r printnum( round(sum(meta$GENDER == "FEMALE")/length(meta$PPN)*100, 2) )`, %cis-gender male = `r printnum( round(sum(meta$GENDER == "MALE")/length(meta$PPN)*100, 2) )`, %right-handed = `r printnum( round(sum(meta$Handedness == "R")/length(meta$PPN)*100, 2) )`), drawn from an undergraduate participant pool at the University of Connecticut. There were `r printnum(sum(meta$N_NATIVE == "Asian"))` non-native English speaking Asian-undergraduates in the current sample.  
  The current design was fully-within subject, with a three-level movement manipulation (passive Vs. wrist-movement Vs. arm-movement condition). Participants performed `r printnum(length(unique(fd$unique_trial)))` trials in total lasting about 40 seconds with Movement condition randomly assigned per trial. The study design was approved by the IRB committee of the University of Connecticut (#H18-227).

## Material & Equipment

### Cartoon vignettes   
  Twelve cartoon vignettes were created from the "Canary Row" and "Snow Business" Tweety and Sylvester cartoons, *M* duration vignettes = `r printnum(mean(cartoon_duration_s))`seconds (*SD* = `r printnum(sd(cartoon_duration_s))`). These cartoons are often used in gesture research [@mcneillHandMindWhat1992]. The videos can be accessed here: https://osf.io/rfj5x/.

### Audio and Motion Tracking   
  A MicroMic C520 cardioid condenser microphone headset (AKG, Inc.) was used to record audio. A Polhemus Liberty motion tracking system (Polhemus, Inc.) was used to record the position of the participant’s index finger of the dominant hand, sampling with one 6D sensor at 240 Hz. We applied a first-order Butterworth filter at 30 Hz for the vertical position (z) traces and its derivatives. 
## Procedure  
  Upon arrival, participants were briefed that this 30-minute experiment entailed retelling cartoon scenes while standing and performing upper-limb movements. A motion sensor was attached to the tip of the index finger of the dominant hand, and a microphone headset was put on. Participants were asked to stand upright and were introduced to three movement conditions (see Figure 1). In the passive condition, participants did not move and kept their arm resting alongside the body. In the wrist-Movement condition, participants were asked to continuously vertically move the hand at the wrist joint while keeping the elbow joint at 90 degrees. In the arm-Movement Condition, participants moved their arm at the elbow joint, without wrist movement. Similar to previous studies [e.g., @pouwAcousticInformationUpper2020a]], participants were asked to give emphasis in the downward motion of the movement with a sudden halt—in other words, a beat—at the maximum extension of their movement.  
  After introduction of the movements, participants were told that they were to move at a particular tempo, indicated by visual feedback system. The feedback system consisted of a horizontal bar that continually updated to the report on the participant’s movement speed in previous movement cycle. The participant was to keep the horizontal bar between two low and higher boundaries (a 20% region, [72-88] BPM) of the 1.33-Hz target tempo (i.e., 80 BPM). Participants briefly practiced moving at the target rate before starting the experiment. Critically, the participants were not exposed to an external rhythmic signal, like a visual metronome. Subsequently, participants were instructed that they would watch cartoon clips that they would then retell while making one of the instructed movements (or making no movements). Participants were asked to keep their speech as normal as possible while making the movements (or no movement). In the conditions requiring movement, participants were to keep their movement tempo within the target range.   
  Subsequently, participants were instructed that they would watch cartoon clips which they would retell after having watched it, while at the same time making one of the instructed movements (or making no movements). Participants were asked to keep their speech as normal as possible while making the movements (or no movement). When moving while speaking, participants were to keep their movement tempo within the target range.  
  Twelve cartoon vignettes were readied to be shown before each trial, but if the total experiment time exceeded 30 minutes, the experiment would be terminated without all scenes being retold. To ensure that all Movement conditions would be performed at least once within that time, we set the maximum time per trial at 1 minute. In other words, when participants were still retelling the same scene after 60, seconds the experimenter would terminate the trial and move to the next trial. Mean retelling time was, however, well below 1 minute *M* = `r printnum(round(trialtime))` seconds, *SD* = `r printnum(round(sdtrialtime, 2))`.

Figure 1. Graphical overview of movement conditions
```{r method_stance_pic, echo = FALSE,warning=FALSE, fig.align = 'center', fig.height= 4}
library(raster)  
mypng <- stack(paste0(parentfolder, "/images/FigureStanceMethod.png"))  
plotRGB(mypng,maxpixels=1e300)  

```
*Note*. Movement conditions are shown. Each participant performed all conditions (i.e., within-subjects). To ensure that movement tempo remained relatively constant participants were shown a moving green bar, which indicated whether they moved too fast or too slow relative to a 20% target region of 1.33Hz. Participants were instructed to have an emphasis in the downbeat with an abrupt stop (i.e., beat) at the maximum extension.

## Preprocessing  
### Speech acoustics
The fundamental frequency was extracted with sex-appropriate preset ranges (male = 50-400Hz; female = 80-640Hz). We used a previously written  [@pouwMaterialsTutorialGespin20192019] R script (https://osf.io/m43qy/) utilizing the R package ‘wrassp’ [@winkelmannWrasspInterfaceASSP2018] which applies a K. Schaefer-Vincent algorithm. We also extracted a smoothed (5-Hz Hann window) amplitude envelope using a previously written custom-written R script (https://osf.io/uvkj6/, which reimplements in R a procedure from  @heAmplitudeEnvelopeKinematics2017a.  

### Data and Exclusions
  We collected `r printnum( round( nrow(fd)*(1/240)/60,2) )` minutes of continuous data (passive condition = `r printnum( round( nrow(fd[fd$condition == "PASSIVE",])*(1/240)/60,2) )`,  wrist-movement condition = `r printnum( round( nrow(fd[fd$condition == "WRIST",])*(1/240)/60,2) )`, arm-movement = `r printnum( round( nrow(fd[fd$condition == "ARM",])*(1/240)/60,2) )`). However, due to a c++ memory allocation error the precise timing data for the motion-tracking was lost for a smaller portion of the data.  Thus the final dataset consists of `r printnum( round( nrow(fd[is.na(fd$EXCLUDE),])*(1/240)/60,2) )` minutes of continuous speech and movement data (passive = `r printnum( round( nrow(fd[is.na(fd$EXCLUDE) & fd$condition == "PASSIVE",])*(1/240)/60,2) )`, wrist-movement condition =  `r printnum( round( nrow(fd[is.na(fd$EXCLUDE) & fd$condition == "WRIST",])*(1/240)/60,2) )`, arm-movement condition = `r printnum( round( nrow(fd[is.na(fd$EXCLUDE) & fd$condition == "ARM",])*(1/240)/60,2) )`).
  
```{r exclude_data, echo=FALSE,warning = FALSE, message = FALSE, output = FALSE, results = 'hide'}
fd <- subset(fd, is.na(EXCLUDE)) #exclude data where we have motion tracking timing loss
```

## Manipulation Checks and Baseline
  For gesture-speech analysis we also created a surrogate condition using speech of a passive condition trial and randomly pairing this with motion-tracking data from a movement condition for that participant (without scrambling the order). This surrogate randomly paired condition allowed us to exclude the possibility that any effects of movement were due to chance coupling inherent to the structure of speech and movement. We only use this surrogate control condition as a contrast when we are performing analysis on the temporal relation between speech and movement.  
   We computed the following measures to check whether our movement manipulation was successful and whether speech rates were comparable across conditions. Figure 2 shows a summary of the results for key manipulation check measures.  
   
### Movement Frequency  
  To ascertain whether participants moved their limbs within the target 1.33-Hz range, we performed a wavelet-based analysis [using R-package 'WaveletComp'; @roschWaveletCompGuidedTour2014]. Figure 2 shows that wrist movements were performed at slightly faster rates *M* = `r printnum( mean( fd$dom_hz_mov[is.na(fd$EXCLUDE) & fd$condition == "WRIST"], na.rm = TRUE))` Hz, *SD* = `r printnum( sd( fd$dom_hz_mov[is.na(fd$EXCLUDE) & fd$condition == "WRIST"], na.rm = TRUE))`, than arm movements, *M* = `r printnum( mean( fd$dom_hz_mov[is.na(fd$EXCLUDE) & fd$condition == "ARM"], na.rm = TRUE))` Hz, *SD* = `r printnum( sd( fd$dom_hz_mov[is.na(fd$EXCLUDE) & fd$condition == "ARM"], na.rm = TRUE))`, but in both cases the movements were distributed over the range 1.33Hz. This confirms that our movement manipulation was successful. For our surrogate control condition, the mean frequency of the artificially paired movement time series fell between both arm- and wrist-movement condition frequency distributions, *M* = `r printnum( mean( fd$dom_hz_mov[is.na(fd$EXCLUDE) & fd$condition == "PASSIVE"], na.rm = TRUE))` Hz, *SD* = `r printnum( sd( fd$dom_hz_mov[is.na(fd$EXCLUDE) & fd$condition == "PASSIVE"], na.rm = TRUE))`. 

```{r speech info, echo=FALSE, message=FALSE, warning=FALSE, results='hide', output = FALSE}
#get vocalization cycles
time_p <- NA    #initizalize a temporary variable
time_p <- ave(fd$ENV, fd$unique_vocalization, FUN= function(x) max(x, na.rm = TRUE))#extract highest amplitude observation observed during a vocalization
time_p <- ifelse(is.na(fd$unique_vocalization), NA, time_p) #if there is NA vocalization ENVELOPE max shoudl be ignroed
fd$time_peak <- ifelse(time_p!=fd$ENV, NA, fd$time_ms_rec)  #insetead of the amplitude fill in the time of that max amplitude
fd$time_peak[!is.na(fd$time_peak)] <- ave(fd$time_peak[!is.na(fd$time_peak)], 
                                          fd$unique_trial[!is.na(fd$time_peak)], 
                                          FUN = function(x) c(0, diff(x)) ) #now for each trial get the difference of these timings
fd$time_peak <- ifelse(fd$time_peak == 0, NA, fd$time_peak) #only consider differences that are nonzero (this ignores the first                                                                 difference observations)
fd$time_peak <- ifelse(is.infinite(fd$time_peak), NA, fd$time_peak)             #ignore infinites that are produced for our missing timing data

fd$time_peak <- 1000/fd$time_peak #get occurent Hz by dividing it by 1 seconds (i.e., 1000 milliseconds)
#get average vocalization duration
time_p <- NA    #re-initizalize a temporary variable
fd$time_voc <- ave(fd$time_ms_rec, fd$unique_vocalization, FUN= function(x) max(x, na.rm = TRUE)-min(x, na.rm = TRUE)) #get for each unique vocalization its begin and end time, substract and be left with time of the vocalization
fd$time_voc <- ifelse(is.na(fd$unique_vocalization), NA, fd$time_voc) #only keep vocalization time for when vocalization != NA
fd$time_voc <- ifelse(is.infinite(fd$time_voc), NA, fd$time_voc)             #ignore infinites that are produced for our missing timing data
fd$time_voc <- 1000/fd$time_voc                                       #make variable into Hz
fd$time_voc <- ifelse(is.infinite(fd$time_voc), NA, fd$time_voc)             #ignore infinites that are produced for our missing timing data

rm(time_p) #remove temporary variable
```
### Speech Rate  
  We calculated two measures to provide an indication of speech rate (see Fig. 2 for examples), namely vocalization duration and vocalization interval. Figure 3 shows relatively uniform distributions for speech measures. This shows no clear 1:1 frequency couplings of movement and vocalization duration or vocalization interval nor any other clear signs of poly-rhythmic coupling of movement and speech as has been observed in basic tapping paradigms  [@zelicArticulatoryConstraintsSpontaneous2015]. Thus, we restrict ourselves for the current report to speech vocalization acoustics, instead of focusing on possible temporal changes of speech rate produced under rhythmic movement [ e.g., @stoltmannSyllablepointingGestureCoordination2017].  
     To compare vocalization rates to movement, we computed the average vocalization duration for each trial by tracking the time of uninterrupted runs of F0 observations and then converting the time in milliseconds to Hz. For the passive condition, the average vocalization duration was *M* = `r printnum( mean( fd$time_voc[fd$condition == "PASSIVE"], na.rm = TRUE))` Hz, *SD* = `r printnum( sd( fd$time_voc[fd$condition == "PASSIVE"], na.rm = TRUE))`. For the wrist-movement condition the vocalization duration was *M* = `r printnum( mean( fd$time_voc[fd$condition == "WRIST"], na.rm = TRUE))` Hz, *SD* = `r printnum( sd( fd$time_voc[fd$condition == "WRIST"], na.rm = TRUE))`, and for the arm-movement condition this was *M* = `r printnum( mean( fd$time_voc[fd$condition == "ARM"], na.rm = TRUE))` Hz, *SD* = `r printnum( sd( fd$time_voc[fd$condition == "ARM"], na.rm = TRUE))`.  
  The average vocalization interval for the passive condition was *M* = `r printnum( mean( fd$time_peak[fd$condition == "PASSIVE"], na.rm = TRUE))` Hz, *SD* = `r printnum( sd( fd$time_peak[fd$condition == "PASSIVE"], na.rm = TRUE))`. For the wrist-movement condition the vocalization interval was *M* = `r printnum( mean( fd$time_peak[fd$condition == "WRIST"], na.rm = TRUE))` Hz, *SD* = `r printnum( sd( fd$time_peak[fd$condition == "WRIST"], na.rm = TRUE))`, and for the arm-movement condition *M* = `r printnum( mean( fd$time_peak[fd$condition == "ARM"], na.rm = TRUE))` Hz, *SD* = `r printnum( sd( fd$time_peak[fd$condition == "ARM"], na.rm = TRUE))`.

```{r example_time_series_code, echo=FALSE,warning = FALSE, message = FALSE, output = FALSE, results = 'hide'}
#make an example time series with acoustic and motion data
sample <- fd[fd$ppn == "31" & fd$trial == 4,]                 #pick a trial
sample$time <- sample$time_ms_rec-min(sample$time_ms_rec)     #start the trial at 0 time 
sample <- sample[sample$time > 15000 & sample$time < 22000,]  #collect this bit of the data as corresponding to the waveform
sample$F0 <- ifelse(sample$F0 == 0, NA, sample$F0)            #for plotting F0;s should be given NA instead of 0's when vocalization is absent      
sample$time <- sample$time-min(sample$time)                   #start this sample of the trial at 0 time
  
#make plots, combine them with grid.arrange (which was later exported for some extra editing)
a <- ggplot(sample, aes(x= time)) + geom_line(aes(y = ENV), color = "purple", size= 1.3) + geom_line(aes(y = rescale(z_mov, c(0.8, 1.3)) )) + bluetheme + ylim( -0.3, 1.35)+ theme(axis.text.y = element_text(face = "bold", color="purple"))+ theme(axis.title.x=element_blank())
b <- ggplot(sample, aes(x= time)) + geom_line(aes(y = F0), color = "red", size = 0.8)+ geom_line(aes(y = rescale(z_mov, c(160, 220)))) + bluetheme + ylim(50, 250) + theme(axis.text.y = element_text(face = "bold", color="red"))+ theme(axis.title.x=element_blank())
c <- ggplot(sample, aes(x= time)) + geom_line(aes(y = dom_hz_mov), color = "cyan3", size = 1.3)+ geom_line(aes(y = rescale(z_mov, c(1.2, 1.4)))) + bluetheme + ylab("wavelet estimate frequency (Hz) ") + theme(axis.text.y = element_text(face = "bold", color="cyan3"))+ xlab("time in milliseconds")
#grid.arrange(a,b,c, nrow =3) # this is the figure that we further edited and is called next
```

Figure 2. Example movement-, amplitude envelope-, F0- time series, and time-dependent movement frequency estimates
```{r plot_example_time_series, fig.height= 5}
#load in the finally edited time series example
mypng <- stack(paste0(parentfolder, "/images/FigureTimeSeriesExample.png"))  
plotRGB(mypng,maxpixels=1e500000)
```

*Note figure 2.* A sample of about 10 seconds is shown. With the participant’s permission the speech sample is available at https://osf.io/2qbc6/. The smoothed amplitude envelope in purple traces the waveform maxima. The F0 traces show the concomitant vocalizations in Hz, with an example of vocalization interval and vocalization duration (which was calculated for all vocalizations). The bottom panel shows the continuously estimated movement frequency in cyan, which hovers around 1.33 Hz. In all these panels, the co-occurring movement is plotted in arbitrary units (a.u.) to show the temporal relation of movement phases and the amplitude envelope, F0, and the movement frequency estimate. In our analysis, we refer to the maximum extension and deceleration phases as relevant moments for speech modulations. In this example, a particularly dramatic acoustic excursion occurs during a moment of deceleration of the arm movement, possibly an example of gesture-speech physics.

Figure 3. Summary of movement-frequency, vocalization duration and vocalization interval  
```{r manipulation_checkplot, echo=FALSE, message=FALSE, warning=FALSE, results='hide', cache = TRUE,fig.width=5, figure.heigth = 8}
#get average vocalization cycle
dd <- fd
dd$condition2 <- ifelse(dd$condition == "PASSIVE", "PASSIVE (Random Pairing)", 
                        as.character(dd$condition)) #rename the passive condition to False Pair for assessing vocal-motor coupling
dd$condition2 <- ordered(dd$condition2, levels = c("PASSIVE (Random Pairing)", "WRIST", "ARM")) #reorder the levels
dd$condition <- ordered(dd$condition, levels = c("PASSIVE", "WRIST", "ARM")) #reorder the levels

#plot relevant summary data for the vocalization cycles (time_peak), and vocalizaiton duration (time_voc)
a <- ggplot(dd, aes(x = dom_hz_mov)) + geom_density()+bluetheme + facet_grid(.~condition2)+ xlim(0.1,3)+ geom_vline(xintercept = 1.33, color = "red", size= 0.4) + xlab("movement frequency (Hz)")+ theme(strip.text.x = element_text(size = 7))
b <- ggplot(dd, aes(x = time_peak)) + geom_density()+bluetheme + facet_grid(.~condition)+ geom_vline(xintercept = 1.33, color = "red", size= 0.4) +xlim(0.1, 6) + xlab("vocalization interval (Hz)")+ theme(strip.text.x = element_text(size = 7))
c <- ggplot(dd[!duplicated(dd$unique_vocalization),], aes(x = time_voc)) + geom_density()+bluetheme + facet_grid(.~condition)+ geom_vline(xintercept = 1.33, color = "red", size= 0.4) +xlim(0.1, 6) + xlab("vocalization duration (Hz)") + theme(strip.text.x = element_text(size = 7))

#grid.arrange(a,b,c,nrow = 3)
#load in the image
mypng <- stack(paste0(parentfolder, "/images/Figure3.png"))  
plotRGB(mypng,maxpixels=1e500000)
```

*Note Figure 3*. Density distributions of movement frequencies, vocalization interval, and vocalization duration are shown. Note, that for the passive condition there was no movement, but we have in this case Random Pairing movement time series for the passive (Random Pairing) condition for which frequency information is shown. The red vertical line indicates the target movement frequency at 1.3Hz.

# Results

## Overview analysis  
  We report three main analyses to show that gesture-speech physics is present in fluent speech. Firstly, we assess overall effects of movement condition on vocalization acoustics (F0 and the amplitude envelope); these would support our hypothesis that (especially high impulse) upper limb movement constrains fluent speech acoustics. Secondly, we assess whether vocalization acoustic modulations are observed at particular phases of the movement cycle, which gesture-speech physics holds should occur at moments of peaks in deceleration. Thirdly, we assess whether a continuous estimate of upper limb physical impulse through deceleration rate predicts vocalization acoustic peaks, which would support the gesture-speech physics hypothesis that physical impulses are transferred onto the vocalization system.
  The following generally applies to all analyses. For hypothesis testing, we performed mixed linear regression models [using R-package ‘nlme’; @pinheiroNlmeLinearNonlinear2019], and non-linear generalized additive modeling or GAM [using R-package 'gam'; @hastieGamGeneralizedAdditive2019] with random intercept for participants by default.
  
## Acousic correlates of movement condition    
  Figure 4 shows the average F0 and amplitude envelope (z-scaled for participants) per trial per condition. The passive condition had generally lower levels of F0 and Amplitude Envelope as compared to the arm- and wrist-Movement conditions. Furthermore, the higher-impulse arm-movement condition generally had higher levels of F0 and Amplitude envelope as compared to lower-impulse wrist-movement condition.    
  Table 1 shows the results of mixed linear regression analysis. For the amplitude envelope, the passive condition had a lower average amplitude envelope as compared to the the wrist-movement condition (*p* < .005), as well as the arm-movement condition (*p* < .0001). We further obtain that after accounting for differences in F0 for gender (males had generally 73Hz lower F0), wrist-movement condition has about 1.6 Hz increase in average as compared to the passive condition, but this was not statistically reliable (*p* = .059). Further, the arm-movement condition increased in F0 by 3.5 Hz over the passive Condition (*p* < .001).  
  
Figure 4. Average F0 and Amplitude Envelope (ENV) per trial per condition

```{r plot_avF0_avENV, echo=FALSE, message = FALSE, warning = FALSE, cache = TRUE, fig.width=5}
#average acoustics plots
    #average F0 per trial
fd$av_f0 <- ave(fd$F0z, fd$unique_trial, FUN = function(x) mean(x, na.rm = TRUE)) #get the average F0
  #plot F0
a <- ggplot(fd[!duplicated(fd$av_f0),], aes(x = condition, y = av_f0)) + geom_violin(color = "red", alpha = 0.6) + 
  geom_boxplot(alpha = 0.2) +  geom_beeswarm(priority='density',cex=1.0, size= 0.4) + ylab("average F0 per trial (z-scaled per participant)") + xlab("condition") + ggtitle("Vocalization F0") + bluetheme

#average Amplitude vocalization per trial
  #note here that we take the amplitude during a vocalization (i.e., when F0 is observed) thereby ignoring voiceless consonents
fd$av_ENV[!is.na(fd$F0z)] <- ave(fd$ENVz[!is.na(fd$F0z)], fd$unique_trial[!is.na(fd$F0z)], FUN = function(x) mean(x, na.rm = TRUE))
  #plot ENV
b <- ggplot(fd[!duplicated(fd$av_ENV),], aes(x = condition, y = av_ENV)) + geom_violin(color= "purple", alpha = 0.6) + 
  geom_boxplot(alpha = 0.2) +  geom_beeswarm(priority='density',cex=1.0, size= 0.4) + ylab("average ENV per trial (z-scaled per participant)") + xlab("condition") + ggtitle("Vocalization ENV") + bluetheme

#grid.arrange(b,a, nrow=1)

#load in the image
mypng <- stack(paste0(parentfolder, "/images/Figure4.png"))  
plotRGB(mypng,maxpixels=1e500000)
```

*Note Figure 4*. Violin and box plots are shown for average F0 (Hz) and amplitude envelope (z-scaled) per trial (jitters show observation).

```{r model_differences_absolute, output= FALSE, message = FALSE, warning = FALSE, results='hide'}
#compute statistics for unscaled F0 within participants

#effect condition F0
dd <- fd              #convert to data to a temporary dataset for this analysis
dd$F0 <- ifelse(dd$F0 == 0, NA, dd$F0) #make NA when F0 is not observed (i.e., 0) thereby non-vocal. not affecting our analysis
fd$av_f0 <- ave(dd$F0, dd$unique_trial, FUN = function(x) mean(x, na.rm = TRUE)) #get the mean F0 per trial
lmdat <- fd[!duplicated(fd$unique_trial),]  #only keep one unique trial, thereby also keeping one mean F0 observation per trial     #run models                                
model1f <- lme(av_f0~gender+condition, data = lmdat, random = ~1|ppn, method = "ML", na.action = na.exclude)
    #random slopes for condition did not converge
#model1fran <- lme(av_f0~gender+condition, data = lmdat, random = ~condition|ppn, method = "ML", na.action = na.exclude) 
cmod1f <- coef(summary(model1f)) #collect summary information of the f model

#effect condition ENV
dd <- fd            #convert to data to a temporary dataset for this analysis
fd$av_ENV[!is.na(dd$F0z)] <- ave(fd$ENVz[!is.na(dd$F0z)], fd$unique_trial[!is.na(dd$F0z)], FUN = function(x) mean(x, na.rm = TRUE))
lmdat <- dd[!duplicated(dd$av_ENV),]
model1e <- lme(av_ENV~condition, data = lmdat, random = ~1|ppn, method = "ML", na.action = na.exclude)
    #random slopes for condition did not converge
  #model1eran <- lme(av_ENV~gender+condition, data = lmdat, random = ~condition|ppn, method = "ML", na.action = na.exclude)
cmod1e <- coef(summary(model1e)) #collect summary information of the e model

```
\pagebreak
Table 1. Linear mixed effects for effects of condition on F0 and amplitude envelope (ENV)

```{r tablesmean1, warning = FALSE, echo = FALSE,fig.width=6}
#make tables
tm <- cbind(contrast =  c("intercept", "Wrist vs. Passive", "Arm vs. Passive"," ",
                   "intercept", "Male vs. Female", "Wrist vs. Passive", "Arm vs. Passive"), b = c(round(cmod1e[,"Value"],3),"",round(cmod1f[,"Value"],3)), 
                                                    SE = c( round(cmod1e[,"Std.Error"],3),"", round(cmod1f[,"Std.Error"],3)),
                                                    df = c(round(cmod1e[,"DF"],0)," ", round(cmod1f[,"DF"],0)),
                                                    p = c(round(cmod1e[,"p-value"],4),"", round(cmod1f[,"p-value"],4)))
rownames(tm) <- c("ENV (z-scaled)", "", ""," ",
                   "F0 (Hz)", "", "", "")
tm[,5] <- ifelse(tm[,5] == 0, "< .0001", tm[,5])
# at most 4 decimal places
knitr::kable(tm, digits = 3, align = "c", booktabs = T)
```


## Coupling of vocalization duration and movement  
  Having ascertained in the previous analysis that there is some kind of acoustic modulation for movement versus no movement, we further need to confirm that such modulations occur at particular moments in the movement cycle. Figure 5 show the main results for all data, where we model over time the acoustic patterning in vocalizations around the maximum extension of the movement cycle, for all movement cycles that occurred. If there are particular moments in the movement cycle where vocalization is affected, for example the moment when the hand starts decelerating (estimated from the data as shown in Figure 5), we would expect acoustic modulations (peaks) at such moments of the movement cycle.   
  Just before the moment of maximum extension, the observed amplitude envelope shows a clear peak, most dramatically for the arm-movement condition, but also present for the wrist-movement condition. For randomly paired movement and passive condition speech, this was not the case; this provides strong evidence that the results observed in the arm- and wrist-movement conditions are not due to mere chance. For F0, the pattern is somewhat less clear, but positive peaks still occur just before the maximum extension. These findings replicate our earlier work on steady-state vocalization and mono-syllabic utterances, showing that moments of peak deceleration also show peaks in acoustics [@pouwEnergyFlowsGesturespeech2019; @pouwGesturespeechPhysicsBiomechanical2019].  
  To test whether trajectories are indeed non-linear and are reliably different from the passive condition, we performed GAM, a type of non-linear mixed effects procedure. GAM is a popular time-series analysis in phonetics, and allows the automatic modeling of more (and less) complex non-linear patterns by combining a set of smooth basis functions. Furthermore, GAM allows for testing whether those non-linear trajectories are modulated depending on some grouping of the data [see e.g., @wielingAnalyzingDynamicPhonetic2018a]. We assessed with GAM the trajectory of acoustics around 800 milliseconds of the maximum extension of the movement. We chose 800 milliseconds (-400, 400) as this is about the time for a 1.33Hz cycle (1000/1.33Hz = 752 ms) with an added margin of error of about 50ms. The model results with random slopes and intercept for participant are shown in table 2.  
  Firstly, for all models tests for non-linearity of the trajectories were statistically reliable (*p*'s < .0001), meaning that there are peaks or valleys in acoustics over the movement cycle rather than a flat linear trend (Figure 6). As shown in Table 2, our results replicate the general finding that the Wrist Movements condition led to reliably different non-linear peaks in acoustics as compared to the passive condition (*p* < .001). Moreover, this effect—relative to the passive condition—is even more extreme for the Arm Movement condition (*p* < .001). Figure 6 provides the fitted trajectories for the GAM models.  
  For readers interested in individual differences in trajectories, we have created interactive graphs for each participant’s average amplitude envelope trajectories (https://osf.io/a423h/) and F0 trajectories (https://osf.io/fdzwj/).  
\pagebreak
Figure 5. Average observed vocalization acoustics relative to the moment of maximum extension  
```{r movementplot_avF0_avENV, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE,fig.height=8}
dd <- fd[is.na(fd$EXCLUDE),]  #make a temporary dataset for this analysis
dd$dist <- -(dd$filled_z_min-dd$time_ms_rec)      #get the time relative to the maximum extension (i.e., z_min)
dd$ENVz <- ifelse(is.na(dd$F0z), NA, dd$ENVz)     #only keep amplitude reading when there is a vocalization
#The next repeated procedure averages for each observation (F0z, ENVz etc.), what the average reading was for distance from maximum extension, for each condition and each participant seperately
dd$averageF0 <- ave(dd$F0z, dd$dist, dd$condition, dd$ppn, FUN = function(x) mean(x, na.rm = TRUE))   
dd$averageENV <- ave(dd$ENVz, dd$dist, dd$condition, dd$ppn, FUN = function(x) mean(x, na.rm = TRUE)) 
dd$acc <- ave(dd$z_mov, dd$unique_trial, FUN = function(x) -1*scale(c(0,0,diff(diff(x)))))
dd$averagedecc <- scale(ave(dd$acc, dd$dist, dd$condition, dd$ppn, FUN = function(x) mean(x, na.rm = TRUE)) )
dd$averagv <-     scale(ave(dd$v, dd$dist, dd$condition, dd$ppn, FUN = function(x) mean(x, na.rm = TRUE))) 
dd$averagez <-    scale(ave(dd$z_mov, dd$dist, dd$condition, dd$ppn, FUN = function(x) mean(x, na.rm = TRUE)))

dn <- dd[!duplicated(paste0(dd$condition, dd$dist, dd$ppn)),] #keep only unique averaged trajectories, by only keeping one row per paticipant, distance from z_min and ppn

dn1 <- dn #make another temporary dataset for plotting results
dn1$condition <- ifelse(dn1$condition == "PASSIVE", "PASSIVE (Random Pairing)", as.character(dn1$condition))
dn1$condition <- factor(dn1$condition, levels = c("PASSIVE (Random Pairing)", "WRIST", "ARM"))

#plot the average Envelope per distance
a <- ggplot(dn1, aes(x = dist, y = averageENV)) + geom_smooth(color = "purple") +
  facet_grid(.~condition) + xlim(-400, 400) + theme_bw() + ggtitle("Vocalization Amplitude") + ylab("time from maximum extension")+
  geom_vline(xintercept = 0, linetype = "dashed") + bluetheme + theme(axis.text.x = element_text(face="bold", 
                            angle=45))+ geom_vline(xintercept =-100, color= "blue", linetype = "dashed")+ labs(x=NULL) +ylab("z-scaled ENV")

#plot the average F0 per distance
b <- ggplot(dn1, aes(x = dist, y = averageF0)) + geom_smooth(color = "red") +
  facet_grid(.~condition) + xlim(-400, 400) + theme_bw() + ggtitle("Vocalization F0") + ylab("time from maximum extension")+
  geom_vline(xintercept = 0, linetype = "dashed") + bluetheme+ theme(axis.text.x = element_text(face="bold", 
                            angle=45))+ geom_vline(xintercept =-100, color= "blue", linetype = "dashed") + labs(x=NULL)+ylab("z-scaled F0")

#plot the average acceleration per distance
c <- ggplot(dn[dn$condition != "PASSIVE",], aes(x = dist)) + 
  geom_smooth(aes(y = averagedecc), color = "blue", size = 2) +
   geom_smooth(aes(y = averagez), color = "black", size = 2) +
  facet_grid(.~condition) + xlim(-400, 400) + theme_bw() + ggtitle("Vertical position and accelaration") + ylab("time from maximum extension")+  geom_vline(xintercept = 0, linetype = "dashed") + bluetheme + theme(axis.text.x = element_text(face="bold", 
                            angle=45)) + geom_vline(xintercept =-100, color= "blue", linetype = "dashed")+
  xlab("time relative to maximum extension")+ylab("movement (acceleration)")

#grid.arrange(a,b,c)
  #load in the image
mypng <- stack(paste0(parentfolder, "/images/Figure5.png"))  
plotRGB(mypng,maxpixels=1e500000)


#these plots were generated to provide information about invidividual variation and were put online on the OSF
extra <- ggplot(dn1, aes(x = dist, y = averageENV, color = as.factor(ppn))) + geom_smooth(size = 0.3, alpha= 0.3) +
  facet_grid(.~condition) + xlim(-400, 400) + theme_bw() + ggtitle("Vocalization Amplitude") + ylab("time from maximum extension")+
  geom_vline(xintercept = 0, linetype = "dashed") + bluetheme + theme(axis.text.x = element_text(face="bold", 
                            angle=45))+ geom_vline(xintercept =-100, color= "blue", linetype = "dashed")+ labs(x=NULL) +ylab("z-scaled ENV")+ ggtitle("individual variation of amplitude envelope trajectories around maximum extension")

extra2 <- ggplot(dn1, aes(x = dist, y = averageF0, color = as.factor(ppn))) + geom_smooth(size = 0.3, alpha= 0.3) +
  facet_grid(.~condition) + xlim(-400, 400) + theme_bw() + ggtitle("Vocalization Amplitude") + ylab("time from maximum extension")+
  geom_vline(xintercept = 0, linetype = "dashed") + bluetheme + theme(axis.text.x = element_text(face="bold", 
                            angle=45))+ geom_vline(xintercept =-100, color= "blue", linetype = "dashed")+ labs(x=NULL) +ylab("z-scaled ENV")+ ggtitle("individual variation of F0 trajectories around maximum extension")


```

*Note Figure 5*. For the upper two panels the average acoustic trajectory is shown around the moment of maximum extension (t = 0, dashed black line). In the lower panel, we have plotted the z-scaled average vertical displacement of the hand and the z-scaled acceleration trace. The blue dashed vertical line marks the moment where the deceleration phase starts, which aligns with peaks in acoustics. 

Figure 6. Fitted trajectories GAM
```{r table_anddiff, echo=FALSE, message=FALSE, warning=FALSE, results = 'hide'}
#model differences with respect to temporal distance from z_min by using generalized additive modeling (GAM)

#Keep only data that are roughly within the 1.3Hz cycle, otherwise will give bad estimates as there are too little data points at higher intervals
CC <- dn[abs(dn$dist) < 400,] #make a temporary subdataset where the absolute temporal distance from max extension is 400 ms

    #PERFORM GAM
#m1 <- bam(averageENV~ condition + s(dist, by=as.factor(condition)) + s(ppn,bs="re"),data=CC) #we used random slopes as it converged
m1r <- bam(averageENV~ condition + s(dist, by=as.factor(condition)) + s(ppn,condition, bs="re") + s(ppn, condition, bs = "re"),data=CC)
mod1 <- summary(m1r) #collect GAM data for average Envelope

#m2 <- bam(averageF0~ gender+condition + s(dist, by=as.factor(condition)) + s(ppn,bs="re"),data=CC) #we used random slopes as it converged
m2r <- bam(averageF0~ gender+condition + s(dist, by=as.factor(condition)) + s(ppn,condition, bs="re") + s(ppn, condition, bs = "re"),data=CC)
mod2 <- summary(m2r) #collect GAM data for average F0

#plot the fitted values per condition
#par(mfrow=c(1,2))
#plot_smooth(m1r,view="dist", plot_all="condition",ylab = 'ENV') 
#plot_smooth(m2r,view="dist", plot_all="condition",ylab = 'F0')

#load in the image
mypng <- stack(paste0(parentfolder, "/images/Figure6.png"))  
plotRGB(mypng,maxpixels=1e500000)
```

Table 2. Model results for GAM analysis
```{r tables_GAM, echo=FALSE, output = FALSE, message = FALSE}
tm2 <- cbind(contrast =  c("intercept", "Wrist vs. Passive", "Arm vs. Passive"," ",
                   "intercept", "Male vs. Female", "Wrist vs. Passive", "Arm vs. Passive"), 
                                          b = c(round(mod1$p.coeff,3),"",round(mod2$p.coeff,3)), 
                                          SE = c(  round(mod1$se[1:3], 3),"", round(mod1$se[1:4],3)),
                                          df = c(round(mod1$p.t,3),"", round(mod2$p.t,3)),
                                          p = c(round(mod1$p.pv,4),"", round(mod2$p.pv,4)))
rownames(tm2) <- c("ENV (z-scaled)", "", ""," ",
                   "F0 (Hz)", "", "", "")
tm2[,5] <- ifelse(tm2[,5] == 0, "< .0001", tm2[,5])
knitr::kable(tm2, digits = 3, align = "c", booktabs = T)

```

*Note*. Model results are shown for the amplitude envelope (ENV; z-scaled) and F0 (Hz). For F0, we accounted for sex differences when estimating independent effects of condition.

## Degree of physical impetus and acoustic peaks  
  We have confirmed that speech acoustics are modulated around moments of the deceleration phase, about 0-200 ms before the maximum extension.  The effect of gesture-speech physics can be further examined by looking at how the forces produced by the upper limb movement predict acoustic peaks. Therefore, for all vocalizations that occurred between 200--0ms before the maximum extension, we assessed whether acoustic peak (i.e., maximum F0 or maximum amplitude envelope) was predicted by the maximum deceleration value (i.e., minimum acceleration observation) observed in that 200 ms window. In previous research, we found that higher deceleration was related to higher amplitude envelope observations but not F0 [@pouwEnergyFlowsGesturespeech2019a].  
  Figure 7 shows the general pattern of the results for the wrist- and arm-movement condition. Here we averaged per trial the maximum deceleration values of max F0 and max ENV for each vocalization event. Table 3 shows the model results of linear mixed effects model with random intercept and slopes for participants, in which we regressed the trial-averaged max observed deceleration against the co-occurring trial-averaged vocalization acoustic peaks for amplitude envelope and F0 (separately). Higher deceleration indeed predicted higher amplitude envelope. This was also the case for F0, but only for arm movements as opposed to wrist movements, as indicated by a statistically reliable interaction between condition x max deceleration effect (*p*'s < .001). This confirms that both acceleration and the mass of the effector are involved in producing a physical impulse.

Figure 7. Relation max deceleration and height acoustic peak
```{r impact analysis, echo=FALSE, warning = FALSE, message = FALSE}

#for this impact analysis we only do the movement conditions
dd <- subset(fd, condition != "PASSIVE")
dd$dist <- -(dd$filled_z_min-dd$time_ms_rec) #calculate the temporal distance from maximum extension again

dn <- subset(dd, dist < 0 & dist > -200) #subset the region around peak deceleration where higher acoustic modulations were found 
dn$peak_env <- ave(dn$ENVz, dn$unique_vocalization, FUN = function(x) max(x, na.rm = TRUE) ) #get the peak amplitude
dn$peak_F0  <- ave(dn$F0z, dn$unique_vocalization, FUN = function(x) max(x, na.rm = TRUE) )  #get the peak in F0
dn$peak_acc <- ave(dn$acc, dn$unique_vocalization, FUN = function(x) max(abs(x), na.rm = TRUE) )  #get the peak in deceleration
dn$peak_acc <- ifelse(dn$peak_acc < 0.1, NA, dn$peak_acc) #consider only cases when there was some acceleration/deceleration
dn$condition <- factor(as.character(dn$condition), levels = c("WRIST", "ARM"))
#plot deceleration and acoustic peaks averaged per trial
dn$peak_env_av <- ave(dn$peak_env, dn$unique_trial, FUN = function(x) mean(x, na.rm = TRUE))
dn$peak_F0_av <- ave(dn$peak_F0, dn$unique_trial, FUN = function(x) mean(x, na.rm = TRUE))
dn$peak_acc_av <- ave(dn$peak_acc, dn$unique_trial, FUN = function(x) mean(x, na.rm = TRUE))

a <- ggplot(dn[!duplicated(dn$unique_trial),], aes(x= peak_acc_av, y = peak_env_av, color = as.factor(ppn))) + geom_point(size = 2) + facet_grid(.~condition) + bluetheme + xlab("max deceleration") + ylab("max ENV (z-scaled)") +theme(legend.position = "none") + geom_smooth(method = "lm",  alpha= 0, color = "purple", size=1.5)

b <- ggplot(dn[!duplicated(dn$unique_trial),], aes(x= peak_acc_av, y = peak_F0_av,color = as.factor(ppn))) + geom_point(size = 2) + facet_grid(.~condition) + bluetheme + xlab("max deceleration") + ylab("max F0 (z-scaled)")+theme(legend.position = "none")+ geom_smooth(method = "lm",  alpha= 0, color = "red", size=1.5)

#grid.arrange(b, a, nrow =2)

#load in the image
mypng <- stack(paste0(parentfolder, "/images/Figure7.png"))  
plotRGB(mypng,maxpixels=1e500000)
```

*Note Figure 7*. On the x-axis the average maximum deceleration is shown per trial is shown (absolutized negative acceleration value), where 0 indicates no deceleration (absolutized) and positive values indicate higher deceleration rates in cm/seconds squared. It can be seen that deceleration rates are more extreme for the Arm versus the Wrist condition. On the y-axis we have the average maximum observed amplitude envelope (lower panel) and F0 (upper panel) for those moments of deceleration. Higher decelerations co-occur with higher peaks in acoustics for arm movements (but not or less so for wrist movements).  
\newpage

Table 4. Linear mixed effects of deceleration and acoustic peaks  
```{r model_impact, warning= FALSE, message = FALSE}
#effect condition and peak deceleration on vocalization peak
lmdat <- dn[!duplicated(dn$unique_trial),] #only keep 1 observation per unique vocalization
model1e <- lme(peak_env_av~peak_acc_av, data = lmdat, random = ~peak_acc_av|ppn, method = "ML", na.action = na.exclude)
mod1e <- coef(summary(model1e))

model1f <- lme(peak_F0_av~condition*peak_acc_av, data = lmdat, random = ~peak_acc_av|ppn, method = "ML", na.action = na.exclude)
mod1f <- coef(summary(model1f))

tm3 <- cbind(contrast =  c("Intercept", "Max Deceleration"," ",
                   "intercept", "Arm vs. Wrist", "Max Deceleration", "Arm x Max Deceleration"), 
                                          b = c(round(mod1e[,"Value"],3),"",round(mod1f[,"Value"],3)), 
                                                    SE = c( round(mod1e[,"Std.Error"],3),"", round(mod1f[,"Std.Error"],3)),
                                                    df = c(round(mod1e[,"DF"],0)," ", round(mod1f[,"DF"],0)),
                                                    p = c(round(mod1e[,"p-value"],4),"", round(mod1f[,"p-value"],4)))
rownames(tm3) <- c("ENV (z-scaled)", "", " ",
                   "F0 (z-scaled)", "", "", "")
tm3[,5] <- ifelse(tm3[,5] == 0, "< .0001", tm3[,5])
knitr::kable(tm3, digits = 3, align = "c", booktabs = T)
```

\pagebreak
# Discussion  
  In the current study, we demonstrated biomechanical effects of flexion-extension upper limb movements on speech by replicating effects obtained in steady-state vocalization and mono-syllabic utterances in fluent speech. We showed that rhythmically moving the wrist or arm affects vocalization acoustics by heightening F0 and amplitude envelope of speech vocalizations, as compared to a passive control and statistical control conditions. We further show that acoustic modulations of speech vocalizations are especially found around moments of the high-impetus beat (i.e., where the movement abruptly decelerates), thereby producing a physical impetus on the body. We finally show that higher deceleration rates observed within 200 milliseconds before the moment of the maximum extension of the arm movement materializes into more extreme acoustic peaks, demonstrating a role for acceleration and effector mass for gesture's effect onto speech, i.e., an effect of physical impulse. Indeed, in all analyses, we observe that higher-mass arm versus wrist movements affect speech more clearly.  
    Thus, it is now possible that stabilities in speaking can arise out of gesture-speech biomechanics, also in fluent speech. This does not mean that speech prosody necessarily requires gesture for reaching prosodic targets. Indeed, other sensorimotor solutions are available for modulating F0 and intensity, for example in the form of vocal-fold tensioning or respiratory actions [@perrierMotorEquivalenceSpeech2015]. However, we do argue that the biomehechanical coupling of gesture and speech provides a 'smart' mechanism for 'timing' acoustic and movement expressions.
  
## Wider implications
  Gesture-speech physics has promise for a revision of our understanding for the emergence of communicative gesture in anatomically modern humans, both ontogenetically and phylogenetically.   
  It is well known that infants produce concurrent vocal-motor babblings. Furtuermore, increased rhythmicity or frequency of motor babbling predicts speech-like maturation of vocalization [@ejiriRelationshipRhythmicBehavior1998; @ejiriCooccurencesPreverbalVocal2001]. Rather than a primarily neural development that instantiates gesture-speech synchrony [@iversonHandMouthBrain2005], we suggest that during such vocal-motor babblings gesture-speech physics is discovered, which provides the basis for infants to develop novel stable sensorimotor solutions for communication, such as a synchronized pointing gesture with a vocalization. Such sensorimotor solutions are, of course, likely solicited and practiced through support of caretakers, yet without the biomorphological scaffolding, gesture-speech synchrony would not get off the ground ontogenetically.  
  Phylogenetic accounts have forefronted the depiction and referential function of gesture as the driver for its modern day instantiation [@tomaselloOriginsHumanCommunication2008; @kendonReflectionsGesturefirstHypothesis2017; @frohlichMultimodalCommunicationLanguage2019]. However, it should be considered that peripheral body movements may have served as a control parameter of an under-evolved vocal system. It has already been proposed that vocal system may have been evolutionarily exapted from rhythmic abilities in the loco-motor domain [@ravignaniRhythmSpeechAnimal2019; @larssonBipedalStepsDevelopment2019], and upper limb movements as having constrained the vocal systems evolution fits neatly in such views. Specifically, when our species became bipedal it has been argued that the respiratory system was thereby liberated from upper-limb locomotary perturbations. Namely, we know that breathing (and vocalization) cycles often rigidly couple 1:1 with locomotion cycles in quadrupeds [@carrierEnergeticParadoxHuman1984], rigidly limiting what can be done (communicated) in one breath. For example, vocalization acoustics of flying bats are synchronized with their wing beats [@lancasterRespiratoryMuscleActivity1995]. Bipedalism, however, did only free respiration from locomotion, it freed the upper limbs too, allowing these highly skilled articulators to modulate a possibly less killed respiratory-vocal system. Gestures may then have played a role in the complexification of the respiratory system in our species which has been attributed to have occurred to serve speech evolution [@maclarnonEvolutionHumanSpeech1999].  
  Thus gesture-speech physics is not in principle culturally specific, as animals such as bats can do it too. It can further be related to other species, such as Orangutangs who deepen their vocalizations by cupping their hands in front of their mouth [@hardusToolUseWild2009]. Other animals have been found to be sensitive to body-related information in sound in that body size and strength can be detected from vocalizations alone [@pisanskiVoiceModulationWindow2016; @ghazanfarVocaltractResonancesIndexical2007], and humans are able to do this with some accuracy as well [@pisanskiReturnOzVoice2014], even when they are blind from birth [@pisanskiCanBlindPersons2016]. In a recent experiment we have found that listeners are exquisitely sensitive to gesture-modulated acoustics as listeners can synchronize their own upper limb movement by listening to a vocalizer producing a steady-state vocalization while rhythmically moving their wrist or arm [@pouwAcousticInformationUpper2020a; @pouwAcousticSpecificationUpper2019]. Thus, bodily dynamics can imprint the (human) voice and this can be informative for listeners.  
  To conclude, gesture-speech physics opens up the possibility that gesture may have evolved as a control parameter on vocal actions. This radically embodied revision of gesture-speech coupling provides a solid phylogenetic basis for an evolution of multimodal behavior, whereby peripheral bodily tensioning naturally formed coalitions with sound-producing organs that were still very much under development.

\newpage

# References
```{r create_references}
r_refs(file = "r-references.bib")
r_refs(file = "mybib.bib")
```


\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
