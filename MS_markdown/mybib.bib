
@article{abneyComplexityMatchingDyadic2014,
  title = {Complexity Matching in Dyadic Conversation},
  author = {Abney, D. H. and Paxton, A. and Dale, R. and Kello, Christopher T.},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {143},
  pages = {2304--2315},
  issn = {1939-2222(Electronic),0096-3445(Print)},
  doi = {10.1037/xge0000021},
  abstract = {Recent studies of dyadic interaction have examined phenomena of synchronization, entrainment, alignment, and convergence. All these forms of behavioral matching have been hypothesized to play a supportive role in establishing coordination and common ground between interlocutors. In the present study, evidence is found for a new kind of coordination termed complexity matching. Temporal dynamics in conversational speech signals were analyzed through time series of acoustic onset events. Timing in periods of acoustic energy was found to exhibit behavioral matching that reflects complementary timing in turn-taking. In addition, acoustic onset times were found to exhibit power law clustering across a range of timescales, and these power law functions were found to exhibit complexity matching that is distinct from behavioral matching. Complexity matching is discussed in terms of interactive alignment and other theoretical principles that lead to new hypotheses about information exchange in dyadic conversation and interaction in general. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZG2VIXUT\\2014-41508-001.html},
  keywords = {Acoustics,Conversation,Dyads,Interpersonal Interaction,Interstimulus Interval,Stimulus Complexity},
  number = {6}
}

@article{abneyMovementDynamicsReflect2015,
  title = {Movement Dynamics Reflect a Functional Role for Weak Coupling and Role Structure in Dyadic Problem Solving},
  author = {Abney, D. H. and Paxton, A. and Dale, R. and Kello, C. T.},
  date = {2015},
  journaltitle = {Cognitive Processing},
  volume = {16},
  pages = {325--332},
  doi = {https://doi.org/10.1007/s10339-015-0648-2},
  number = {4}
}

@inproceedings{arnoldUsingGeneralizedAdditive2013,
  title = {Using Generalized Additive Models and Random Forests to Model Prosodic Prominence in {{German}}},
  booktitle = {14th {{Annual Conference}} of the {{International Speech Communication Association}}},
  author = {Arnold, D. and Wagner, P. and Baayen, R. H.},
  date = {2013},
  pages = {5},
  location = {{Lyon, France}},
  abstract = {The perception of prosodic prominence is influenced by different sources like different acoustic cues, linguistic expectations and context. We use a generalized additive model and a random forest to model the perceived prominence on a corpus of spoken German. Both models are able to explain over 80\% of the variance. While the random forests give us some insights on the relative importance of the cues, the general additive model gives us insights on the interaction between different cues to prominence.},
  eventtitle = {{{INTERSPEECH}} 2013},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HFUMCEN4\\Arnold et al. - Using generalized additive models and random fores.pdf},
  langid = {english}
}

@article{ascherslebenTemporalControlMovements2002,
  title = {Temporal {{Control}} of {{Movements}} in {{Sensorimotor Synchronization}}},
  author = {Aschersleben, Gisa},
  date = {2002-02-01},
  journaltitle = {Brain and Cognition},
  shortjournal = {Brain and Cognition},
  volume = {48},
  pages = {66--79},
  issn = {0278-2626},
  doi = {10.1006/brcg.2001.1304},
  url = {http://www.sciencedirect.com/science/article/pii/S0278262601913041},
  urldate = {2019-04-02},
  abstract = {Under conditions in which the temporal structure of events (e.g., a sequence of tones) is predictable, performing movements in synchrony with this sequence of events (e.g., dancing) is an easy task. A rather simplified version of this task is studied in the sensorimotor synchronization paradigm. Participants are instructed to synchronize their finger taps with an isochronous sequence of signals (e.g., clicks). Although this is an easy task, a systematic error is observed: Taps usually precede clicks by several tens of milliseconds. Different models have been proposed to account for this effect (“negative asynchrony” or “synchronization error”). One group of explanations is based on the idea that synchrony is established at the level of central representations (and not at the level of external events), and that the timing of an action is determined by the (anticipated) action effect. These assumptions are tested by manipulating the amount of sensory feedback available from the tap as well as its temporal characteristics. This article presents an overview of these representational models and the empirical evidence supporting them. It also discusses other accounts briefly in the light of further evidence.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MUE975DA\\Aschersleben - 2002 - Temporal Control of Movements in Sensorimotor Sync.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\CAAEWIZD\\S0278262601913041.html},
  number = {1}
}

@article{ascherslebenTimingMechanismsSensorimotor2002,
  title = {Timing Mechanisms in Sensorimotor Synchronization},
  author = {Aschersleben, Gisa and Stenneken, Prisca and Cole, J. D. and Prinz, Wolfgang},
  date = {2002},
  journaltitle = {Common mechanisms in perception and action},
  pages = {227--244},
  url = {https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_727550},
  urldate = {2019-04-02},
  abstract = {Author: Aschersleben, Gisa et al.; Genre: Book Chapter; Published in Print: 2002; Title: Timing mechanisms in sensorimotor synchronization},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LZMPZVPB\\ViewItemOverviewPage.html},
  langid = {english}
}

@article{ayRobustnessComplexityCoconstructed2007,
  title = {Robustness and Complexity Co-Constructed in Multimodal Signalling Networks},
  author = {Ay, Nihat and Flack, Jessica and Krakauer, David C},
  date = {2007-03-29},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {362},
  pages = {441--447},
  issn = {0962-8436},
  doi = {10.1098/rstb.2006.1971},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2323562/},
  urldate = {2019-08-15},
  abstract = {In animal communication, signals are frequently emitted using different channels (e.g. frequencies in a vocalization) and different modalities (e.g. gestures can accompany vocalizations). We explore two explanations that have been provided for multimodality: (i) selection for high information transfer through dedicated channels and (ii) increasing fault tolerance or robustness through multichannel signals. Robustness relates to an accurate decoding of a signal when parts of a signal are occluded. We show analytically in simple feed-forward neural networks that while a multichannel signal can solve the robustness problem, a multimodal signal does so more effectively because it can maximize the contribution made by each channel while minimizing the effects of exclusion. Multimodality refers to sets of channels where within each set information is highly correlated. We show that the robustness property ensures correlations among channels producing complex, associative networks as a by-product. We refer to this as the principle of robust overdesign. We discuss the biological implications of this for the evolution of combinatorial signalling systems; in particular, how robustness promotes enough redundancy to allow for a subsequent specialization of redundant components into novel signals.},
  eprint = {17255020},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\W59H23UJ\\Ay et al. - 2007 - Robustness and complexity co-constructed in multim.pdf},
  number = {1479},
  pmcid = {PMC2323562}
}

@article{azarLanguageContactDoes2019,
  title = {Language Contact Does Not Drive Gesture Transfer: {{Heritage}} Speakers Maintain Language Specific Gesture Patterns in Each Language},
  shorttitle = {Language Contact Does Not Drive Gesture Transfer},
  author = {Azar, Zeynep and Backus, Ad and Özyürek, Aslı},
  date = {2019-04-30},
  journaltitle = {Bilingualism: Language and Cognition},
  pages = {1--15},
  issn = {1366-7289, 1469-1841},
  doi = {10.1017/S136672891900018X},
  url = {https://www.cambridge.org/core/product/identifier/S136672891900018X/type/journal_article},
  urldate = {2019-08-30},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SIHT5VTL\\Azar et al. - 2019 - Language contact does not drive gesture transfer .pdf},
  langid = {english}
}

@article{baerReflexActivationLaryngeal1979,
  title = {Reflex Activation of Laryngeal Muscles by Sudden Induced Subglottal Pressure Changes},
  author = {Baer, T.},
  date = {1979-05},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {65},
  pages = {1271--1275},
  issn = {0001-4966},
  doi = {10.1121/1.382795},
  abstract = {In measuring the effect of subglottal pressure changes on fundamental frequency (Fo) of phonation, the effects of changing laryngeal muscle activity must be eliminated. Several investigators have used a strategy in which pulsatile increases of subglottal pressure are induced by pushing on the chest or abdomen of a phonating subject. Fundamental frequency is then correlated with subglottal pressure changes during an interval before laryngeal response is assumed to occur. The present study was undertaken to repeat such an experiment while monitoring electromyographic (EMG) activity of some laryngeal muscles, to discover empirically the latency of the laryngeal response. The results showed a consistent response to each push, with a latency of about 30 ms. Despite this response, analyses of fundamental frequency versus subglottal pressure changes during the interval of constant EMG activity were in general agreement with previously published values. With respect to the nature of the electromyographic response itself, its timing was found to be within the range of latencies appropriate for peripheral feedback, and was also similar to that for an acoustically--or tactually--elicited startle reflex.},
  eprint = {458049},
  eprinttype = {pmid},
  keywords = {Electromyography,Glottis,Humans,Laryngeal Muscles,Muscles,Phonation,Pressure,Reflex,Voice},
  langid = {english},
  number = {5}
}

@article{bakersusane.VentilationSpeechCharacteristics2008,
  title = {Ventilation and {{Speech Characteristics During Submaximal Aerobic Exercise}}},
  author = {{Baker Susan E.} and {Hipp Jenny} and {Alessio Helaine}},
  date = {2008-10-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {51},
  pages = {1203--1214},
  doi = {10.1044/1092-4388(2008/06-0223)},
  url = {https://pubs.asha.org/doi/10.1044/1092-4388%282008/06-0223%29},
  urldate = {2019-10-17},
  abstract = {Purpose
      This study examined alterations in ventilation and speech characteristics as well
         as perceived dyspnea during submaximal aerobic exercise tasks.
      
      
      Method
      Twelve healthy participants completed aerobic exercise-only and simultaneous speaking
         and aerobic exercise tasks at 50\% and 75\% of their maximum oxygen consumption (VO2 max). Measures of ventilation, oxygen consumption, heart rate, perceived dyspnea,
         syllables per phrase, articulation rate, and inappropriate linguistic pause placements
         were obtained at baseline and throughout the experimental tasks.
      
      
      Results
      Ventilation was significantly lower during the speaking tasks compared with the nonspeaking
         tasks. Oxygen consumption, however, did not significantly differ between speaking
         and nonspeaking tasks. The perception of dyspnea was significantly higher during the
         speaking tasks compared with the nonspeaking tasks. All speech parameters were significantly
         altered over time at both task intensities.
      
      
      Conclusions
      It is speculated that decreased ventilation without a reduction in oxygen consumption
         implies that utilization of oxygen by the working muscles was increased during the
         speaking tasks to meet the metabolic needs. A greater ability to utilize oxygen from
         inspired air is found in individuals who are at higher fitness levels, and therefore
         these findings may have implications for individuals who must complete simultaneous
         speech and exercise for occupational purposes (e.g., fitness/military drill instructors,
         singers performing choreography).},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5Y2ALGCH\\06-0223).html},
  number = {5}
}

@article{banseAcousticProfilesVocal,
  title = {Acoustic {{Profiles}} in {{Vocal Emotion Expression}}},
  author = {Banse, Rainer and Scherer, Klaus R},
  journaltitle = {Journal of Personality and Social Psychology},
  volume = {70},
  pages = {614--636},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\X8RRIRXT\\Banse and Scherer - Acoustic Profiles in Vocal Emotion Expression.pdf},
  langid = {english}
}

@article{bardRoleAfferentInformation1992,
  title = {Role of Afferent Information in the Timing of Motor Commands: A Comparative Study with a Deafferented Patient},
  shorttitle = {Role of Afferent Information in the Timing of Motor Commands},
  author = {Bard, C. and Paillard, J. and Lajoie, Y. and Fleury, M. and Teasdale, N. and Forget, R. and Lamarre, Y.},
  date = {1992-02},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  volume = {30},
  pages = {201--206},
  issn = {0028-3932},
  abstract = {The accuracy of the motor system in synchronizing simultaneous movements initiations was tested in two conditions: (1) when the motor commands were triggered by an external signal (reactive condition), and (2) when subjects self-paced their movement onsets (self-paced condition). The task consisted of initiating simultaneously ipsilateral finger extension and heel raising. Eight normal subjects and a deafferented patient were tested. In the reactive condition, both normal subjects and the deafferented patient exhibited a precession of finger initiation over heel raising. This delay corresponds to the difference observed in the reaction time of the two limbs when measured independently. It reflects the difference in conduction times of the efferent pathways, as if the two motor commands were released simultaneously through a common triggering signal in the motor cortex. In contrast, in the self-paced condition normal subjects showed precession of heel over finger onsets, suggesting that synchrony is based upon the evaluation of afferent information. Unlike normal subjects, the patient showed no heel precession in the self-paced condition. These findings suggest that reactive and self-paced responses are produced through two different control modes and that afferent information contributes to the timing of motor commands in the self-paced mode.},
  eprint = {1560897},
  eprinttype = {pmid},
  keywords = {Adult,Afferent Pathways,Demyelinating Diseases,Electromyography,Female,Humans,Male,Mental Processes,Movement,Reaction Time,Time Factors},
  langid = {english},
  number = {2}
}

@article{baronchelliNetworksCognitiveScience2013,
  title = {Networks in {{Cognitive Science}}},
  author = {Baronchelli, Andrea and Ferrer-i-Cancho, Ramon and Pastor-Satorras, Romualdo and Chater, Nick and Christiansen, Morten H.},
  date = {2013-07},
  journaltitle = {Trends in Cognitive Sciences},
  volume = {17},
  pages = {348--360},
  issn = {13646613},
  doi = {10.1016/j.tics.2013.04.010},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S136466131300096X},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\RFFKLKMX\\Baronchelli et al. - 2013 - Networks in Cognitive Science.pdf},
  langid = {english},
  number = {7}
}

@incollection{barthSlightestWhiffAir2014,
  title = {The {{Slightest Whiff}} of {{Air}}: {{Airflow Sensing}} in {{Arthropods}}},
  shorttitle = {The {{Slightest Whiff}} of {{Air}}},
  booktitle = {Flow {{Sensing}} in {{Air}} and {{Water}}: {{Behavioral}}, {{Neural}} and {{Engineering Principles}} of {{Operation}}},
  author = {Barth, Friedrich G.},
  editor = {Bleckmann, Horst and Mogdans, Joachim and Coombs, Sheryl L.},
  date = {2014},
  pages = {169--196},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-41446-6_7},
  url = {https://doi.org/10.1007/978-3-642-41446-6_7},
  urldate = {2019-05-07},
  abstract = {The perception of medium flows has received ever increasing attention during the last two decades and has increasingly been recognized as a sensory capacity of its own. A combination of experimental work and physical–mathematical modeling has deepened our understanding of the workings of airflow sensors, mainly represented by insect filiform hairs and arachnid trichobothria, both as individual sensors and sensor arrays. This chapter points to the diversity of arthropod airflow sensors and stresses the importance of comparative studies. These should include animal groups so far largely neglected by sensory biology and neuroethology. Another need is to analyze biologically relevant flow patterns and to relate these to the functional properties of the various patterns of sensor arrangement found in different animal taxa. Finally, the capture of a freely flying fly by a wandering spider is taken to illustrate the challenges and promises of studies that aim to reveal the relation between a particular airflow pattern and a specific behavior.},
  isbn = {978-3-642-41446-6},
  keywords = {Digital Particle Image Velocimetry,Hair Shaft,High Frequency Component,Medium Flow,Oribatid Mite},
  langid = {english}
}

@article{bavelasGesturingTelephoneIndependent2008,
  title = {Gesturing on the Telephone: {{Independent}} Effects of Dialogue and Visibility},
  shorttitle = {Gesturing on the Telephone},
  author = {Bavelas, Janet and Gerwing, Jennifer and Sutton, Chantelle and Prevost, Danielle},
  date = {2008-02-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {58},
  pages = {495--520},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2007.02.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0749596X07000289},
  urldate = {2019-10-14},
  abstract = {Speakers often gesture in telephone conversations, even though they are not visible to their addressees. To test whether this effect is due to being in a dialogue, we separated visibility and dialogue with three conditions: face-to-face dialogue (10 dyads), telephone dialogue (10 dyads), and monologue to a tape recorder (10 individuals). For the rate of gesturing, both dialogue and visibility had significant, independent effects, with the telephone condition consistently higher than the tape recorder. Also, as predicted, visibility alone significantly affected how speakers gestured: face-to-face speakers were more likely to make life-size gestures, to put information in their gestures that was not in their words, to make verbal reference to their gestures, and to use more gestures referring to the interaction itself. We speculate that demonstration, as a modality, may underlie these findings and may be intimately tied to dialogue while being suppressed in monologue.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FFGN2YU7\\S0749596X07000289.html},
  keywords = {Demonstration,Face-to-face dialogue,Gestures,Telephone,Visibility},
  number = {2}
}

@article{beckageSmallWorldsSemantic2011,
  title = {Small {{Worlds}} and {{Semantic Network Growth}} in {{Typical}} and {{Late Talkers}}},
  author = {Beckage, Nicole and Smith, Linda and Hills, Thomas},
  editor = {Perc, Matjaz},
  date = {2011-05-11},
  journaltitle = {PLoS ONE},
  volume = {6},
  pages = {e19348},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0019348},
  url = {http://dx.plos.org/10.1371/journal.pone.0019348},
  urldate = {2020-01-21},
  abstract = {Network analysis has demonstrated that systems ranging from social networks to electric power grids often involve a small world structure-with local clustering but global ac cess. Critically, small world structure has also been shown to characterize adult human semantic networks. Moreover, the connectivity pattern of these mature networks is consistent with lexical growth processes in which children add new words to their vocabulary based on the structure of the language-learning environment. However, thus far, there is no direct evidence that a child’s individual semantic network structure is associated with their early language learning. Here we show that, while typically developing children’s early networks show small world structure as early as 15 months and with as few as 55 words, children with language delay (late talkers) have this structure to a smaller degree. This implicates a maladaptive bias in word acquisition for late talkers, potentially indicating a preference for ‘‘oddball’’ words. The findings provide the first evidence of a link between small-world connectivity and lexical development in individual children.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BCG8ME59\\Beckage et al. - 2011 - Small Worlds and Semantic Network Growth in Typica.pdf},
  langid = {english},
  number = {5}
}

@article{bergesonAbsolutePitchTempo2002,
  title = {Absolute Pitch and Tempo in Mothers' Songs to Infants},
  author = {Bergeson, Tonya R. and Trehub, Sandra E.},
  date = {2002-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {13},
  pages = {72--75},
  issn = {0956-7976},
  doi = {10.1111/1467-9280.00413},
  abstract = {We examined the relative stability of pitch, tempo, and rhythm in maternal speech and singing to prelinguistic infants. Mothers were recorded speaking and singing to their infants on two occasions separated by 1 week or more. The pitch level and tempo of identical utterances were highly variable across the 1-week period, but these features were virtually unchanged in song repetitions. Rhythmic patterning was largely maintained in speech, as in song. Mothers' accurate reproduction of their sung performances can be considered a form of absolute pitch and absolute tempo.},
  eprint = {11892783},
  eprinttype = {pmid},
  keywords = {Communication,Humans,Infant,Mother-Child Relations,Mothers,Periodicity,Speech,Verbal Behavior},
  langid = {english},
  number = {1}
}

@book{bernsteinCoordinationRegulationsMovements1967,
  title = {The {{Co}}-Ordination and {{Regulations}} of {{Movements}}},
  author = {Bernstein, N.},
  date = {1967},
  edition = {[1st English ed.] edition},
  publisher = {{Pergamon Press}},
  langid = {english}
}

@article{bertranProsodicTypologyDichotomy1999,
  title = {Prosodic {{Typology}}: {{On}} the {{Dichotomy}} between {{Stress}}-{{Timed}} and {{Syllable}}-{{Timed Languages}}},
  author = {Bertran, A. P.},
  date = {1999},
  journaltitle = {Language Design},
  volume = {2},
  pages = {103--130}
}

@article{beugherSemiautomaticAnnotationTool2018,
  title = {A Semi-Automatic Annotation Tool for Unobtrusive Gesture Analysis},
  author = {Beugher, Stijn De and Brône, Geert and Goedemé, Toon},
  date = {2018-06},
  journaltitle = {Language Resources and Evaluation},
  volume = {52},
  pages = {433--460},
  issn = {1574-020X, 1574-0218},
  doi = {10.1007/s10579-017-9404-9},
  url = {http://link.springer.com/10.1007/s10579-017-9404-9},
  urldate = {2019-06-21},
  abstract = {In a variety of research fields, including linguistics, human–computer interaction research, psychology, sociology and behavioral studies, there is a growing interest in the role of gestural behavior related to speech and other modalities. The analysis of multimodal communication requires high-quality video data and detailed annotation of the different semiotic resources under scrutiny. In the majority of cases, the annotation of hand position, hand motion, gesture type, etc. is done manually, which is a time-consuming enterprise requiring multiple annotators and substantial resources. In this paper we present a semi-automatic alternative, in which the focus lies on minimizing the manual workload while guaranteeing highly accurate annotations. First, we discuss our approach, which consists of several processing steps such as identifying the hands in images, calculating motion of the hands, segmenting the recording in gesture and non-gesture events, etc. Second, we validate our approach against existing corpora in terms of accuracy and usefulness. The proposed approach is designed to provide annotations according to the McNeill (Hand and mind: what gestures reveal about thought, University of Chicago Press, Chicago, 1992) gesture space and the output is compatible with annotation tools such as ELAN or ANVIL.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\75PKN23Z\\Beugher et al. - 2018 - A semi-automatic annotation tool for unobtrusive g.pdf},
  langid = {english},
  number = {2}
}

@article{billonRoleSensoryInformation1996,
  title = {The Role of Sensory Information in the Production of Periodic Finger-Tapping Sequences},
  author = {Billon, M. and Semjen, A. and Cole, J. and Gauthier, G.},
  date = {1996-06-01},
  journaltitle = {Experimental Brain Research},
  shortjournal = {Exp Brain Res},
  volume = {110},
  pages = {117--130},
  issn = {1432-1106},
  doi = {10.1007/BF00241381},
  url = {https://doi.org/10.1007/BF00241381},
  urldate = {2019-04-02},
  abstract = {A subject lacking proprioceptive and tactile sensibility below the neck and a group of control subjects performed sequences of periodic finger taps involving a pattern of accentuation. The required intertap interval was 700 ms. In some situations, the taps were synchronized with the clicks of a metronome. Feedback conditions were manipulated by either allowing or not allowing the subjects to hear the taps and see their finger movements. We recorded the trajectory of the subjects' finger displacement in the vertical plane, and the force and moment of contact of the finger with the response key. The control subjects achieved precise timing of the finger taps by trading off downstroke onset for movement duration, e.g., they initiated shorter-duration tapping movements with a delay. This strategy did not vary depending on task demands (e.g., synchronization) or feedback conditions. The deafferented patient produced intertap intervals on average close to the required value. However, his tap timing was characterized by increased variability and severe distortion (lengthening) after the accentuated tap, regardless of feedback conditions. He did not manifest the compensatory strategy whereby, in control subjects, movement onset was adjusted to movement duration. Thus, such a strategy in controls seems to depend on intact proprioceptive and/or tactile information from the moving limb. Upon withdrawal of visual and acoustic feedback, the deafferented subject increased the force of the taps and the amplitude of tapping movements; his mean synchronization error with the metronome also increased. However, he did not lose correct phasing between the taps and the clicks of the metronome. These findings suggest that, under normal circumstances, sequential movements are timed by an internal timekeeper which paces sensory consequences relating to the occurrence of behaviorally important events (e.g., finger taps), and not the onset of the movements eliciting those events. In the synchronization task, the timekeeper may be phase locked to the periodic acoustic stimuli by direct entrainment. Feedback information may be needed, however, for keeping any synchronization error as small as possible.},
  keywords = {Internal clock,Movement timing,Sequence timing,Synchronization},
  langid = {english},
  number = {1}
}

@article{billonSimultaneityTwoEffectors1996,
  title = {Simultaneity of Two Effectors in Synchronization with a Periodic External Signal},
  author = {Billon, M. and Bard, C. and Fleury, M. and Blouin, J. and Teasdale, N.},
  date = {1996-02-01},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {15},
  pages = {25--38},
  issn = {0167-9457},
  doi = {10.1016/0167-9457(95)00037-2},
  url = {http://www.sciencedirect.com/science/article/pii/0167945795000372},
  urldate = {2019-04-02},
  abstract = {Several studies have shown that the control of simultaneous movements differ according to the execution context. For instance, when subjects raise simultaneously their index finger and heel as fast as possible after an auditory signal, the simultaneity is controlled by sending synchronously the motor commands to both effectors. On the other hand, when subjects self-pace their movements, the simultaneity is controlled by processing the delay between afferent signals from both movements at the central level (Paillard, 1948). It has been hypothesized that a mode of control similar to the self-paced condition is also used when subjects produce simultaneous and repetitive movements in synchronization with a metronome (Fraisse, 1980). We examined this hypothesis by asking subjects to move simultaneously the index finger and heel in synchronization with metronome sounds. Results showed that the events chronology (i.e., heel movement first, finger movement second and metronome sound third) was a function of the relative distance of the effectors and auditory organ from the central comparator. We deduced that the synchronization and simultaneity was evaluated by computing the time elapsed between the arrival of the sensory feedback of the movement and auditory signal. The second goal of the study was to assess whether, in such a task, each effector is synchronized separately to the metronome sound or together as an unit. A strong positive correlation was found between finger and heel synchronization errors. This supports the hypothesis that finger and heel movements are synchronized as an unit to the metronome rather than independently. In conclusion, simultaneity between effectors and synchronization between effectors and an external signal, although they share similar processes based on afferent information, are likely to be controlled separately.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TQNZ46K7\\0167945795000372.html},
  keywords = {Movement sequence,Sensorimotor process,Tapping,Temporal coordination,Timing},
  number = {1}
}

@article{blasiHumanSoundSystems2019,
  title = {Human Sound Systems Are Shaped by Post-{{Neolithic}} Changes in Bite Configuration},
  author = {Blasi, D. E. and Moran, S. and Moisik, S. R. and Widmer, P. and Dediu, D. and Bickel, B.},
  date = {2019-03-15},
  journaltitle = {Science},
  volume = {363},
  pages = {eaav3218},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aav3218},
  url = {https://science.sciencemag.org/content/363/6432/eaav3218},
  urldate = {2019-05-04},
  abstract = {The first fricatives
In 1985, the linguist Charles Hockett proposed that the use of teeth and jaws as tools in hunter-gatherer populations makes consonants produced with lower lip and upper teeth (“f” and “v” sounds) hard to produce. He thus conjectured that these sounds were a recent innovation in human language. Blasi et al. combined paleoanthropology, speech sciences, historical linguistics, and methods from evolutionary biology to provide evidence for a Neolithic global change in the sound systems of the world's languages. Spoken languages have thus been shaped by changes in the human bite configuration owing to changes in dietary and behavioral practices since the Neolithic.
Science, this issue p. eaav3218
Structured Abstract
INTRODUCTIONHuman speech manifests itself in spectacular diversity, ranging from ubiquitous sounds such as “m” and “a” to the rare click consonants in some languages of southern Africa. This range is generally thought to have been fixed by biological constraints since at least the emergence of Homo sapiens. At the same time, the abundance of each sound in the languages of the world is commonly taken to depend on how easy the sound is to produce, perceive, and learn. This dependency is also regarded as fixed at the species level.
RATIONALEGiven this dependency, we expect that any change in the human apparatus for production, perception, or learning affects the probability—or even the range—of the sounds that languages have. Paleoanthropological evidence suggests that the production apparatus has undergone a fundamental change of just this kind since the Neolithic. Although humans generally start out with vertical and horizontal overlap in their bite configuration (overbite and overjet, respectively), masticatory exertion in the Paleolithic gave rise to an edge-to-edge bite after adolescence. Preservation of overbite and overjet began to persist long into adulthood only with the softer diets that started to become prevalent in the wake of agriculture and intensified food processing. We hypothesize that this post-Neolithic decline of edge-to-edge bite enabled the innovation and spread of a new class of speech sounds that is now present in nearly half of the world’s languages: labiodentals, produced by positioning the lower lip against the upper teeth, such as in “f” or “v.”
RESULTSBiomechanical models of the speech apparatus show that labiodentals incur about 30\% less muscular effort in the overbite and overjet configuration than in the edge-to-edge bite configuration. This difference is not present in similar articulations that place the upper lip, instead of the teeth, against the lower lip (as in bilabial “m,” “w,” or “p”). Our models also show that the overbite and overjet configuration reduces the incidental tooth/lip distance in bilabial articulations to 24 to 70\% of their original values, inviting accidental production of labiodentals. The joint effect of a decrease in muscular effort and an increase in accidental production predicts a higher probability of labiodentals in the language of populations where overbite and overjet persist into adulthood. When the persistence of overbite and overjet in a population is approximated by the prevalence of agriculturally produced food, we find that societies described as hunter-gatherers indeed have, on average, only about one-fourth the number of labiodentals exhibited by food-producing societies, after controlling for spatial and phylogenetic correlation. When the persistence is approximated by the increase in food-processing technology over the history of one well-researched language family, Indo-European, we likewise observe a steady increase of the reconstructed probability of labiodental sounds, from a median estimate of about 3\% in the proto-language (6000 to 8000 years ago) to a presence of 76\% in extant languages.
CONCLUSIONOur findings reveal that the transition from prehistoric foragers to contemporary societies has had an impact on the human speech apparatus, and therefore on our species’ main mode of communication and social differentiation: spoken language. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/363/6432/eaav3218/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Labiodentals depend on bite configuration.Biomechanical modeling shows that labiodental sounds like “f” are easier to produce (and to accidentally arise) under overbite and overjet (A) than under the edge-to-edge bite (B) that prevailed before the Neolithic (C). Overbite and overjet persisted only when exposed to the softer diets that became characteristic with food production (D versus E) and more recently with intensified food processing (F). Both developments led to a spread of labiodental sounds.
Linguistic diversity, now and in the past, is widely regarded to be independent of biological changes that took place after the emergence of Homo sapiens. We show converging evidence from paleoanthropology, speech biomechanics, ethnography, and historical linguistics that labiodental sounds (such as “f” and “v”) were innovated after the Neolithic. Changes in diet attributable to food-processing technologies modified the human bite from an edge-to-edge configuration to one that preserves adolescent overbite and overjet into adulthood. This change favored the emergence and maintenance of labiodentals. Our findings suggest that language is shaped not only by the contingencies of its history, but also by culturally induced changes in human biology.
Diet-induced changes in the human bite over recent millennia led to the spread of new speech sounds, including “f” and “v.”
Diet-induced changes in the human bite over recent millennia led to the spread of new speech sounds, including “f” and “v.”},
  eprint = {30872490},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SNSELIZ9\\eaav3218.html},
  langid = {english},
  number = {6432}
}

@software{boersmaPraatDoingPhonetics2019,
  title = {Praat: Doing Phonetics by Computer [{{Computer}} Program]},
  author = {Boersma, P. and Weenink, D.},
  date = {2019},
  url = {http://www.praat.org/},
  version = {6.1.03}
}

@article{boeWhichWayDawn2019,
  title = {Which Way to the Dawn of Speech?: {{Reanalyzing}} Half a Century of Debates and Data in Light of Speech Science},
  shorttitle = {Which Way to the Dawn of Speech?},
  author = {Boë, Louis-Jean and Sawallis, Thomas R. and Fagot, Joël and Badin, Pierre and Barbier, Guillaume and Captier, Guillaume and Ménard, Lucie and Heim, Jean-Louis and Schwartz, Jean-Luc},
  date = {2019-12-01},
  journaltitle = {Science Advances},
  volume = {5},
  pages = {eaaw3916},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aaw3916},
  url = {https://advances.sciencemag.org/content/5/12/eaaw3916},
  urldate = {2020-01-03},
  abstract = {Recent articles on primate articulatory abilities are revolutionary regarding speech emergence, a crucial aspect of language evolution, by revealing a human-like system of proto-vowels in nonhuman primates and implicitly throughout our hominid ancestry. This article presents both a schematic history and the state of the art in primate vocalization research and its importance for speech emergence. Recent speech research advances allow more incisive comparison of phylogeny and ontogeny and also an illuminating reinterpretation of vintage primate vocalization data. This review produces three major findings. First, even among primates, laryngeal descent is not uniquely human. Second, laryngeal descent is not required to produce contrasting formant patterns in vocalizations. Third, living nonhuman primates produce vocalizations with contrasting formant patterns. Thus, evidence now overwhelmingly refutes the long-standing laryngeal descent theory, which pushes back “the dawn of speech” beyond \textasciitilde{}200 ka ago to over \textasciitilde{}20 Ma ago, a difference of two orders of magnitude.
Fresh analysis of primate calls shows that speech dawned in monkeys some 100 times earlier than the appearance of modern humans.
Fresh analysis of primate calls shows that speech dawned in monkeys some 100 times earlier than the appearance of modern humans.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\G3TPISG8\\Boë et al. - 2019 - Which way to the dawn of speech Reanalyzing half.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AXFU8KRA\\eaaw3916.html},
  langid = {english},
  number = {12}
}

@article{bohnYoungChildrenSpontaneously2019,
  title = {Young Children Spontaneously Recreate Core Properties of Language in a New Modality},
  author = {Bohn, Manuel and Kachel, Gregor and Tomasello, Michael},
  date = {2019-11-27},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1904871116},
  url = {https://www.pnas.org/content/early/2019/11/26/1904871116},
  urldate = {2019-12-03},
  abstract = {How the world’s 6,000+ natural languages have arisen is mostly unknown. Yet, new sign languages have emerged recently among deaf people brought together in a community, offering insights into the dynamics of language evolution. However, documenting the emergence of these languages has mostly consisted of studying the end product; the process by which ad hoc signs are transformed into a structured communication system has not been directly observed. Here we show how young children create new communication systems that exhibit core features of natural languages in less than 30 min. In a controlled setting, we blocked the possibility of using spoken language. In order to communicate novel messages, including abstract concepts, dyads of children spontaneously created novel gestural signs. Over usage, these signs became increasingly arbitrary and conventionalized. When confronted with the need to communicate more complex meanings, children began to grammatically structure their gestures. Together with previous work, these results suggest that children have the basic skills necessary, not only to acquire a natural language, but also to spontaneously create a new one. The speed with which children create these structured systems has profound implications for theorizing about language evolution, a process which is generally thought to span across many generations, if not millennia.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\B342IPFB\\Bohn et al. - 2019 - Young children spontaneously recreate core propert.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\MXCZ975A\\1904871116.html},
  keywords = {cognitive development,evolution,gesture,language},
  langid = {english}
}

@article{bosbachInferringAnotherExpectation2005,
  title = {Inferring Another's Expectation from Action: The Role of Peripheral Sensation},
  shorttitle = {Inferring Another's Expectation from Action},
  author = {Bosbach, S. and Cole, J. D. and Prinz, W. and Knoblich, G.},
  date = {2005-10},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat. Neurosci.},
  volume = {8},
  pages = {1295--1297},
  issn = {1097-6256},
  doi = {10.1038/nn1535},
  abstract = {It is unclear how knowledge of one's actions and one's body contribute to the understanding of others' actions. Here we show that two subjects lacking cutaneous touch and sense of movement and position show a selective deficit in interpreting another person's anticipation of weight when seeing him lifting boxes. We suggest that this ability occurs through mental simulation of action dependent on internal motor representations, which require peripheral sensation for their maintenance.},
  eprint = {16136040},
  eprinttype = {pmid},
  keywords = {Female,Forecasting,Humans,Judgment,Kinesthesis,Male,Mental Processes,Sensation,Sensory Deprivation,Weight Lifting},
  langid = {english},
  number = {10}
}

@article{brookshireVisualCortexEntrains2017,
  title = {Visual Cortex Entrains to Sign Language},
  author = {Brookshire, Geoffrey and Lu, Jenny and Nusbaum, Howard C. and Goldin-Meadow, Susan and Casasanto, Daniel},
  date = {2017-05-24},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  pages = {201620350},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1620350114},
  url = {https://www.pnas.org/content/early/2017/05/23/1620350114},
  urldate = {2019-08-22},
  abstract = {Despite immense variability across languages, people can learn to understand any human language, spoken or signed. What neural mechanisms allow people to comprehend language across sensory modalities? When people listen to speech, electrophysiological oscillations in auditory cortex entrain to slow ({$<<<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>\&$}lt;{$<$}/mml:mo{$><$}/mml:math{$>$}8 Hz) fluctuations in the acoustic envelope. Entrainment to the speech envelope may reflect mechanisms specialized for auditory perception. Alternatively, flexible entrainment may be a general-purpose cortical mechanism that optimizes sensitivity to rhythmic information regardless of modality. Here, we test these proposals by examining cortical coherence to visual information in sign language. First, we develop a metric to quantify visual change over time. We find quasiperiodic fluctuations in sign language, characterized by lower frequencies than fluctuations in speech. Next, we test for entrainment of neural oscillations to visual change in sign language, using electroencephalography (EEG) in fluent speakers of American Sign Language (ASL) as they watch videos in ASL. We find significant cortical entrainment to visual oscillations in sign language {$<$}5 Hz, peaking at ∼∼{$<$}mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"{$><$}mml:mo{$>$}∼{$<$}/mml:mo{$><$}/mml:math{$>$}1 Hz. Coherence to sign is strongest over occipital and parietal cortex, in contrast to speech, where coherence is strongest over the auditory cortex. Nonsigners also show coherence to sign language, but entrainment at frontal sites is reduced relative to fluent signers. These results demonstrate that flexible cortical entrainment to language does not depend on neural processes that are specific to auditory speech perception. Low-frequency oscillatory entrainment may reflect a general cortical mechanism that maximizes sensitivity to informational peaks in time-varying signals.},
  eprint = {28559320},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ECNYBPH7\\Brookshire et al. - 2017 - Visual cortex entrains to sign language.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\WUCDMU8F\\1620350114.html},
  keywords = {cortical entrainment,EEG,oscillations,sign language},
  langid = {english}
}

@article{byrneGreatApeGestures2017,
  title = {Great Ape Gestures: Intentional Communication with a Rich Set of Innate Signals},
  shorttitle = {Great Ape Gestures},
  author = {Byrne, R. W. and Cartmill, E. and Genty, E. and Graham, K. E. and Hobaiter, C. and Tanner, J.},
  date = {2017},
  journaltitle = {Animal Cognition},
  shortjournal = {Anim Cogn},
  volume = {20},
  pages = {755--769},
  issn = {1435-9448},
  doi = {10.1007/s10071-017-1096-4},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5486474/},
  urldate = {2019-11-18},
  abstract = {Great apes give gestures deliberately and voluntarily, in order to influence particular target audiences, whose direction of attention they take into account when choosing which type of gesture to use. These facts make the study of ape gesture directly relevant to understanding the evolutionary precursors of human language; here we present an assessment of ape gesture from that perspective, focusing on the work of the “St Andrews Group” of researchers. Intended meanings of ape gestures are relatively few and simple. As with human words, ape gestures often have several distinct meanings, which are effectively disambiguated by behavioural context. Compared to the signalling of most other animals, great ape gestural repertoires are large. Because of this, and the relatively small number of intended meanings they achieve, ape gestures are redundant, with extensive overlaps in meaning. The great majority of gestures are innate, in the sense that the species’ biological inheritance includes the potential to develop each gestural form and use it for a specific range of purposes. Moreover, the phylogenetic origin of many gestures is relatively old, since gestures are extensively shared between different genera in the great ape family. Acquisition of an adult repertoire is a process of first exploring the innate species potential for many gestures and then gradual restriction to a final (active) repertoire that is much smaller. No evidence of syntactic structure has yet been detected.},
  eprint = {28502063},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BRYNHC2D\\Byrne et al. - 2017 - Great ape gestures intentional communication with.pdf},
  number = {4},
  pmcid = {PMC5486474}
}

@inproceedings{caoRealtimeMultiPerson2D2017,
  title = {Realtime {{Multi}}-{{Person 2D Pose Estimation Using Part Affinity Fields}}},
  author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2017},
  pages = {7291--7299},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html},
  urldate = {2019-04-17},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\F9E2SARJ\\Cao et al. - 2017 - Realtime Multi-Person 2D Pose Estimation Using Par.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\LC5IZ8SB\\Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html}
}

@inproceedings{caoRealtimeMultiperson2D2017,
  title = {Realtime {{Multi}}-Person {{2D Pose Estimation Using Part Affinity Fields}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2017-07},
  pages = {1302--1310},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.143},
  url = {http://ieeexplore.ieee.org/document/8099626/},
  urldate = {2019-12-11},
  abstract = {We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII MultiPerson benchmark, both in performance and efficiency.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\86MLSR4E\\Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf},
  isbn = {978-1-5386-0457-1},
  langid = {english}
}

@inproceedings{carelloEcologicalAcousticsAcoustic2001,
  title = {Ecological Acoustics 1 {{Acoustic Specification}} of {{Object Properties}}},
  author = {Carello, C. and Wagman, J. B. and Turvey, M. T.},
  date = {2001},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DNTEG4F7\\Carello et al. - 2001 - Ecological acoustics 1 Acoustic Specification of O.pdf},
  keywords = {Acoustics}
}

@article{carelloPerceptionObjectLength1998,
  title = {Perception of Object Length by Sound},
  author = {Carello, Claudia and Anderson, Krista L. and Kunkler-Peck, Andrew J.},
  date = {1998},
  journaltitle = {Psychological Science},
  volume = {9},
  pages = {211--214},
  issn = {1467-9280(Electronic),0956-7976(Print)},
  doi = {10.1111/1467-9280.00040},
  abstract = {The goal of the present research was to provide an empirical evaluation of the basic capability of size perception by sound (in particular, perception of the lengths of dropped wooden dowels) and to identify the physical properties of the objects that constrain that perception. In Exp 1, 8 Ss were told to listen to a rod dropped 5 times and move the adjustable surface out from the proximal edge of the desk to a position that could just be reached with the rod, so its length would fit between the desk and the surface. The location of the surface was recorded. The same procedure was followed in a 2nd experiment using 6 Ss. Results show the ordinal and metrical success of naive listeners was related to length but not to the simple acoustic variables (duration, amplitude, frequency) likely to be related to it. Additional analysis suggests the potential relevance of an object's inertia tensor in constraining perception of that object's length, analogous to the case that has been made for perceiving length by effortful touch. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CJAAPZYY\\Carello et al. - 1998 - Perception of object length by sound.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AXRWYI7M\\1998-02495-009.html},
  keywords = {Auditory Perception,Auditory Stimulation,Stimulus Parameters},
  number = {3}
}

@article{chandrasekaranNaturalStatisticsAudiovisual2009,
  title = {The Natural Statistics of Audiovisual Speech},
  author = {Chandrasekaran, Chandramouli and Trubanova, Andrea and Stillittano, Sébastien and Caplier, Alice and Ghazanfar, Asif A.},
  editor = {Friston, Karl J.},
  date = {2009-07-17},
  journaltitle = {PLoS Computational Biology},
  volume = {5},
  pages = {e1000436},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000436},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1000436},
  urldate = {2019-07-11},
  abstract = {Humans, like other animals, are exposed to a continuous stream of signals, which are dynamic, multimodal, extended, and time varying in nature. This complex input space must be transduced and sampled by our sensory systems and transmitted to the brain where it can guide the selection of appropriate actions. To simplify this process, it’s been suggested that the brain exploits statistical regularities in the stimulus space. Tests of this idea have largely been confined to unimodal signals and natural scenes. One important class of multisensory signals for which a quantitative input space characterization is unavailable is human speech. We do not understand what signals our brain has to actively piece together from an audiovisual speech stream to arrive at a percept versus what is already embedded in the signal structure of the stream itself. In essence, we do not have a clear understanding of the natural statistics of audiovisual speech. In the present study, we identified the following major statistical features of audiovisual speech. First, we observed robust correlations and close temporal correspondence between the area of the mouth opening and the acoustic envelope. Second, we found the strongest correlation between the area of the mouth opening and vocal tract resonances. Third, we observed that both area of the mouth opening and the voice envelope are temporally modulated in the 2–7 Hz frequency range. Finally, we show that the timing of mouth movements relative to the onset of the voice is consistently between 100 and 300 ms. We interpret these data in the context of recent neural theories of speech which suggest that speech communication is a reciprocally coupled, multisensory event, whereby the outputs of the signaler are matched to the neural processes of the receiver.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QWRX4JBI\\Chandrasekaran et al. - 2009 - The Natural Statistics of Audiovisual Speech.pdf},
  langid = {english},
  number = {7}
}

@article{changMutualInteractionsSpeech1987,
  title = {Mutual Interactions between Speech and Finger Movements},
  author = {Chang, P. and Hammond, G. R.},
  date = {1987-06},
  journaltitle = {Journal of Motor Behavior},
  shortjournal = {J Mot Behav},
  volume = {19},
  pages = {265--274},
  issn = {0022-2895},
  doi = {10.1080/00222895.1987.10735411},
  abstract = {Speech output and finger movements were recorded as right-handed males repeated a syllable while making cyclical finger movements in three experimental conditions: (2) maintaining constant amplitude in both response systems; (b) alternating speech amplitude while attempting to maintain constant finger movement amplitude; and (c) alternating finger movement amplitude while attempting to maintain constant speech amplitude. Observations showed that output of the two response systems was coupled (one syllable was uttered with each finger movement) and entrained in amplitude (the amplitude pattern of the response that the subject attempted to keep constant followed that of the concurrently-active amplitude-modulated response). These interactions were bidirectional and were present with both left-handed and right-handed finger movements. The interactions are more extensive and subtle than mere interference wtih one response system by the other, and apparently do not depend on anatomical overlap of the responding neural systems.},
  eprint = {14988062},
  eprinttype = {pmid},
  langid = {english},
  number = {2}
}

@article{chanNetworkStructureInfluences,
  title = {Network {{Structure Influences Speech Production}}},
  author = {Chan, K. Y. and Vitevich, M. S.},
  journaltitle = {Cognitive Science},
  volume = {43},
  pages = {685--697},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2010.01100.x},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9T6NVBGM\\j.1551-6709.2010.01100.html}
}

@article{charltonAreMenBetter2013,
  title = {Are Men Better than Women at Acoustic Size Judgements?},
  author = {Charlton, Benjamin D. and Taylor, Anna M. and Reby, David},
  date = {2013-08-23},
  journaltitle = {Biology Letters},
  shortjournal = {Biology Letters},
  volume = {9},
  pages = {20130270},
  doi = {10.1098/rsbl.2013.0270},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsbl.2013.0270},
  urldate = {2019-10-17},
  abstract = {Formants are important phonetic elements of human speech that are also used by humans and non-human mammals to assess the body size of potential mates and rivals. As a consequence, it has been suggested that formant perception, which is crucial for speech perception, may have evolved through sexual selection. Somewhat surprisingly, though, no previous studies have examined whether sexes differ in their ability to use formants for size evaluation. Here, we investigated whether men and women differ in their ability to use the formant frequency spacing of synthetic vocal stimuli to make auditory size judgements over a wide range of fundamental frequencies (the main determinant of vocal pitch). Our results reveal that men are significantly better than women at comparing the apparent size of stimuli, and that lower pitch improves the ability of both men and women to perform these acoustic size judgements. These findings constitute the first demonstration of a sex difference in formant perception, and lend support to the idea that acoustic size normalization, a crucial prerequisite for speech perception, may have been sexually selected through male competition. We also provide the first evidence that vocalizations with relatively low pitch improve the perception of size-related formant information.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\JMAPCEK8\\Charlton et al. - 2013 - Are men better than women at acoustic size judgeme.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ADS7BQVN\\rsbl.2013.html},
  number = {4}
}

@article{christiansenNoworNeverBottleneckFundamental2016,
  title = {The {{Now}}-or-{{Never}} Bottleneck: {{A}} Fundamental Constraint on Language},
  shorttitle = {The {{Now}}-or-{{Never}} Bottleneck},
  author = {Christiansen, Morten H. and Chater, Nick},
  date = {2016-01},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  volume = {39},
  pages = {e62},
  issn = {1469-1825},
  doi = {10.1017/S0140525X1500031X},
  abstract = {Memory is fleeting. New material rapidly obliterates previous material. How, then, can the brain deal successfully with the continual deluge of linguistic input? We argue that, to deal with this "Now-or-Never" bottleneck, the brain must compress and recode linguistic input as rapidly as possible. This observation has strong implications for the nature of language processing: (1) the language system must "eagerly" recode and compress linguistic input; (2) as the bottleneck recurs at each new representational level, the language system must build a multilevel linguistic representation; and (3) the language system must deploy all available information predictively to ensure that local linguistic ambiguities are dealt with "Right-First-Time"; once the original input is lost, there is no way for the language system to recover. This is "Chunk-and-Pass" processing. Similarly, language learning must also occur in the here and now, which implies that language acquisition is learning to process, rather than inducing, a grammar. Moreover, this perspective provides a cognitive foundation for grammaticalization and other aspects of language change. Chunk-and-Pass processing also helps explain a variety of core properties of language, including its multilevel representational structure and duality of patterning. This approach promises to create a direct relationship between psycholinguistics and linguistic theory. More generally, we outline a framework within which to integrate often disconnected inquiries into language processing, language acquisition, and language change and evolution.},
  eprint = {25869618},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\WNN4FEGI\\Christiansen and Chater - 2016 - The Now-or-Never bottleneck A fundamental constra.pdf},
  keywords = {Brain,chunking,Cognition,grammaticalization,Humans,incremental interpretation,Language,language acquisition,language evolution,language processing,online learning,prediction,processing bottleneck,psycholinguistics,Time Factors},
  langid = {english}
}

@article{chuSynchronizationSpeechGesture2014,
  title = {Synchronization of Speech and Gesture: {{Evidence}} for Interaction in Action},
  author = {Chu, Mingyuan and Hagoort, Peter},
  date = {2014},
  journaltitle = {Journal of Experimental Psychology: General},
  volume = {143},
  doi = {10.1037/a0036281},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3V5X83P7\\Chu and Hagoort - Synchronization of Speech and Gesture Evidence fo.pdf},
  langid = {english},
  number = {3}
}

@software{clarkeGgbeeswarmCategoricalScatter2017,
  title = {Ggbeeswarm: {{Categorical Scatter}} ({{Violin Point}}) {{Plots}}},
  shorttitle = {Ggbeeswarm},
  author = {Clarke, Erik and Sherrill-Mix, Scott},
  date = {2017-08-07},
  url = {https://CRAN.R-project.org/package=ggbeeswarm},
  urldate = {2019-09-03},
  abstract = {Provides two methods of plotting categorical scatter plots such that the arrangement of points within a category reflects the density of data at that region, and avoids over-plotting.},
  version = {0.6.0}
}

@article{coleEvokedPotentialsMan1991,
  title = {Evoked Potentials in a Man with a Complete Large Myelinated Fibre Sensory Neuropathy below the Neck},
  author = {Cole, J. D. and Katifi, H. A.},
  date = {1991-03-01},
  journaltitle = {Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section},
  shortjournal = {Electroencephalography and Clinical Neurophysiology/Evoked Potentials Section},
  volume = {80},
  pages = {103--107},
  issn = {0168-5597},
  doi = {10.1016/0168-5597(91)90147-P},
  url = {http://www.sciencedirect.com/science/article/pii/016855979190147P},
  urldate = {2019-04-02},
  abstract = {Cortical somatosensory evoked potentials (SEPs) were recorded from a man with a severe neuropathy without touch and proprioception below the neck. Peripheral neurophysiological tests showed a complete large myelinated fibre sensory neuropathy. Sensory threshold to electrical stimulation of the median nerve was 15 mA (normal 2–4 mA). With a stimulus of 39 mA, duration 400 μsec, applied at the wrist a cortical SEP was recorded with a latency of 84 msec, giving a propagation velocity of 11.9 m/sec. At stimulation rates of above 3.3 Hz the SEP was absent. It is concluded that the SEPs recorded were conducted along Aδ peripheral fibres.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TA76HKNA\\016855979190147P.html},
  keywords = {Aδ SEPs,Electrical stimuli,Sensory neuropathy},
  number = {2}
}

@article{coleEvokedPotentialsSubject1995,
  title = {Evoked Potentials in a Subject with a Large-Fibre Sensory Neuropathy below the Neck},
  author = {Cole, J. D. and Merton, W. L. and Barrett, G. and Katifi, H. A. and Treede, R.-D.},
  date = {1995-02-01},
  journaltitle = {Canadian Journal of Physiology and Pharmacology},
  shortjournal = {Can. J. Physiol. Pharmacol.},
  volume = {73},
  pages = {234--245},
  issn = {0008-4212},
  doi = {10.1139/y95-034},
  url = {https://www.nrcresearchpress.com/doi/abs/10.1139/y95-034},
  urldate = {2019-04-02},
  abstract = {The results from experiments in various modalities of evoked potentials are described in a subject with a complete large peripheral neuropathy below the neck. He has no tactile or position sensitivity below that level, but has retained fatigue, pain, and temperature sensation. Percutaneous electrical stimulation of peripheral nerves led to scalp recorded evoked potentials with thresholds and propagation velocities compatible with conduction along A-δ peripheral pathways. CO2 laser evoked potentials were similar to those seen in controls, further support for intact A-δ peripheral fibres. Movement-related cortical potentials (MRCPs) were recorded associated with active and passive movement of the middle finger. The former were normal, evidence that the termination of the MRCP is not dependent on peripheral feedback. By comparing passive MRCPs between controls and the subject it was possible to establish which parts of the potentials are visual and which are proprioceptive and to gain evidence of central reo..., On décrit les résultats d'expériences effectuées en utilisant divers protocoles de potentiels évoqués (PÉ) chez un sujet souffrant d'une neuropathie périphérique des fibres de grand diamètre de toute la partie du corps située au-dessous du cou. Ce sujet n'a ni sensibilité posturale ni sensibilité tactile sous ce niveau, mais ressent encore la douleur due à la fatigue et présente une sensibilité thermique. Une stimulation électrique percutanée des nerfs périphériques a permis d'enregistrer au niveau crânien des PÉ dont les seuils et les vitesses de propagation s'accordaient avec une conduction le long des voies périphériques A-δ. Les PÉ au laser CO2 ont été similaires à ceux observés chez les témoins, confirmant ainsi l'existence de fibres périphériques A-δ intactes. On a enregistré des potentiels corticaux liés au mouvement (PCLM) lors de mouvements actifs et passifs du médius. Les premiers étaient normaux, ce qui indique que l'interruption du PCLM ne dépend pas d'une boucle de rétroaction périphérique. L...},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4D4VU529\\y95-034.html},
  number = {2}
}

@article{coleGestureFollowingDeafferentation2002,
  title = {Gesture Following Deafferentation: A Phenomenologically Informed Experimental Study},
  shorttitle = {Gesture Following Deafferentation},
  author = {Cole, Jonathan and Gallagher, Shaun and McNeill, David},
  date = {2002-03-01},
  journaltitle = {Phenomenology and the Cognitive Sciences},
  shortjournal = {Phenomenology and the Cognitive Sciences},
  volume = {1},
  pages = {49--67},
  issn = {1572-8676},
  doi = {10.1023/A:1015572619184},
  url = {https://doi.org/10.1023/A:1015572619184},
  urldate = {2019-04-16},
  abstract = {Empirical studies of gesture in a subject who has lost proprioception and the sense of touch from the neck down show that specific aspects of gesture remain normal despite abnormal motor processes for instrumental movement. The experiments suggest that gesture, as a linguistic phenomenon, is not reducible to instrumental movement. They also support and extend claims made by Merleau-Ponty concerning the relationship between language and cognition. Gesture, as language, contributes to the accomplishment of thought.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\WMQEJIJ8\\Cole et al. - 2002 - Gesture following deafferentation a phenomenologi.pdf},
  keywords = {Artificial Intelligence,Empirical Study,Experimental Study,Motor Process,Specific Aspect},
  langid = {english},
  number = {1}
}

@book{coleLosingTouchMan2016,
  title = {Losing {{Touch}}: {{A}} Man without His Body},
  shorttitle = {Losing {{Touch}}},
  author = {Cole, J. D.},
  date = {2016-10-28},
  publisher = {{Oxford University Press}},
  abstract = {What is like to live without touch or movement/position sense (proprioception)? The only way to understand the importance of these senses, so familiar we cannot imagine their absence, is to ask someone in that position. Ian Waterman lost them below the neck over forty years ago, though pain and temperature perception and his peripheral movement nerves were unaffected. Without proprioceptive feedback and touch the movement brain was disabled. Completely unable to move, he felt disembodied and frightened. Then, slowly, he taught himself to dress, eat and walk by thinking about each movement and with visual supervision. In Losing Touch, the narrative moves between biography and scientific research, theatre, documentary and zero gravity. He has been married three times, and built up successful careers in disability access audit, using his impairment to his advantage, and in rare turkey breeding and journalism. The neuroscience has led to data on movement without feedback, the pleasantness of touch, gesture, pain and body orientation in space. The account shows how the science was actually done but also reveals Ian's journey from passive subject to informed critic of science and scientists and that the science has given him both more understanding but also greater confidence personally. His unique response to such a rare condition has also led to a BBC documentary, theatrical portrayals and a weightless flight with NASA. As a young man he sought triumph over his impairment; now, nearly 65, he has more mature reflections on living with such an extraordinary loss, the limits it has imposed and the opportunities it has enabled. He gives his views on scientists and on others he has met including Oliver Sacks and Peter Brook. In an Afterword those from science, the arts and philosophy give an appreciation of his contribution. The book is the result of nearly 30 years close collaboration between author and subject.},
  eprint = {V6akDAAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-19-108769-1},
  keywords = {Medical / Neurology,Philosophy / Mind & Body,Psychology / Cognitive Psychology & Cognition,Psychology / Physiological Psychology,Science / General,Science / Life Sciences / Neuroscience},
  langid = {english},
  pagetotal = {201}
}

@book{coleLosingTouchMan2016a,
  title = {Losing {{Touch}}: {{A}} Man without His Body},
  shorttitle = {Losing {{Touch}}},
  author = {Cole, Jonathan},
  date = {2016-09-01},
  edition = {1 edition},
  publisher = {{Oxford University Press}},
  location = {{Oxford, United Kingdom ; New York, NY, United States of America}},
  abstract = {What is like to live without touch or movement/position sense (proprioception)? The only way to understand the importance of these senses, so familiar we cannot imagine their absence, is to ask someone in that position. Ian Waterman lost them below the neck over forty years ago, though pain and temperature perception and his peripheral movement nerves were unaffected. Without proprioceptive feedback and touch the movement brain was disabled. Completely unable to move, he felt disembodied and frightened. Then, slowly, he taught himself to dress, eat and walk by thinking about each movement and with visual supervision. In Losing Touch, the narrative moves between biography and scientific research, theatre, documentary and zero gravity. He has been married three times, and built up successful careers in disability access audit, using his impairment to his advantage, and in rare turkey breeding and journalism. The neuroscience has led to data on movement without feedback, the pleasantness of touch, gesture, pain and body orientation in space. The account shows how the science was actually done but also reveals Ian's journey from passive subject to informed critic of science and scientists and that the science has given him both more understanding but also greater confidence personally. His unique response to such a rare condition has also led to a BBC documentary, theatrical portrayals and a weightless flight with NASA.As a young man he sought triumph over his impairment; now, nearly 65, he has more mature reflections on living with such an extraordinary loss, the limits it has imposed and the opportunities it has enabled. He gives his views on scientists and on others he has met including Oliver Sacks and Peter Brook. In an Afterword those from science, the arts and philosophy give an appreciation of his contribution. The book is the result of nearly 30 years close collaboration between author and subject.},
  isbn = {978-0-19-877887-5},
  langid = {english},
  pagetotal = {224}
}

@book{colePrideDailyMarathon1995,
  title = {Pride and a {{Daily Marathon}}},
  author = {Cole, Jonathan},
  date = {1995-07-11},
  edition = {New Ed edition},
  publisher = {{A Bradford Book}},
  location = {{Cambridge, Mass}},
  abstract = {At the age of 19, Ian Waterman was suddenly struck down at work by a rare neurological illness that deprived him of all sensation below the neck. He fell on the floor in a heap, unable to stand or control his limbs, having lost the sense of joint position and proprioception, of that "sixth sense" of his body in space, which we all take for granted. After months in a neurological ward he was judged incurable and condemned to a life of wheelchair dependence. This is the first U.S. publication of a remarkable book by his physician, Jonathan Cole. It tells the compelling story, including a clear clinical description of a rare condition, of how Waterman reclaimed a life of full mobility against all expectations, by mental effort and sheer courage. Cole describes how Waterman gradually adapted to his strange condition. As the doctors had predicted, there was no neurological recovery. He had to monitor every movement by sight to work out where his limbs were, since he had no feedback from his peripheral nerves. But with astonishing persistence Waterman developed elaborate tricks and strategies to control his movements, enabling him to cope not only with the day-to-day problems of living, but even with the challenges of work, love, and marriage.},
  isbn = {978-0-262-53136-8},
  langid = {english},
  pagetotal = {216}
}

@book{colombettiFeelingBodyAffective2014,
  title = {The Feeling Body: {{Affective}} Science Meets the Enactive Mind},
  author = {Colombetti, G.},
  date = {2014},
  publisher = {{MIT press}},
  location = {{Cambridge, MA}}
}

@article{comstockSensorimotorSynchronizationAuditory2018,
  title = {Sensorimotor {{Synchronization With Auditory}} and {{Visual Modalities}}: {{Behavioral}} and {{Neural Differences}}},
  shorttitle = {Sensorimotor {{Synchronization With Auditory}} and {{Visual Modalities}}},
  author = {Comstock, Daniel C. and Hove, Michael J. and Balasubramaniam, Ramesh},
  date = {2018-07-18},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front Comput Neurosci},
  volume = {12},
  issn = {1662-5188},
  doi = {10.3389/fncom.2018.00053},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6058047/},
  urldate = {2019-10-18},
  abstract = {It has long been known that the auditory system is better suited to guide temporally precise behaviors like sensorimotor synchronization (SMS) than the visual system. Although this phenomenon has been studied for many years, the underlying neural and computational mechanisms remain unclear. Growing consensus suggests the existence of multiple, interacting, context-dependent systems, and that reduced precision in visuo-motor timing might be due to the way experimental tasks have been conceived. Indeed, the appropriateness of the stimulus for a given task greatly influences timing performance. In this review, we examine timing differences for sensorimotor synchronization and error correction with auditory and visual sequences, to inspect the underlying neural mechanisms that contribute to modality differences in timing. The disparity between auditory and visual timing likely relates to differences in the processing specialization between auditory and visual modalities (temporal vs. spatial). We propose this difference could offer potential explanation for the differing temporal abilities between modalities. We also offer suggestions as to how these sensory systems interface with motor and timing systems.},
  eprint = {30072885},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SV43Z9DY\\Comstock et al. - 2018 - Sensorimotor Synchronization With Auditory and Vis.pdf},
  pmcid = {PMC6058047}
}

@online{ConversationalGesturesAutism,
  title = {Conversational Gestures in Autism Spectrum Disorders: {{Asynchrony}} but Not Decreased Frequency - de {{Marchena}} - 2010 - {{Autism Research}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/aur.159?casa_token=mR8s8HhLJ-wAAAAA:YDn6WZNz0NJB1KygloMRwkT5_5E2x9v3qvGSQSVfOOo4vfS7ED1CzE7a3HvHtetb5H8_vf8vRyZppYbY},
  urldate = {2019-08-31},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5J538ALY\\aur.html}
}

@article{cookEmbodiedCommunicationSpeakers2009,
  title = {Embodied Communication: Speakers' Gestures Affect Listeners' Actions},
  shorttitle = {Embodied Communication},
  author = {Cook, Susan Wagner and Tanenhaus, Michael K.},
  date = {2009-10},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {113},
  pages = {98--104},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2009.06.006},
  abstract = {We explored how speakers and listeners use hand gestures as a source of perceptual-motor information during naturalistic communication. After solving the Tower of Hanoi task either with real objects or on a computer, speakers explained the task to listeners. Speakers' hand gestures, but not their speech, reflected properties of the particular objects and the actions that they had previously used to solve the task. Speakers who solved the problem with real objects used more grasping handshapes and produced more curved trajectories during the explanation. Listeners who observed explanations from speakers who had previously solved the problem with real objects subsequently treated computer objects more like real objects; their mouse trajectories revealed that they lifted the objects in conjunction with moving them sideways, and this behavior was related to the particular gestures that were observed. These findings demonstrate that hand gestures are a reliable source of perceptual-motor information during human communication.},
  eprint = {19682672},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9AQ99SZN\\Cook and Tanenhaus - 2009 - Embodied communication speakers' gestures affect .pdf},
  keywords = {Acoustic Stimulation,Adult,Cognition,Gestures,Humans,Problem Solving,Psychomotor Performance,Speech,Speech Perception},
  langid = {english},
  number = {1},
  pmcid = {PMC2763957}
}

@article{cooperriderForegroundGestureBackground2019,
  title = {Foreground Gesture, Background Gesture},
  author = {Cooperrider, K.},
  date = {2019},
  journaltitle = {Gesture},
  volume = {16},
  pages = {176--202},
  doi = {https://doi.org/10.1075/gest.16.2.02coo},
  url = {https://benjamins.com/catalog/gest.16.2.02coo},
  urldate = {2020-01-26},
  abstract = {Do speakers intend their gestures to communicate? Central as this question is to the study of gesture, researchers cannot seem to agree on the answer. According to one common framing, gestures are an “unwitting” window into the mind (McNeill, 1992); but, according to another common framing, they are designed along with speech to form “composite utterances” (Enfield, 2009). These two framings correspond to two cultures within gesture studies~– the first cognitive and the second interactive in orientation~– and they appear to make incompatible claims. In this article I attempt to bridge the cultures by developing a distinction between foreground gestures and background gestures. Foreground gestures are designed in their particulars to communicate a critical part of the speaker’s message; background gestures are not designed in this way. These are two fundamentally different kinds of gesture, not two different ways of framing the same monolithic behavior. Foreground gestures can often be identified by one or more of the following hallmarks: they are produced along with demonstratives; they are produced in the absence of speech; they are co-organized with speaker gaze; and they are produced with conspicuous effort. The distinction between foreground and background gestures helps dissolve the apparent tension between the two cultures: interactional researchers have focused on foreground gestures and elevated them to the status of a prototype, whereas cognitive researchers have done the same with background gestures. The distinction also generates a number of testable predictions about gesture production and understanding, and it opens up new lines of inquiry into gesture across child development and across cultures.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Q6GLFS9Y\\gest.16.2.html},
  langid = {english},
  number = {2}
}

@book{corballisHandMouthOrigins2002,
  title = {From Hand to Mouth: {{The}} Origins of Language},
  author = {Corballis, M. C.},
  date = {2002},
  publisher = {{Princeton University Press}},
  location = {{Princeton, NJ.}}
}

@article{cordoPropertiesPosturalAdjustments1982,
  title = {Properties of Postural Adjustments Associated with Rapid Arm Movements},
  author = {Cordo, P. J. and Nashner, L. M.},
  date = {1982-02},
  journaltitle = {Journal of Neurophysiology},
  shortjournal = {J. Neurophysiol.},
  volume = {47},
  pages = {287--302},
  issn = {0022-3077},
  doi = {10.1152/jn.1982.47.2.287},
  abstract = {1. We have examined rapid postural adjustments associated with a class of voluntary movements that disturb postural equilibrium. In the text that follows, these motor activities are termed associated postural adjustments and voluntary focal movements, respectively. Standing human subjects performed a variety of movement tasks on a hand-held manipulandum, resulting in disturbances to their postural equilibrium. The experimental use of movements that interact with the subject's environment in a relatively simple was permitted a more precise comparison of the postural adjustments with their associated focal movements. 2. Subjects either pulled or pushed on a stiff interface (the handle) or they responded in a predetermined way to handle perturbations. These activities were carried out with various degrees of steady-state postural stability. Prior to and during these movements, support surface and handle forces, electromyographic (EMG) signals, and body sway were monitored. 3. In addition to previously shown postural adjustments associated with reaction-time armed movements, we have demonstrated these postural activities occur in concept with segmental stretch reflexes and self-initiated (untriggered) movements. Postural adjustments were initiated shortly before all focal movements tested except the short-latency component of the biceps stretch reflex (25- to 30-ms latency). However, this reflex component was rarely elicited by handle perturbations in free-standing subjects; therefore, postural adjustments usually preceded any biceps activity under this condition. 4. By varying the degree of steady-state postural equilibrium, a reciprocal gain/threshold relationship between postural and focal components was documented, i.e., when stability was high, postural activity was reduced or absent and focal activity enhanced. Conversely, the biceps stretch reflex was difficult to elicit under any condition where the subjects was not fully supported in the direction of movement and reaction times of focal movements were prolonged. 5. Postural activities associated with focal movements were found to share a number of organizational properties with automatic postural adjustments to support surface movements. Specifically, the postural muscle synergies were equivalent in muscle composition, relative activation magnitudes, and relative temporal sequencing. Furthermore, both types of postural adjustments were highly specific in locus and magnitude to the quality of steady-state postural equilibrium (e.g., postural "set"). 6. A conceptual model is proposed that suggests one simple way in which the reciprocal influence of postural set on postural and focal movement components and their temporal sequencing might be accomplished. Furthermore, we propose in this model a common central organization of postural adjustments associated with focal movements and those elicited by support-surface movements.},
  eprint = {7062101},
  eprinttype = {pmid},
  keywords = {Afferent Pathways,Arm,Feedback,Humans,Models; Neurological,Motor Activity,Movement,Muscle Contraction,Muscles,Organ Specificity,Posture},
  langid = {english},
  number = {2}
}

@incollection{cox2016,
  booktitle = {Chromatic and {{Anisotropic Cross}}-{{Recurrence Quantification Analysis}} of {{Interpersonal Behavior}} | {{SpringerLink}}},
  author = {Cox, R. F. A. and van der Steen, S. and Guevara, M. and De Jonge-Hoekstra, L. and van Dijk, M.},
  date = {2016},
  url = {https://link.springer.com/chapter/10.1007/978-3-319-29922-8_11},
  urldate = {2019-12-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VCUXJUG5\\978-3-319-29922-8_11.html},
  options = {useprefix=true},
  series = {Recurrence {{Plots}} and {{Their Quanitifactions}}}
}

@article{crasbornCombiningVideoNumeric,
  title = {Combining Video and Numeric Data in the Analysis of Sign Languages within the {{ELAN}} Annotation Software},
  author = {Crasborn, Onno and Sloetjes, Han and Auer, Eric and Wittenburg, Peter},
  pages = {7},
  abstract = {This paper describes hardware and software that can be used for the phonetic study of sign languages. The field of sign language phonetics is characterised, and the hardware that is currently in use is described. The paper focuses on the software that was developed to enable the recording of finger and hand movement data, and the additions to the ELAN annotation software that facilitate the further visualisation and analysis of the data.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QLUDTLMV\\Crasborn et al. - Combining video and numeric data in the analysis o.pdf},
  langid = {english}
}

@article{cravottaEffectsEncouragingUse2019,
  title = {Effects of {{Encouraging}} the {{Use}} of {{Gestures}} on {{Speech}}},
  author = {Cravotta, A. and Busà, M. G. and Prieto, P.},
  date = {2019},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  doi = {10.21437/SpeechProsody.2018-42}
}

@article{cravottaRestrainingEncouragingUse2018,
  title = {Restraining and Encouraging the Use of Hand Gestures: Effects on Speech},
  shorttitle = {Restraining and Encouraging the Use of Hand Gestures},
  author = {Cravotta, A. and Grazia, B. M. and Prieto, P.},
  date = {2018},
  issn = {2333-2042},
  doi = {http://dx.doi.org/10.21437/SpeechProsody.2018-42},
  url = {http://repositori.upf.edu/handle/10230/35125},
  urldate = {2019-05-04},
  abstract = {Previous studies have investigated the effects of the inability to make hand gestures on speakers’ fluency; however, the question of whether encouraging speakers to gesture affects their fluency has received little attention. This study investigates the effect of restraining (Experiment 1) and encouraging (Experiment 2) hand gestures on the following correlates of speech: speech discourse length (number of words and discourse length in seconds), disfluencies (filled pauses, self-corrections, repetitions, insertions, interruptions, silent pauses), and acoustic properties (speech rate, measures of intensity and pitch). In two experiments, 10 native speakers of
Italian took part in a narration task where they were asked to describe comic strips. Each experiment compared two conditions. In Experiment 1, subjects first received no instructions as to how to behave when narrating. Then they were told to sit on their hands while speaking. In Experiment 2, subjects first received no instructions and were then actively encouraged to use hand gestures. The results showed that restraining gestures leads to quieter and slower paced speech, while encouraging gestures triggers longer speech discourse, faster speech rate and more fluent and louder speech. Thus, both restraining and encouraging hand gestures seem to clearly affect prosodic properties of speech, particularly speech fluency.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YKLIAL3A\\Prieto Vives et al. - 2018 - Restraining and encouraging the use of hand gestur.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\64PB283C\\35125.html},
  langid = {english}
}

@software{csardiPackageIgraphNetwork2019,
  title = {Package 'igraph' {{Network Analysis}} and {{Visualization}}},
  author = {Csárdi, G.},
  date = {2019},
  url = {http://bioconductor.statistik.tu-dortmund.de/cran/web/packages/igraph/igraph.pdf},
  version = {1.2.4.1}
}

@incollection{cumminsRhythmSpeech2015,
  title = {Rhythm and {{Speech}}},
  booktitle = {The {{Handbook}} of {{Speech Production}}},
  author = {Cummins, Fred},
  editor = {Redford, Melissa A.},
  date = {2015-04-24},
  pages = {158--177},
  publisher = {{John Wiley \& Sons, Inc}},
  location = {{Hoboken, NJ}},
  doi = {10.1002/9781118584156.ch8},
  url = {http://doi.wiley.com/10.1002/9781118584156.ch8},
  urldate = {2019-09-27},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5SS5Q277\\Cummins - 2015 - Rhythm and Speech.pdf},
  isbn = {978-1-118-58415-6 978-0-470-65993-9},
  langid = {english}
}

@inproceedings{cwiekIconicProsodyRooted2019,
  title = {Iconic {{Prosody}} Is Rooted in Sensori-Motor Properties: {{Fundamental}} Frequency and the Vertical Space},
  booktitle = {41st {{Annual Meeting}} of the {{Cognitive Science SocietyAt}}},
  author = {Cwiek, A. and Fuchs, S.},
  date = {2019},
  location = {{Montreal, Canada}},
  eventtitle = {{{CogSci}} 2019}
}

@article{daleHowHumansMake2018,
  title = {“{{How}} Do Humans Make Sense?” Multiscale Dynamics and Emergent Meaning},
  shorttitle = {“{{How}} Do Humans Make Sense?},
  author = {Dale, Rick and Kello, Christopher T.},
  date = {2018-08-01},
  journaltitle = {New Ideas in Psychology},
  shortjournal = {New Ideas in Psychology},
  volume = {50},
  pages = {61--72},
  issn = {0732-118X},
  doi = {10.1016/j.newideapsych.2017.09.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0732118X1730020X},
  urldate = {2019-08-15},
  abstract = {The challenges posed by the composite nature of sense-making encourage us to study how that composite is dynamically assembled. In this paper, we consider the computational underpinnings that drive the composite nature of interaction. We look to the dynamic properties of recurrent neural networks. What kind of dynamic system inherently integrates multiple signals across different levels and modalities? We argue below that three fundamental properties are needed: dynamic memory, timescale integration, and multimodal integration. We argue that a growing area of investigation in neural networks, reservoir computing, has all these properties (Jaeger, 2001). A simple version of this model is then created to demonstrate “emergent meaning,” using a simplified model communication system.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ANKZFS9Q\\S0732118X1730020X.html}
}

@article{dalziellDanceChoreographyCoordinated2013,
  title = {Dance {{Choreography Is Coordinated}} with {{Song Repertoire}} in a {{Complex Avian Display}}},
  author = {Dalziell, Anastasia H. and Peters, Richard A. and Cockburn, Andrew and Dorland, Alexandra D. and Maisey, Alex C. and Magrath, Robert D.},
  date = {2013-06-17},
  journaltitle = {Current Biology},
  shortjournal = {Current Biology},
  volume = {23},
  pages = {1132--1135},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2013.05.018},
  url = {https://www.cell.com/current-biology/abstract/S0960-9822(13)00581-2},
  urldate = {2019-10-17},
  eprint = {23746637},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EHU86PNY\\Dalziell et al. - 2013 - Dance Choreography Is Coordinated with Song Repert.pdf},
  langid = {english},
  number = {12}
}

@article{dannerQuantitativeAnalysisMultimodal2018,
  title = {Quantitative Analysis of Multimodal Speech Data},
  author = {Danner, Samantha Gordon and Barbosa, Adriano Vilela and Goldstein, Louis},
  date = {2018-11-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {71},
  pages = {268--283},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2018.09.007},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447017302280},
  urldate = {2019-05-04},
  abstract = {This study presents techniques for quantitatively analyzing coordination and kinematics in multimodal speech using video, audio and electromagnetic articulography (EMA) data. Multimodal speech research has flourished due to recent improvements in technology, yet gesture detection/annotation strategies vary widely, leading to difficulty in generalizing across studies and in advancing this field of research. We describe how FlowAnalyzer software can be used to extract kinematic signals from basic video recordings; and we apply a technique, derived from speech kinematic research, to detect bodily gestures in these kinematic signals. We investigate whether kinematic characteristics of multimodal speech differ dependent on communicative context, and we find that these contexts can be distinguished quantitatively, suggesting a way to improve and standardize existing gesture identification/annotation strategy. We also discuss a method, Correlation Map Analysis (CMA), for quantifying the relationship between speech and bodily gesture kinematics over time. We describe potential applications of CMA to multimodal speech research, such as describing characteristics of speech-gesture coordination in different communicative contexts. The use of the techniques presented here can improve and advance multimodal speech and gesture research by applying quantitative methods in the detection and description of multimodal speech.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\U3QL5MLF\\Danner et al. - 2018 - Quantitative analysis of multimodal speech data.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\H4ZUTUED\\S0095447017302280.html},
  keywords = {Bodily gesture,Communicative context,Correlation Map Analysis,FlowAnalyzer,Multimodal speech,Time-varying coordination}
}

@article{debreslioskaDiscourseReferenceBimodal2019,
  title = {Discourse {{Reference Is Bimodal}}: {{How Information Status}} in {{Speech Interacts}} with {{Presence}} and {{Viewpoint}} of {{Gestures}}},
  shorttitle = {Discourse {{Reference Is Bimodal}}},
  author = {Debreslioska, Sandra and Gullberg, Marianne},
  date = {2019-01-02},
  journaltitle = {Discourse Processes},
  volume = {56},
  pages = {41--60},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2017.1351909},
  url = {https://doi.org/10.1080/0163853X.2017.1351909},
  urldate = {2019-09-20},
  abstract = {Speakers use speech and gestures to represent referents in discourse. Depending on referents’ information status, in speech speakers will vary richness of expression (e.g., lexical noun phrase [NP]/pronoun), nominal definiteness (indefinite/definite), and grammatical role (subject/object). This study tested whether these three linguistic markers of information status interact with presence of gestures and gestural viewpoint (observer/character). The results show that gestures are more frequent with less accessible referents expressed with richer spoken forms but that richness of expression does not interact with viewpoint. In contrast, nominal definiteness and grammatical role interact with both presence and viewpoint of gestures. Gestures occur mainly with indefinite lexical NPs and objects. Character viewpoint gestures occur mainly with indefinite lexical NPs and objects plus predicates. The results shed light on when and how speakers use gestures in connected discourse and specifically highlight the discursive function of gestural viewpoint.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZCX2ASCE\\0163853X.2017.html},
  keywords = {bimodal discourse,gestures,information status,mode of representation,referential expressions},
  number = {1}
}

@article{dejongCorrelationPcenterAdjustments1994,
  title = {The Correlation of {{P}}-Center Adjustments with Articulatory and Acoustic Events},
  author = {De Jong, Kenneth J.},
  date = {1994-07-01},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  volume = {56},
  pages = {447--460},
  issn = {1532-5962},
  doi = {10.3758/BF03206736},
  url = {https://doi.org/10.3758/BF03206736},
  urldate = {2019-09-28},
  abstract = {To evaluate articulatory models of perceptual center (P-center) location, listeners performed perceptual adjustments on stimuli which were extracted from a corpus of articulatory data. To avoid streaming effects, the stimuli were not edited to obtain temporal variation; instead, they varied in stress and segmental content. Adjustments were evaluated as to their simultaneity with acoustic and articulatory events. The first experiment yielded various articulatory and acoustic correlates of P-center location; the second yielded different articulatory predictors and no acoustic effective predictors. Multiple correlation analyses showed a variation from P-center locations predicted by the articulatory events that were associated with other predictors. Thus, P-center locations do not correspond to any particular kinematic articulatory event, but rather to a complex of events taken from throughout the stimuli. These results are discussed in terms of their relevance to a model of P-centers as indices of underlying gestural timing.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8JYH24TV\\De Jong - 1994 - The correlation of P-center adjustments with artic.pdf},
  keywords = {Acoustic Event,Tongue Dorsum,Voice Onset Time,Vowel Duration,Word Pair},
  langid = {english},
  number = {4}
}

@article{delacruz-paviaFindingPhrasesInterplay2019,
  title = {Finding {{Phrases}}: {{The Interplay}} of {{Word Frequency}}, {{Phrasal Prosody}} and {{Co}}-Speech {{Visual Information}} in {{Chunking Speech}} by {{Monolingual}} and {{Bilingual Adults}}},
  shorttitle = {Finding {{Phrases}}},
  author = {de la Cruz-Pavía, Irene and Werker, Janet F. and Vatikiotis-Bateson, Eric and Gervain, Judit},
  date = {2019-04-19},
  journaltitle = {Language and Speech},
  shortjournal = {Lang Speech},
  pages = {0023830919842353},
  issn = {0023-8309},
  doi = {10.1177/0023830919842353},
  url = {https://doi.org/10.1177/0023830919842353},
  urldate = {2020-01-06},
  abstract = {The audiovisual speech signal contains multimodal information to phrase boundaries. In three artificial language learning studies with 12 groups of adult participants we investigated whether English monolinguals and bilingual speakers of English and a language with opposite basic word order (i.e., in which objects precede verbs) can use word frequency, phrasal prosody and co-speech (facial) visual information, namely head nods, to parse unknown languages into phrase-like units. We showed that monolinguals and bilinguals used the auditory and visual sources of information to chunk “phrases” from the input. These results suggest that speech segmentation is a bimodal process, though the influence of co-speech facial gestures is rather limited and linked to the presence of auditory prosody. Importantly, a pragmatic factor, namely the language of the context, seems to determine the bilinguals’ segmentation, overriding the auditory and visual cues and revealing a factor that begs further exploration.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HUVB9ZII\\de la Cruz-Pavía et al. - 2019 - Finding Phrases The Interplay of Word Frequency, .pdf},
  keywords = {artificial grammar learning,bilingualism,co-speech visual information,frequency-based information,phrase segmentation,prosody},
  langid = {english},
  options = {useprefix=true}
}

@article{dennisPrivacyOpenScience2019,
  title = {Privacy versus Open Science},
  author = {Dennis, Simon and Garrett, Paul and Yim, Hyungwook and Hamm, Jihun and Osth, Adam F. and Sreekumar, Vishnu and Stone, Ben},
  date = {2019-08-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {51},
  pages = {1839--1848},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01259-5},
  url = {https://doi.org/10.3758/s13428-019-01259-5},
  urldate = {2019-11-30},
  abstract = {Pervasive internet and sensor technologies promise to revolutionize psychological science. However, the data collected using these technologies are often very personal—indeed, the value of the data is often directly related to how personal they are. At the same time, driven by the replication crisis, there is a sustained push to publish data to open repositories. These movements are in fundamental conflict. In this article, we propose a way to navigate this issue. We argue that there are significant advantages to be gained by ceding the ownership of data to the participants who generate the data. We then provide desiderata for a privacy-preserving platform. In particular, we suggest that researchers should use an interface to perform experiments and run analyses, rather than observing the stimuli themselves. We argue that this method not only improves privacy but will also encourage greater compliance with good research practices than is possible through open repositories.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\V4WH7AAN\\Dennis et al. - 2019 - Privacy versus open science.pdf},
  keywords = {Differential privacy,Open repositories,Open science,Privacy},
  langid = {english},
  number = {4}
}

@online{DimensionsGuideDatabase,
  title = {Dimensions.{{Guide}} | {{Database}} of {{Dimensioned Drawings}}},
  url = {https://www.dimensions.guide},
  urldate = {2019-05-01},
  abstract = {A comprehensive reference database of dimensioned drawings documenting the standard measurements and sizes of the everyday objects and spaces that make up our world.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NZQXJZAC\\www.dimensions.guide.html},
  langid = {english}
}

@article{dingemanseArbitrarinessIconicitySystematicity2015,
  title = {Arbitrariness, {{Iconicity}}, and {{Systematicity}} in {{Language}}},
  author = {Dingemanse, Mark and Blasi, Damián E. and Lupyan, Gary and Christiansen, Morten H. and Monaghan, Padraic},
  date = {2015-10},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends Cogn. Sci. (Regul. Ed.)},
  volume = {19},
  pages = {603--615},
  issn = {1879-307X},
  doi = {10.1016/j.tics.2015.07.013},
  abstract = {The notion that the form of a word bears an arbitrary relation to its meaning accounts only partly for the attested relations between form and meaning in the languages of the world. Recent research suggests a more textured view of vocabulary structure, in which arbitrariness is complemented by iconicity (aspects of form resemble aspects of meaning) and systematicity (statistical regularities in forms predict function). Experimental evidence suggests these form-to-meaning correspondences serve different functions in language processing, development, and communication: systematicity facilitates category learning by means of phonological cues, iconicity facilitates word learning and communication by means of perceptuomotor analogies, and arbitrariness facilitates meaning individuation through distinctive forms. Processes of cultural evolution help to explain how these competing motivations shape vocabulary structure.},
  eprint = {26412098},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BYKW3H3D\\Dingemanse et al. - 2015 - Arbitrariness, Iconicity, and Systematicity in Lan.pdf},
  keywords = {arbitrariness,Cues,Humans,Iconicity,Language,Language Development,lexicon,Linguistics,Phonetics,Semantics,sound-symbolism,systematicity,vocabulary,Vocabulary},
  langid = {english},
  number = {10}
}

@article{dingemanseArbitrarinessIconicitySystematicity2015a,
  title = {Arbitrariness, {{Iconicity}}, and {{Systematicity}} in {{Language}}},
  author = {Dingemanse, Mark and Blasi, Damián E. and Lupyan, Gary and Christiansen, Morten H. and Monaghan, Padraic},
  date = {2015-10-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {19},
  pages = {603--615},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2015.07.013},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661315001771},
  urldate = {2019-12-03},
  abstract = {The notion that the form of a word bears an arbitrary relation to its meaning accounts only partly for the attested relations between form and meaning in the languages of the world. Recent research suggests a more textured view of vocabulary structure, in which arbitrariness is complemented by iconicity (aspects of form resemble aspects of meaning) and systematicity (statistical regularities in forms predict function). Experimental evidence suggests these form-to-meaning correspondences serve different functions in language processing, development, and communication: systematicity facilitates category learning by means of phonological cues, iconicity facilitates word learning and communication by means of perceptuomotor analogies, and arbitrariness facilitates meaning individuation through distinctive forms. Processes of cultural evolution help to explain how these competing motivations shape vocabulary structure.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SBDXKW63\\Dingemanse et al. - 2015 - Arbitrariness, Iconicity, and Systematicity in Lan.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AU3S92AN\\S1364661315001771.html},
  keywords = {arbitrariness,Iconicity,lexicon,sound-symbolism,systematicity,vocabulary},
  langid = {english},
  number = {10}
}

@article{drijversVisualContextEnhanced2017,
  title = {Visual {{Context Enhanced}}: {{The Joint Contribution}} of {{Iconic Gestures}} and {{Visible Speech}} to {{Degraded Speech Comprehension}}},
  shorttitle = {Visual {{Context Enhanced}}},
  author = {Drijvers, Linda and Özyürek, Asli},
  date = {2017-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  volume = {60},
  pages = {212--222},
  issn = {1092-4388, 1558-9102},
  doi = {10.1044/2016_JSLHR-H-16-0101},
  url = {http://pubs.asha.org/doi/10.1044/2016_JSLHR-H-16-0101},
  urldate = {2019-09-18},
  abstract = {Purpose: This study investigated whether and to what extent iconic co-speech gestures 7 8 contribute to information from visible speech to enhance degraded speech comprehension at 9 10 11 different levels of noise-vocoding. Previously, the contributions of these two visual 12 13 articulators to speech comprehension have only been studied separately. 14 15 16
Method: Twenty participants watched videos of an actress uttering an action verb and 17 18 completed a free-recall task. The videos were presented in three speech (2-band; 6-band For Peer Review 19 20 noise-vocoding; clear), three multimodal (Speech+Lips blurred; Speech+VisibleSpeech; 21 22 Speech+VisibleSpeech+Gesture) and two visual only conditions (VisibleSpeech; 23 24 25 VisibleSpeech+Gesture). 26 27 28
Results: Accuracy levels were higher when both visual articulators were present compared to 29 30 one or none. The enhancement effects of a) visible speech, b) gestural information on top of 31 32 visible speech and c) both visible speech and iconic gestures were larger in 6-band than 233 34 band noise-vocoding or visual only conditions. Gestural enhancement in 2-band noise35 36 37 vocoding did not differ from gestural enhancement in visual only conditions. 38 39 40
Conclusions: When perceiving degraded speech in a visual context, listeners benefit more 41 42},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\UTLHPE5D\\Drijvers and Özyürek - 2017 - Visual Context Enhanced The Joint Contribution of.pdf},
  langid = {english},
  number = {1}
}

@online{DynamicsMultipleSignalling,
  title = {Dynamics of Multiple Signalling Systems: Animal Communication in a World in Flux | {{Elsevier Enhanced Reader}}},
  url = {https://reader.elsevier.com/reader/sd/pii/S0169534709003450?token=5025236735D1260CACE1396EEDD537C42FB865B2301A20CC868105583859347FDC86D5B96C5CE1B9DEFE2046778DC01A},
  urldate = {2019-11-15},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\28XCG4GC\\S0169534709003450.html}
}

@online{DynamicsMultipleSignallinga,
  title = {Dynamics of Multiple Signalling Systems: Animal Communication in a World in Flux | {{Elsevier Enhanced Reader}}},
  shorttitle = {Dynamics of Multiple Signalling Systems},
  doi = {10.1016/j.tree.2009.11.003},
  url = {https://reader.elsevier.com/reader/sd/pii/S0169534709003450?token=5025236735D1260CACE1396EEDD537C42FB865B2301A20CC868105583859347FDC86D5B96C5CE1B9DEFE2046778DC01A},
  urldate = {2019-11-15},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4HW3XZPQ\\S0169534709003450.html},
  langid = {english}
}

@article{edelsbrunnerTopologicalPersistenceSimplification2002,
  title = {Topological {{Persistence}} and {{Simplification}}},
  author = {{Edelsbrunner} and {Letscher} and {Zomorodian}},
  date = {2002-11},
  journaltitle = {Discrete \& Computational Geometry},
  volume = {28},
  pages = {511--533},
  issn = {0179-5376, 1432-0444},
  doi = {10.1007/s00454-002-2885-2},
  url = {http://link.springer.com/10.1007/s00454-002-2885-2},
  urldate = {2020-01-06},
  abstract = {We formalize a notion of topological simplification within the framework of a filtration, which is the history of a growing complex. We classify a topological change that happens during growth as either a feature or noise depending on its lifetime or persistence within the filtration. We give fast algorithms for computing persistence and experimental evidence for their speed and utility.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9Z9IY3DU\\Edelsbrunner et al. - 2002 - Topological Persistence and Simplification.pdf},
  langid = {english},
  number = {4}
}

@article{ejiriCooccurencesPreverbalVocal2001,
  title = {Co-Occurences of Preverbal Vocal Behavior and Motor Action in Early Infancy},
  author = {Ejiri, Keiko and Masataka, Nobuo},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {40--48},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00147},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00147},
  urldate = {2020-01-26},
  abstract = {This study reports on co-occurrence of vocal behaviors and motor actions in infants in the prelinguistic stage. Four Japanese infants were studied longitudinally from the age of 6 months to 11 months. For all the infants, a 40 min sample was coded for each monthly period. The vocalizations produced by the infants co-occurred with their rhythmic actions with high frequency, particularly in the period preceding the onset of canonical babbling. Acoustical analysis was conducted on the vocalizations recorded before and after the period when co-occurrence took place most frequently. Among the vocalizations recorded in the period when co-occurrence appeared most frequently, those that co-occurred with rhythmic action had significantly shorter syllable duration and shorter formant-frequency transition duration compared with those that did not co-occur with rhythmic action. The rapid transitions and short syllables were similar to patterns of duration found in mature speech. The acoustic features remained even after co-occurrence disappeared. These findings suggest that co-occurrence of rhythmic action and vocal behavior may contribute to the infant’s acquisition of the ability to perform the rapid glottal and articulatory movements that are indispensable for spoken language acquisition.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7EB3ZUMA\\1467-7687.html},
  langid = {english},
  number = {1}
}

@article{ejiriRelationshipRhythmicBehavior1998,
  title = {Relationship between {{Rhythmic Behavior}} and {{Canonical Babbling}} in {{Infant Vocal Development}}},
  author = {Ejiri, Keiko},
  date = {1998},
  journaltitle = {Phonetica},
  volume = {55},
  pages = {226--237},
  issn = {1423-0321, 0031-8388},
  doi = {10.1159/000028434},
  url = {https://www.karger.com/Article/FullText/28434},
  urldate = {2020-01-26},
  abstract = {The onset of canonical babbling (CB) is a landmark event in infants’ vocal development for spoken language. Previous research has suggested that the onset of CB coincides with the peak period of rhythmic activities. To examine this phenomenon in detail, 28 Japanese infants (14 girls, 14 boys) were observed longitudinally from the age of 5 to 9 months. In the experimental sessions, an audible or an inaudible rattle was placed into a hand of each tested infant. Then the number of times that the infant shook the rattle was counted. In the observational sessions, infants’ spontaneous rhythmic activities under natural conditions were observed. The result shows that rhythmic activities reached their peak around the onset of CB. When the infants began to babble, they shook whichever rattle was in their hand, regardless of its audibility. After this period, they shook the audible rattles more frequently than the inaudible ones. These findings suggest that, around the onset of CB, infants learn to control their motor activities based on auditory feedback.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\N5V7PDX5\\Ejiri - 1998 - Relationship between Rhythmic Behavior and Canonic.pdf},
  langid = {english},
  number = {4}
}

@software{eklundBeeswarmBeeSwarm2016,
  title = {Beeswarm: {{The Bee Swarm Plot}}, an {{Alternative}} to {{Stripchart}}},
  shorttitle = {Beeswarm},
  author = {Eklund, Aron},
  date = {2016-04-25},
  url = {https://CRAN.R-project.org/package=beeswarm},
  urldate = {2019-04-23},
  abstract = {The bee swarm plot is a one-dimensional scatter plot like "stripchart", but with closely-packed, non-overlapping points.},
  version = {0.2.3}
}

@online{EmergenceDynamicalOrder,
  title = {Emergence of {{Dynamical Order}}: {{Synchronization Phenomena}} in {{Complex Systems}} - {{Susanna C}}. {{Manrubia}}, {{Alexander S}}. {{Mikhailov}}, {{Dam}}¡an {{H}}. {{Zannette}} - {{Google Books}}},
  url = {https://books.google.nl/books?id=w6RpDQAAQBAJ&pg=PA340&lpg=PA340&dq=pikovsky+emergence&source=bl&ots=uKWbVFXqRZ&sig=ACfU3U1zs5_xWCrVeyw3FXY_clGXvKTRCA&hl=en&sa=X&ved=2ahUKEwj5l6S516zjAhUrIMUKHW7QBCYQ6AEwAHoECAkQAQ#v=onepage&q=pikovsky%20emergence&f=false},
  urldate = {2019-07-11},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QRZ95AGX\\books.html}
}

@article{esteve-gibertProsodicStructureShapes2013,
  title = {Prosodic Structure Shapes the Temporal {{Realization}} of Intonation and Manual Gesture Movements},
  author = {Esteve-Gibert, N. and Prieto, P.},
  date = {2013-06-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {56},
  pages = {850--864},
  doi = {10.1044/1092-4388(2012/12-0049)},
  url = {https://jslhr.pubs.asha.org/doi/full/10.1044/1092-4388%282012/12-0049%29},
  urldate = {2019-04-16},
  abstract = {Purpose
      Previous work on the temporal coordination between gesture and speech found that the
         prominence in gesture coordinates with speech prominence. In this study, the authors
         investigated the anchoring regions in speech and pointing gesture that align with
         each other. The authors hypothesized that (a) in contrastive focus conditions, the
         gesture apex is anchored in the intonation peak and (b) the upcoming prosodic boundary
         influences the timing of gesture and intonation movements.
      
      
      Method
      Fifteen Catalan speakers pointed at a screen while pronouncing a target word with
         different metrical patterns in a contrastive focus condition and followed by a phrase
         boundary. A total of 702 co-speech deictic gestures were acoustically and gesturally
         analyzed.
      
      
      Results
      Intonation peaks and gesture apexes showed parallel behavior with respect to their
         position within the accented syllable: They occurred at the end of the accented syllable
         in non–phrase-final position, whereas they occurred well before the end of the accented
         syllable in phrase-final position. Crucially, the position of intonation peaks and
         gesture apexes was correlated and was bound by prosodic structure.
      
      
      Conclusions
      The results refine the phonological synchronization rule (McNeill, 1992), showing
         that gesture apexes are anchored in intonation peaks and that gesture and prosodic
         movements are bound by prosodic phrasing.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5K5WWP56\\Esteve-Gibert Núria and Prieto Pilar - 2013 - Prosodic Structure Shapes the Temporal Realization.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\QM79RK99\\12-0049).html},
  number = {3}
}

@article{esteve-gibertProsodyAuditoryVisual2018,
  title = {Prosody in the {{Auditory}} and {{Visual Domains}}: {{A Developmental Perspective}}},
  author = {Esteve-Gibert, N. and Guellai, Bahia},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  doi = {10.3389/fpsyg.2018.00338},
  url = {http://www.readcube.com/articles/10.3389/fpsyg.2018.00338},
  urldate = {2019-08-09},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\683BYBAJ\\fpsyg.2018.html}
}

@article{falandaysInteractionismLanguageNeural2018,
  title = {Interactionism in Language: {{From}} Neural Networks to Bodies to Dyads},
  shorttitle = {Interactionism in Language},
  author = {Falandays, J. Benjamin and Batzloff, Brandon J. and Spevack, Samuel C. and Spivey, Michael J.},
  date = {2018},
  journaltitle = {Language, Cognition and Neuroscience},
  pages = {No Pagination Specified-No Pagination Specified},
  issn = {2327-3801(Electronic),2327-3798(Print)},
  doi = {10.1080/23273798.2018.1501501},
  abstract = {In a science of language, it can be useful to partition different formats of linguistic information into different categories, such as phonetics, phonology, semantics, and syntax. However, when the actual phenomena of language processing cross those boundaries and blur those lines, it can become difficult to understand how these different formats of linguistic information maintain their integrity while engaging in complex interactions with one another. For example, if the function of a cortical network that is known to process phonetics is immediately taking into account contextual influences from a cortical network that is known to process semantics, then it seems clear that this “phonetic cortical network” is doing more than just phonetics. In the neuroscience and cognitive science of language, the scope of analysis where different formats of linguistic information are seen to interact reveals a wide array of context effects in almost every possible direction. When one expands the scope of analysis to include nonlinguistic sensory modalities, such as vision and action, research is showing that even those lines are getting blurred. Visual perception and motor movement appear to influence various aspects of language processing in real time. As this scope of analysis expands further still, research is showing that two human brains and bodies exhibit various forms of synchrony or coordination with one another during natural conversation. Interactionism at all these levels of analysis poses a challenge to traditional frameworks that treat different components of language, perception, and action as operating via domain specific computations. (PsycINFO Database Record (c) 2018 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7GGTYQQG\\2018-36198-001.html}
}

@article{ferrericanchoPatternsSyntacticDependency2004,
  title = {Patterns in Syntactic Dependency Networks},
  author = {Ferrer i Cancho, Ramon and Solé, Ricard V. and Köhler, Reinhard},
  date = {2004-05-26},
  journaltitle = {Physical Review E},
  volume = {69},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.69.051915},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.051915},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XC7Y9XLI\\Ferrer i Cancho et al. - 2004 - Patterns in syntactic dependency networks.pdf},
  langid = {english},
  number = {5}
}

@book{feyereisenCognitivePsychologySpeechRelated2017,
  title = {The {{Cognitive Psychology}} of {{Speech}}-{{Related Gesture}}},
  author = {Feyereisen, P.},
  date = {2017-07-28},
  publisher = {{Routledge}},
  location = {{New York}},
  abstract = {Why do we gesture when we speak? The Cognitive Psychology of Speech-Related Gesture offers answers to this question while introducing readers to the huge interdisciplinary field of gesture. Drawing on ideas from cognitive psychology, this book highlights key debates in gesture research alongside advocating new approaches to conventional thinking.  Beginning with the definition of the notion of communication, this book explores experimental approaches to gesture production and comprehension, the possible gestural origin of language and its implication for brain organization, and the development of gestural communication from infancy to childhood. Through these discussions the author presents the idea that speech-related gestures are not just peripheral phenomena, but rather a key function of the cognitive architecture, and should consequently be studied alongside traditional concepts in cognitive psychology.  The Cognitive Psychology of Speech Related Gesture offers a broad overview which will be essential reading for all students of gesture research and language, as well as speech therapists, teachers and communication practitioners. It will also be of interest to anybody who is curious about why we move our bodies when we talk.},
  eprint = {nJguDwAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-1-351-78827-4},
  keywords = {Psychology / Cognitive Psychology & Cognition,Psychology / General},
  langid = {english},
  pagetotal = {353}
}

@article{filippiTemporalModulationSpeech2019,
  title = {Temporal Modulation in Speech, Music, and Animal Vocal Communication: Evidence of Conserved Function},
  shorttitle = {Temporal Modulation in Speech, Music, and Animal Vocal Communication},
  author = {Filippi, Piera and Hoeschele, Marisa and Spierings, Michelle and Bowling, Daniel L.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  volume = {1453},
  pages = {99--113},
  issn = {1749-6632},
  doi = {10.1111/nyas.14228},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14228},
  urldate = {2019-10-22},
  abstract = {Speech is a distinctive feature of our species. It is the default channel for language and constitutes our primary mode of social communication. Determining the evolutionary origins of speech is a challenging prospect, in large part because it appears to be unique in the animal kingdom. However, direct comparisons between speech and other forms of acoustic communication, both in humans (music) and animals (vocalization), suggest that important components of speech are shared across domains and species. In this review, we focus on a single aspect of speech—temporal patterning—examining similarities and differences across speech, music, and animal vocalization. Additional structure is provided by focusing on three specific functions of temporal patterning across domains: (1) emotional expression, (2) social interaction, and (3) unit identification. We hypothesize an evolutionary trajectory wherein the ability to identify units within a continuous stream of vocal sounds derives from social vocal interaction, which, in turn, derives from vocal emotional communication. This hypothesis implies that unit identification has parallels in music and precursors in animal vocal communication. Accordingly, we demonstrate the potential of comparisons between fundamental domains of biological acoustic communication to provide insight into the evolution of language.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PWE2GTN9\\nyas.html},
  keywords = {emotion expression,language evolution,social interaction,temporal patterns,unit identification},
  langid = {english},
  number = {1}
}

@article{fowlerEmbodiedEmbeddedLanguage2010,
  title = {Embodied, {{Embedded Language Use}}},
  author = {Fowler, Carol A.},
  date = {2010-10-01},
  journaltitle = {Ecological psychology : a publication of the International Society for Ecological Psychology},
  volume = {22},
  pages = {286},
  issn = {10.1080/10407413.2010.517115},
  doi = {10.1080/10407413.2010.517115},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3020794/},
  urldate = {2019-05-04},
  abstract = {Language use has a public face that is as important to study as the private faces under intensive psycholinguistic study. In the domain of phonology, public use of speech must meet an interpersonal “parity” constraint if it is to serve ...},
  eprint = {21243080},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IL765TVU\\Fowler - 2010 - Embodied, Embedded Language Use.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\WLRZ6HFX\\PMC3020794.html},
  langid = {english},
  number = {4}
}

@article{fowlerEventApproachStudy1986,
  title = {An Event Approach to the Study of Speech Perception from a Direct-Realist Perspective},
  author = {Fowler, Carol A.},
  date = {1986},
  journaltitle = {Journal of Phonetics},
  volume = {14},
  pages = {3--28},
  issn = {1095-8576(Electronic),0095-4470(Print)},
  abstract = {Proposes an event approach to a theory of speech perception and speech production, focusing on the perception of speech events (i.e., a talker's phonetically structured articulations). In defining a speech event interchangeably from the perspectives of talkers and listeners, the author adopts a "direct realist" perspective: Perception is assumed to recover events in the real world. To do this, perception must be direct and unmediated by cognitive processes of inference or hypothesis testing. Barriers to the theory are outlined and evidence presented to refute them. Support for direct perception of local, short-term events and longer ones is discussed. Attention is also given to the way in which perception of a linguistic message guides a listener's behavior. Differences in the reliability of the information conveyed between direct and indirect perception and the implications for the relation between an utterance and what it signified are examined. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YUI62C42\\1987-23991-001.html},
  keywords = {Articulation (Speech),Phonetics,Speech Perception,Theories},
  number = {1}
}

@article{fowlerListeningEyeHand1991,
  title = {Listening with Eye and Hand: {{Cross}}-Modal Contributions to Speech Perception},
  shorttitle = {Listening with Eye and Hand},
  author = {Fowler, Carol A. and Dekle, Dawn J.},
  date = {1991},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {17},
  pages = {816--828},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.17.3.816},
  abstract = {Three experiments investigated the "McGurk effect" whereby optically specified syllables experienced synchronously with acoustically specified syllables integrate in perception to determine a listener's auditory perceptual experience. Experiments contrasted the cross-modal effect of orthographic on acoustic syllables presumed to be associated in experience and memory with that of haptically experienced and acoustic syllables presumed not to be associated. The latter pairing gave rise to cross-modal influences when Ss were informed that cross-modal syllables were paired independently. Mouthed syllables affected reports of simultaneously heard syllables (and vice versa). These effects were absent when syllables were simultaneously seen (spelled) and heard. The McGurk effect does not arise from association in memory but from conjoint near specification of the same casual source in the environment—in speech, the moving vocal tract producing phonetic gestures. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NRS9DDFB\\1992-00274-001.html},
  keywords = {Auditory Perception,Auditory Stimulation,Orthography,Speech Perception},
  number = {3}
}

@article{frohlichMultimodalCommunicationLanguage2019,
  title = {Multimodal Communication and Language Origins: Integrating Gestures and Vocalizations},
  shorttitle = {Multimodal Communication and Language Origins},
  author = {Fröhlich, Marlen and Sievers, Christine and Townsend, Simon W. and Gruber, Thibaud and van Schaik, Carel P.},
  date = {2019},
  journaltitle = {Biological Reviews},
  volume = {94},
  pages = {1809--1829},
  issn = {1469-185X},
  doi = {10.1111/brv.12535},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12535},
  urldate = {2019-12-09},
  abstract = {The presence of divergent and independent research traditions in the gestural and vocal domains of primate communication has resulted in major discrepancies in the definition and operationalization of cognitive concepts. However, in recent years, accumulating evidence from behavioural and neurobiological research has shown that both human and non-human primate communication is inherently multimodal. It is therefore timely to integrate the study of gestural and vocal communication. Herein, we review evidence demonstrating that there is no clear difference between primate gestures and vocalizations in the extent to which they show evidence for the presence of key language properties: intentionality, reference, iconicity and turn-taking. We also find high overlap in the neurobiological mechanisms producing primate gestures and vocalizations, as well as in ontogenetic flexibility. These findings confirm that human language had multimodal origins. Nonetheless, we note that in great apes, gestures seem to fulfil a carrying (i.e. predominantly informative) role in close-range communication, whereas the opposite holds for face-to-face interactions of humans. This suggests an evolutionary shift in the carrying role from the gestural to the vocal stream, and we explore this transition in the carrying modality. Finally, we suggest that future studies should focus on the links between complex communication, sociality and cooperative tendency to strengthen the study of language origins.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QP3HIK3A\\Fröhlich et al. - 2019 - Multimodal communication and language origins int.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8H8XMILJ\\brv.html},
  keywords = {cognition,comparative approach,evolution of language,gestural origins,learning,multimodality,ontogeny,primates,vocal origins},
  langid = {english},
  number = {5}
}

@article{fuchsAssessingRespiratoryContributions2015,
  title = {Assessing Respiratory Contributions to F0 Declination in {{German}} across Varying Speech Tasks and Respiratory Demands},
  author = {Fuchs, S. and Petrone, C. and Rochet-Capellan, A. and Reichel, W. D. and Koenig, L. L.},
  date = {2015},
  journaltitle = {Journal of Phonetics},
  volume = {52},
  pages = {35--45},
  doi = {10.1016/j.wocn.2015.04.002},
  url = {https://hal.archives-ouvertes.fr/hal-01164773},
  urldate = {2019-08-08},
  abstract = {Many past studies have sought to determine the factors that affect f0 declination, and the physiological underpinnings of the phenomenon.  This study assessed the relation between respiration and f0 declination by means of simultaneous acoustic and respiratory recordings from read and spontaneous speech from speakers of German. Within the respective Intonational Phrase unit, we analysed the effect of the number of syllables and voiceless obstruents. Both factors could influence the slope of either f0 declination or rib cage movement. If respiration and f0 declination are related physiologically, their relationship might also be modulated by either one or both factors. Our results show consistently for both speech tasks that the slope of the rib cage movement is not related with f0 declination when length and consonant content vary. Furthermore f0 slopes are generally shallower in spontaneous than in read speech. Finally, although a higher number of voiceless obstruents yielded a greater rib cage compression, it did not affect f0 declination. These results suggest that although f0 declination occurs in many languages, it might not have a purely physiological origin in breathing, but rather reflects cognitive processing which allows speakers to look ahead when planning their utterances.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\49ZD9E2Z\\Fuchs et al. - 2015 - Assessing respiratory contributions to f0 declinat.pdf},
  keywords = {f0 declination,number of syllables,reading,respiration,speech tasks,spontaneous speech,voiceless obstruents}
}

@article{fuchsExploringSourceShortterm2019,
  title = {Exploring the Source of Short-Term Variations in Respiratory Data},
  author = {Fuchs, Susanne and Koenig, Laura L. and Petrone, Caterina},
  date = {2019-01-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {145},
  pages = {EL66-EL71},
  issn = {0001-4966},
  doi = {10.1121/1.5087272},
  url = {https://asa.scitation.org/doi/10.1121/1.5087272},
  urldate = {2019-08-08},
  abstract = {This study explores short-term respiratory volume changes in German oral and nasal stops and discusses to what extent these changes may be explained by laryngeal-oral coordination. It is expected that respiratory volumes decrease more rapidly when the glottis and the vocal tract are open after the release of voiceless aspirated stops. Two experiments were performed using Inductance Plethysmography and acoustics, varying consonantal properties, loudness, and prosodic focus. Results show consistent differences in respiratory slopes between voiceless vs voiced and nasal stops, which are more extreme in a loud or focused position. Thus, respiratory changes can even occur at a local level.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FZBWF5AZ\\Fuchs et al. - 2019 - Exploring the source of short-term variations in r.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\7J4V352F\\1.html},
  number = {1}
}

@book{gallagherHowBodyShapes2005,
  title = {How the {{Body Shapes}} the {{Mind}}},
  author = {Gallagher, Shaun},
  date = {2005-01-27},
  publisher = {{Oxford University Press}},
  url = {http://www.oxfordscholarship.com/view/10.1093/0199271941.001.0001/acprof-9780199271948},
  urldate = {2019-04-16},
  abstract = {This book contributes to the idea that to have an understanding of the mind, consciousness, or cognition, a detailed scientific and phenomenological understanding of the body is essential. There is still a need to develop a common vocabulary that is capable of integrating discussions of brain mechanisms in neuroscience, behavioral expressions in psychology, design concerns in artificial intelligence and robotics, and debates about embodied experience in the phenomenology and philosophy of mind. This book helps to formulate this common vocabulary by developing a conceptual framework that avoids both the overly reductionistic approaches that explain everything in terms of bottom-up neuronal mechanisms, and the inflationistic approaches that explain everything in terms of Cartesian, top-down cognitive states. Through discussions of neonate imitation, the Molyneux problem, gesture, self-awareness, free will, social cognition and intersubjectivity, as well as pathologies such as deafferentation, unilateral neglect, phantom limb, autism and schizophrenia, the book proposes to remap the conceptual landscape by revitalizing the concepts of body image and body schema, proprioception, ecological experience, intermodal perception, and enactive concepts of ownership and agency for action. Informed by both philosophical theory and scientific evidence, it addresses two basic sets of questions that concern the structure of embodied experience. First, questions about the phenomenal aspects of that structure, specifically the relatively regular and constant phenomenal features found in the content of experience. Second, questions about aspects of the structure of consciousness that are more hidden, those that may be more difficult to get at because they happen before one knows it, and do not normally enter into the phenomenal content of experience in an explicit way.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Y5QC7YKC\\acprof-9780199271948.html},
  isbn = {978-0-19-160311-2},
  langid = {american}
}

@article{gardenforsDemonstrationPantomimeEvolution2017,
  title = {Demonstration and {{Pantomime}} in the {{Evolution}} of {{Teaching}}},
  author = {Gärdenfors, Peter},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {8},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2017.00415},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2017.00415/full},
  urldate = {2020-01-26},
  abstract = {Donald proposes that early Homo evolved mimesis as a new form of cognition. This article investigates the mimesis hypothesis in relation to the evolution of teaching. The capacities that distinguish hominin teaching from that of other animals are demonstration and pantomime. A conceptual analysis of the instructional and communicative functions of demonstration and pantomime is presented. Archaeological evidence that demonstration was used for transmitting the Oldowan technology is summarized. It is argued that pantomime develops out of demonstration so that the primary objective of pantomime is that the onlooker learns the motoric patterns shown in the pantomime. The communicative use of pantomime is judged to be secondary. This use of pantomime is also contrasted with other forms of gestures. A key feature of the analysis is that the meaning of a pantomime is characterized by the force patterns of the movements. These force patterns form the core of a model of the cognitive mechanism behind pantomime. Finally, the role of pantomime in the evolution of language is also discussed.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\R5WJ35IA\\Gärdenfors - 2017 - Demonstration and Pantomime in the Evolution of Te.pdf},
  keywords = {Demonstration,evolution of language,Gesture,mental simulation,Mimesis,pantomime,Teaching},
  langid = {english}
}

@article{garrodJointActionInteractive2009,
  title = {Joint {{Action}}, {{Interactive Alignment}}, and {{Dialog}}},
  author = {Garrod, Simon and Pickering, Martin J.},
  date = {2009},
  journaltitle = {Topics in Cognitive Science},
  volume = {1},
  pages = {292--304},
  issn = {1756-8765},
  doi = {10.1111/j.1756-8765.2009.01020.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-8765.2009.01020.x},
  urldate = {2019-06-24},
  abstract = {Dialog is a joint action at different levels. At the highest level, the goal of interlocutors is to align their mental representations. This emerges from joint activity at lower levels, both concerned with linguistic decisions (e.g., choice of words) and nonlinguistic processes (e.g., alignment of posture or speech rate). Because of the high-level goal, the interlocutors are particularly concerned with close coupling at these lower levels. As we illustrate with examples, this means that imitation and entrainment are particularly pronounced during interactive communication. We then argue that the mechanisms underlying such processes involve covert imitation of interlocutors’ communicative behavior, leading to emulation of their expected behavior. In other words, communication provides a very good example of predictive emulation, in a way that leads to successful joint activity.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EQUU2PFQ\\Garrod and Pickering - 2009 - Joint Action, Interactive Alignment, and Dialog.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\UK5T6L9Y\\j.1756-8765.2009.01020.html},
  keywords = {Dialog,Emulation,Interactive alignment,Joint action,Prediction},
  langid = {english},
  number = {2}
}

@article{ghazanfarEvolutionHumanVocal2008,
  title = {Evolution of Human Vocal Production},
  author = {Ghazanfar, Asif A. and Rendall, Drew},
  date = {2008-06-03},
  journaltitle = {Current biology: CB},
  shortjournal = {Curr. Biol.},
  volume = {18},
  pages = {R457-460},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2008.03.030},
  eprint = {18522811},
  eprinttype = {pmid},
  keywords = {Animals,Biological Evolution,Humans,Larynx,Neocortex,Primates,Respiratory Mechanics,Speech,Speech Perception,Thorax,Vocalization; Animal},
  langid = {english},
  number = {11}
}

@article{ghazanfarMultisensoryVocalCommunication2013,
  title = {Multisensory Vocal Communication in Primates and the Evolution of Rhythmic Speech},
  author = {Ghazanfar, Asif A.},
  date = {2013-09-01},
  journaltitle = {Behavioral ecology and sociobiology},
  shortjournal = {Behav Ecol Sociobiol},
  volume = {67},
  issn = {0340-5443},
  doi = {10.1007/s00265-013-1491-z},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3821777/},
  urldate = {2019-10-13},
  abstract = {The integration of the visual and auditory modalities during human speech perception is the default mode of speech processing. That is, visual speech perception is not a capacity that is “piggybacked” on to auditory-only speech perception. Visual information from the mouth and other parts of the face is used by all perceivers to enhance auditory speech. This integration is ubiquitous and automatic and is similar across all individuals across all cultures. The two modalities seem to be integrated even at the earliest stages of human cognitive development. If multisensory speech is the default mode of perception, then this should be reflected in the evolution of vocal communication. The purpose of this review is to describe the data that reveal that human speech is not uniquely multisensory. In fact, the default mode of communication is multisensory in nonhuman primates as well but perhaps emerging with a different developmental trajectory. Speech production, however, exhibits a unique bimodal rhythmic structure in that both the acoustic output and the movements of the mouth are rhythmic and tightly correlated. This structure is absent in most monkey vocalizations. One hypothesis is that the bimodal speech rhythm may have evolved through the rhythmic facial expressions of ancestral primates, as indicated by mounting comparative evidence focusing on the lip-smacking gesture.},
  eprint = {24222931},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IN8HWBHH\\Ghazanfar - 2013 - Multisensory vocal communication in primates and t.pdf},
  number = {9},
  pmcid = {PMC3821777}
}

@article{ghazanfarVocaltractResonancesIndexical2007,
  title = {Vocal-Tract Resonances as Indexical Cues in Rhesus Monkeys},
  author = {Ghazanfar, Asif A. and Turesson, Hjalmar K. and Maier, Joost X. and van Dinther, Ralph and Patterson, Roy D. and Logothetis, Nikos K.},
  date = {2007-03-06},
  journaltitle = {Current Biology},
  shortjournal = {Curr Biol},
  volume = {17},
  pages = {425--430},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2007.01.029},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2361420/},
  urldate = {2019-10-17},
  abstract = {Vocal-tract resonances (or formants) are acoustic signatures in the voice and are related to the shape and length of the vocal tract. Formants play an important role in human communication, helping us not only to distinguish several different speech sounds , but also to extract important information related to the physical characteristics of the speaker, so-called indexical cues. How did formants come to play such an important role in human vocal communication? One hypothesis suggests that the ancestral role of formant perception—a role that might be present in extant nonhuman primates—was to provide indexical cues . Although formants are present in the acoustic structure of vowel-like calls of monkeys  and implicated in the discrimination of call types , it is not known whether they use this feature to extract indexical cues. Here, we investigate whether rhesus monkeys can use the formant structure in their “coo” calls to assess the age-related body size of conspecifics. Using a preferential-looking paradigm  and synthetic coo calls in which formant structure simulated an adult/large- or juvenile/small-sounding individual, we demonstrate that untrained monkeys attend to formant cues and link large-sounding coos to large faces and small-sounding coos to small faces—in essence, they can, like humans , use formants as indicators of age-related body size.},
  eprint = {17320389},
  eprinttype = {pmid},
  number = {5-2},
  options = {useprefix=true},
  pmcid = {PMC2361420}
}

@inproceedings{ginosarLearningIndividualStyles2019,
  title = {Learning Individual Styles of Conversational Gesture},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ginosar, S. and Bar, A. and Kohavi, G. and Chan, C. and Owens, A. and Malik, J.},
  date = {2019},
  pages = {3497--3506},
  url = {https://arxiv.org/abs/1906.04160},
  urldate = {2019-08-09},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LYKM7A5J\\1906.html}
}

@article{giorginoComputingVisualizingDynamic2009,
  title = {Computing and {{Visualizing Dynamic Time Warping Alignments}} in {{{\emph{R}}}} : {{The}} {\textbf{Dtw}} {{Package}}},
  shorttitle = {Computing and {{Visualizing Dynamic Time Warping Alignments}} in {{{\emph{R}}}}},
  author = {Giorgino, Toni},
  date = {2009},
  journaltitle = {Journal of Statistical Software},
  volume = {31},
  issn = {1548-7660},
  doi = {10.18637/jss.v031.i07},
  url = {http://www.jstatsoft.org/v31/i07/},
  urldate = {2019-08-07},
  abstract = {This introduction to the R package dtw is a (slightly) modified version of Giorgino (2009), published in the Journal of Statistical Software. Dynamic time warping is a popular technique for comparing time series, providing both a distance measure that is insensitive to local compression and stretches and the warping which optimally deforms one of the two input series onto the other. A variety of algorithms and constraints have been discussed in the literature. The dtw package provides an unification of them; it allows R users to compute time series alignments mixing freely a variety of continuity constraints, restriction windows, endpoints, local distance definitions, and so on. The package also provides functions for visualizing alignments and constraints using several classic diagram types.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HBYA9Z4B\\Giorgino - 2009 - Computing and Visualizing Dynamic Time Warping Ali.pdf},
  langid = {english},
  number = {7}
}

@article{giraudCorticalOscillationsSpeech2012,
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  shorttitle = {Cortical Oscillations and Speech Processing},
  author = {Giraud, Anne-Lise and Poeppel, David},
  date = {2012-04},
  journaltitle = {Nature Neuroscience},
  volume = {15},
  pages = {511--517},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3063},
  url = {http://www.nature.com/articles/nn.3063},
  urldate = {2019-08-30},
  abstract = {Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example, by segregating information and organizing spike timing. Recent data show that delta, theta and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. We argue that they are foundational in speech and language processing, ‘packaging’ incoming information into units of the appropriate temporal granularity. Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\B5CYUJLD\\Giraud and Poeppel - 2012 - Cortical oscillations and speech processing emerg.pdf},
  langid = {english},
  number = {4}
}

@article{goldin-meadowGestureSignLanguage2017,
  title = {Gesture, Sign, and Language: {{The}} Coming of Age of Sign Language and Gesture Studies},
  shorttitle = {Gesture, Sign, and Language},
  author = {Goldin-Meadow, Susan and Brentari, Diane},
  date = {2017-01},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  volume = {40},
  pages = {e46},
  issn = {1469-1825},
  doi = {10.1017/S0140525X15001247},
  abstract = {How does sign language compare with gesture, on the one hand, and spoken language on the other? Sign was once viewed as nothing more than a system of pictorial gestures without linguistic structure. More recently, researchers have argued that sign is no different from spoken language, with all of the same linguistic structures. The pendulum is currently swinging back toward the view that sign is gestural, or at least has gestural components. The goal of this review is to elucidate the relationships among sign language, gesture, and spoken language. We do so by taking a close look not only at how sign has been studied over the past 50 years, but also at how the spontaneous gestures that accompany speech have been studied. We conclude that signers gesture just as speakers do. Both produce imagistic gestures along with more categorical signs or words. Because at present it is difficult to tell where sign stops and gesture begins, we suggest that sign should not be compared with speech alone but should be compared with speech-plus-gesture. Although it might be easier (and, in some cases, preferable) to blur the distinction between sign and gesture, we argue that distinguishing between sign (or speech) and gesture is essential to predict certain types of learning and allows us to understand the conditions under which gesture takes on properties of sign, and speech takes on properties of gesture. We end by calling for new technology that may help us better calibrate the borders between sign and gesture.},
  eprint = {26434499},
  eprinttype = {pmid},
  keywords = {categorical,gesture-speech mismatch,gradient,homesign,imagistic,learning,morphology,phonology,syntax},
  langid = {english},
  pmcid = {PMC4821822}
}

@article{goldin-meadowNaturalOrderEvents2008,
  title = {The Natural Order of Events: {{How}} Speakers of Different Languages Represent Events Nonverbally},
  shorttitle = {The Natural Order of Events},
  author = {Goldin-Meadow, Susan and So, Wing Chee and Özyürek, Aslı and Mylander, Carolyn},
  date = {2008-07-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {105},
  pages = {9163--9168},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0710060105},
  url = {https://www.pnas.org/content/105/27/9163},
  urldate = {2019-08-26},
  abstract = {To test whether the language we speak influences our behavior even when we are not speaking, we asked speakers of four languages differing in their predominant word orders (English, Turkish, Spanish, and Chinese) to perform two nonverbal tasks: a communicative task (describing an event by using gesture without speech) and a noncommunicative task (reconstructing an event with pictures). We found that the word orders speakers used in their everyday speech did not influence their nonverbal behavior. Surprisingly, speakers of all four languages used the same order and on both nonverbal tasks. This order, actor–patient–act, is analogous to the subject–object–verb pattern found in many languages of the world and, importantly, in newly developing gestural languages. The findings provide evidence for a natural order that we impose on events when describing and reconstructing them nonverbally and exploit when constructing language anew.},
  eprint = {18599445},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\L9ZXNEJ5\\Goldin-Meadow et al. - 2008 - The natural order of events How speakers of diffe.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\54FTX72G\\9163.html},
  keywords = {gesture,language genesis,sign language,word order},
  langid = {english},
  number = {27}
}

@article{gordonMultimodalCommunicationWolf2011,
  title = {Multimodal Communication of Wolf Spiders on Different Substrates: Evidence for Behavioural Plasticity},
  shorttitle = {Multimodal Communication of Wolf Spiders on Different Substrates},
  author = {Gordon, Shira D. and Uetz, George W.},
  date = {2011-02},
  journaltitle = {Animal Behaviour},
  volume = {81},
  pages = {367--375},
  issn = {00033472},
  doi = {10.1016/j.anbehav.2010.11.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0003347210004409},
  urldate = {2019-09-17},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YUMBQQ6V\\Gordon and Uetz - 2011 - Multimodal communication of wolf spiders on differ.pdf},
  langid = {english},
  number = {2}
}

@article{guskiAcousticTauEasy1992,
  title = {Acoustic {{Tau}}: {{An Easy Analogue}} to {{Visual Tau}}?},
  shorttitle = {Acoustic {{Tau}}},
  author = {Guski, Rainer},
  date = {1992-09-01},
  journaltitle = {Ecological Psychology},
  volume = {4},
  pages = {189--197},
  issn = {1040-7413},
  doi = {10.1207/s15326969eco0403_4},
  url = {https://doi.org/10.1207/s15326969eco0403_4},
  urldate = {2019-05-04},
  abstract = {In an earlier article in this journal, B. Shaw, McGowan, and Turvey (1991) resented an acoustic variable they supposed would specify the time to contact with a sound-emitting source moving rectilinearly toward an observer at constant speed. Their formulation, t = 2l(dI/dt), follows Lee's (1976) main ideas in his derivation of optical tau for small visual angles. In this article I compare the functions of vision and hearing more generally and consider what information vision and hearing would use in normal circumstances. In this context, one may then ask a more specific question, Which kind of auditory information might be used about impending collisions? I make the following points: (a) The specification analysis of Shaw et al. (1991) is a substantial contribution to the growing field of ecological acoustics, (b) the idea that a more general attack on the acoustic guidance of action (Shaw et al., 1991, p. 254) starts with a proposal for an uncommon case without considering the general functions of seeing and hearing first seems inconsistent, (c) an auditory variable specifying time to turn or time to jump is needed, and (d) I do not believe that the auditory system can use the kind of information proposed by Shaw et al. (1991) in estimating time to contact.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TANWDUB2\\s15326969eco0403_4.html},
  number = {3}
}

@article{gustisonVocalLocomotorCoordination2019,
  title = {Vocal and Locomotor Coordination Develops in Association with the Autonomic Nervous System},
  author = {Gustison, Morgan L and Borjon, Jeremy I and Takahashi, Daniel Y and Ghazanfar, Asif A},
  editor = {Tchernichovski, Ofer and Calabrese, Ronald L and Goller, Franz},
  date = {2019-07-16},
  journaltitle = {eLife},
  volume = {8},
  pages = {e41853},
  issn = {2050-084X},
  doi = {10.7554/eLife.41853},
  url = {https://doi.org/10.7554/eLife.41853},
  urldate = {2019-10-17},
  abstract = {In adult animals, movement and vocalizations are coordinated, sometimes facilitating, and at other times inhibiting, each other. What is missing is how these different domains of motor control become coordinated over the course of development. We investigated how postural-locomotor behaviors may influence vocal development, and the role played by physiological arousal during their interactions. Using infant marmoset monkeys, we densely sampled vocal, postural and locomotor behaviors and estimated arousal fluctuations from electrocardiographic measures of heart rate. We found that vocalizations matured sooner than postural and locomotor skills, and that vocal-locomotor coordination improved with age and during elevated arousal levels. These results suggest that postural-locomotor maturity is not required for vocal development to occur, and that infants gradually improve coordination between vocalizations and body movement through a process that may be facilitated by arousal level changes.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9PA6LWLP\\Gustison et al. - 2019 - Vocal and locomotor coordination develops in assoc.pdf},
  keywords = {energetics,locomotion,marmoset monkey,motor development,vocal ontogeny}
}

@article{hagoortNeurobiologyLanguageSingleword2019,
  title = {The Neurobiology of Language beyond Single-Word Processing},
  author = {Hagoort, Peter},
  date = {2019-10-04},
  journaltitle = {Science},
  volume = {366},
  pages = {55--58},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax0289},
  url = {https://science.sciencemag.org/content/366/6461/55},
  urldate = {2019-10-14},
  abstract = {In this Review, I propose a multiple-network view for the neurobiological basis of distinctly human language skills. A much more complex picture of interacting brain areas emerges than in the classical neurobiological model of language. This is because using language is more than single-word processing, and much goes on beyond the information given in the acoustic or orthographic tokens that enter primary sensory cortices. This requires the involvement of multiple networks with functionally nonoverlapping contributions.},
  eprint = {31604301},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IGUFE222\\55.html},
  langid = {english},
  number = {6461}
}

@article{hansonEffectsObstruentConsonants2009,
  title = {Effects of Obstruent Consonants on Fundamental Frequency at Vowel Onset in {{English}}},
  author = {Hanson, Helen M.},
  date = {2009-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {125},
  pages = {425--441},
  issn = {1520-8524},
  doi = {10.1121/1.3021306},
  abstract = {When a vowel follows an obstruent, the fundamental frequency in the first few tens of milliseconds of the vowel is known to be influenced by the voicing characteristics of the consonant. This influence was re-examined in the study reported here. Stops, fricatives, and the nasal /m/ were paired with the vowels /i,a/ to form CVm syllables. Target syllables were embedded in carrier sentences, and intonation was varied to produce each syllable in either a high, low, or neutral pitch environment. In a high-pitch environment, F0 following voiceless obstruents is significantly increased relative to the baseline /m/, but following voiced obstruents it closely traces the baseline. In a low-pitch environment, F0 is very slightly increased following all obstruents, voiced and unvoiced. It is suggested that for certain pitch environments a conflict can occur between gestures corresponding to the segmental feature [stiff vocal folds] and intonational elements. The results are different acoustic manifestations of [stiff] in different pitch environments. The spreading of the vocal folds that occurs during unvoiced stops in certain contexts in English is an enhancing gesture, which aids the resolution of the gestural conflict by allowing the defining segmental gesture to be weakened without losing perceptual salience.},
  eprint = {19173428},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\WIKTF2GU\\Hanson - 2009 - Effects of obstruent consonants on fundamental fre.pdf},
  keywords = {Female,Humans,Linguistics,Male,Phonetics,Pitch Perception},
  langid = {english},
  number = {1},
  pmcid = {PMC2677272}
}

@article{hardusToolUseWild2009,
  title = {Tool Use in Wild Orang-Utans Modifies Sound Production: A Functionally Deceptive Innovation?},
  shorttitle = {Tool Use in Wild Orang-Utans Modifies Sound Production},
  author = {Hardus, M. E. and Lameira, A. R. and Schaik, C. S. and Wich, S. A.},
  date = {2009-10-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {276},
  pages = {3689--3694},
  doi = {10.1098/rspb.2009.1027},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2009.1027},
  urldate = {2019-05-04},
  abstract = {Culture has long been assumed to be uniquely human but recent studies, in particular on great apes, have suggested that cultures also occur in non-human primates. The most apparent cultural behaviours in great apes involve tools in the subsistence context where they are clearly functional to obtain valued food. On the other hand, tool-use to modify acoustic communication has been reported only once and its function has not been investigated. Thus, the question whether this is an adaptive behaviour remains open, even though evidence indicates that it is socially transmitted (i.e. cultural). Here we report on wild orang-utans using tools to modulate the maximum frequency of one of their sounds, the kiss squeak, emitted in distress. In this variant, orang-utans strip leaves off a twig and hold them to their mouth while producing a kiss squeak. Using leaves as a tool lowers the frequency of the call compared to a kiss squeak without leaves or with only a hand to the mouth. If the lowering of the maximum frequency functions in orang-utans as it does in other animals, two predictions follow: (i) kiss squeak frequency is related to body size and (ii) the use of leaves will occur in situations of most acute danger. Supporting these predictions, kiss squeaks without tools decreased with body size and kiss squeaks with leaves were only emitted by highly distressed individuals. Moreover, we found indications that the calls were under volitional control. This finding is significant for at least two reasons. First, although few animal species are known to deceptively lower the maximum frequency of their calls to exaggerate their perceived size to the listener (e.g. vocal tract elongation in male deer) it has never been reported that animals may use tools to achieve this, or that they are primates. Second, it shows that the orang-utan culture extends into the communicative domain, thus challenging the traditional assumption that primate calling behaviour is overall purely emotional.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AMGVVAKC\\Hardus Madeleine E. et al. - 2009 - Tool use in wild orang-utans modifies sound produc.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\PSELNL5N\\rspb.2009.html},
  number = {1673}
}

@article{harrisonHorsingSpontaneousFourLegged2009,
  title = {Horsing {{Around}}: {{Spontaneous Four}}-{{Legged Coordination}}},
  shorttitle = {Horsing {{Around}}},
  author = {Harrison, Steven J. and Richardson, Michael J.},
  date = {2009-11-06},
  journaltitle = {Journal of Motor Behavior},
  volume = {41},
  pages = {519--524},
  issn = {0022-2895},
  doi = {10.3200/35-08-014},
  url = {https://doi.org/10.3200/35-08-014},
  urldate = {2019-05-07},
  abstract = {Motivated by previous research suggesting that informational and mechanical interlimb coupling can stabilize rhythmic movement patterns, the authors show that stable 4-legged patterns between 2 individuals, either walking or running, can emerge unintentionally from simple forms of coupling. Specifically, they show that the leg movements of pairs of naive individuals become spontaneously phase locked when visually or mechanically coupled via a foam appendage. Analysis of each of the phase locked trials revealed distinct preferences for particular 4-legged patterns, with interpersonal in- and anti-phase coordination patterns (equitable with quadruped pace and trot, respectively) observed almost exclusively. Preference for either pattern depended on the strength of coupling. The authors discuss these findings in light of previous claims that the patterns of human and animal locomotion—as well as coordinated movements in general—can emerge from lawful coupling relations that exist between the subcomponents of perceptual-motor systems.},
  eprint = {19567365},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AIALRCKZ\\Harrison and Richardson - 2009 - Horsing Around Spontaneous Four-Legged Coordinati.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\NWAQA7S9\\35-08-014.html},
  keywords = {coordination dynamics,interpersonal coordination,mechanical coupling,quadruped gait,visual coupling},
  number = {6}
}

@article{hasselmanClassifyingAcousticSignals2015,
  title = {Classifying Acoustic Signals into Phoneme Categories: Average and Dyslexic Readers Make Use of Complex Dynamical Patterns and Multifractal Scaling Properties of the Speech Signal},
  shorttitle = {Classifying Acoustic Signals into Phoneme Categories},
  author = {Hasselman, Fred},
  date = {2015-03-26},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  volume = {3},
  pages = {e837},
  issn = {2167-8359},
  doi = {10.7717/peerj.837},
  url = {https://peerj.com/articles/837},
  urldate = {2019-04-05},
  abstract = {Several competing aetiologies of developmental dyslexia suggest that the problems with acquiring literacy skills are causally entailed by low-level auditory and/or speech perception processes. The purpose of this study is to evaluate the diverging claims about the specific deficient peceptual processes under conditions of strong inference. Theoretically relevant acoustic features were extracted from a set of artificial speech stimuli that lie on a /bAk/-/dAk/ continuum. The features were tested on their ability to enable a simple classifier (Quadratic Discriminant Analysis) to reproduce the observed classification performance of average and dyslexic readers in a speech perception experiment. The ‘classical’ features examined were based on component process accounts of developmental dyslexia such as the supposed deficit in Envelope Rise Time detection and the deficit in the detection of rapid changes in the distribution of energy in the frequency spectrum (formant transitions). Studies examining these temporal processing deficit hypotheses do not employ measures that quantify the temporal dynamics of stimuli. It is shown that measures based on quantification of the dynamics of complex, interaction-dominant systems (Recurrence Quantification Analysis and the multifractal spectrum) enable QDA to classify the stimuli almost identically as observed in dyslexic and average reading participants. It seems unlikely that participants used any of the features that are traditionally associated with accounts of (impaired) speech perception. The nature of the variables quantifying the temporal dynamics of the speech stimuli imply that the classification of speech stimuli cannot be regarded as a linear aggregate of component processes that each parse the acoustic signal independent of one another, as is assumed by the ‘classical’ aetiologies of developmental dyslexia. It is suggested that the results imply that the differences in speech perception performance between average and dyslexic readers represent a scaled continuum rather than being caused by a specific deficient component.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\G5RLG5YL\\Hasselman - 2015 - Classifying acoustic signals into phoneme categori.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\CG3LHAFN\\837.html},
  langid = {english}
}

@software{hastieGamGeneralizedAdditive2019,
  title = {Gam: {{Generalized Additive Models}}},
  shorttitle = {Gam},
  author = {Hastie, Trevor},
  date = {2019-07-03},
  url = {https://CRAN.R-project.org/package=gam},
  urldate = {2020-01-27},
  abstract = {Functions for fitting and working with generalized additive models, as described in chapter 7 of "Statistical Models in S" (Chambers and Hastie (eds), 1991), and "Generalized Additive Models" (Hastie and Tibshirani, 1990).},
  keywords = {Econometrics,Environmetrics,SocialSciences},
  version = {1.16.1}
}

@article{healeyRunningRepairsCoordinating2018,
  title = {Running {{Repairs}}: {{Coordinating Meaning}} in {{Dialogue}}},
  shorttitle = {Running {{Repairs}}},
  author = {Healey, Patrick G. T. and Mills, Gregory J. and Eshghi, Arash and Howes, Christine},
  date = {2018},
  journaltitle = {Topics in Cognitive Science},
  volume = {10},
  pages = {367--388},
  issn = {1756-8765},
  doi = {10.1111/tops.12336},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12336},
  urldate = {2020-01-22},
  abstract = {People give feedback in conversation: both positive signals of understanding, such as nods, and negative signals of misunderstanding, such as frowns. How do signals of understanding and misunderstanding affect the coordination of language use in conversation? Using a chat tool and a maze-based reference task, we test two experimental manipulations that selectively interfere with feedback in live conversation: (a) “Attenuation” that replaces positive signals of understanding such as “right” or “okay” with weaker, more provisional signals such as “errr” or “umm” and (2) “Amplification” that replaces relatively specific signals of misunderstanding from clarification requests such as “on the left?” with generic signals of trouble such as “huh?” or “eh?”. The results show that Amplification promotes rapid convergence on more systematic, abstract ways of describing maze locations while Attenuation has no significant effect. We interpret this as evidence that “running repairs”—the processes of dealing with misunderstandings on the fly—are key drivers of semantic coordination in dialogue. This suggests a new direction for experimental work on conversation and a productive way to connect the empirical accounts of Conversation Analysis with the representational and processing concerns of Formal Semantics and Psycholinguistics.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AS38SYQX\\Healey et al. - 2018 - Running Repairs Coordinating Meaning in Dialogue.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\YNE7349C\\tops.html},
  keywords = {Dialogue,Miscommunication,Repair},
  langid = {english},
  number = {2}
}

@inproceedings{heAmplitudeEnvelopeKinematics2017,
  title = {Amplitude Envelope Kinematics of Speech Signal: Parameter Extraction and Applications},
  booktitle = {Konferenz {{Elektronische Sprachsignalverarbeitung}}},
  date = {2017},
  pages = {107--113},
  location = {{Saarbrücken}},
  editora = {He, L. and Dellwo, V.},
  editoratype = {collaborator}
}

@article{heAmplitudeEnvelopeKinematics2017a,
  title = {Amplitude Envelope Kinematics of Speech: {{Parameter}} Extraction and Applications},
  shorttitle = {Amplitude Envelope Kinematics of Speech},
  author = {He, Lei and Dellwo, Volker},
  date = {2017-05-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {141},
  pages = {3582--3582},
  issn = {0001-4966},
  doi = {10.1121/1.4987638},
  url = {https://asa.scitation.org/doi/10.1121/1.4987638},
  urldate = {2019-05-05},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\96CYPDK7\\1.html},
  number = {5}
}

@online{HierarchyAutonomousSystems,
  title = {A {{Hierarchy}} of {{Autonomous Systems}} for {{Vocal Production}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.tins.2019.12.006},
  url = {https://reader.elsevier.com/reader/sd/pii/S0166223619302401?token=944D77B8B1ECDE0BE1D2E949C7ED1446036B5D3B1B7C62D1E7F61AE7791DFA5D78C530C05B641392085395272CECB253},
  urldate = {2020-01-22},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TCMUAANF\\S0166223619302401.html},
  langid = {english}
}

@article{holderiedEcholocationRangeWingbeat2003,
  title = {Echolocation Range and Wingbeat Period Match in Aerial-Hawking Bats},
  author = {Holderied, M. W. and von Helversen, O.},
  date = {2003-11-07},
  journaltitle = {Proceedings. Biological Sciences},
  shortjournal = {Proc. Biol. Sci.},
  volume = {270},
  pages = {2293--2299},
  issn = {0962-8452},
  doi = {10.1098/rspb.2003.2487},
  abstract = {Aerial-hawking bats searching the sky for prey face the problem that flight and echolocation exert independent and possibly conflicting influences on call intervals. These bats can only exploit their full echolocation range unambiguously if they emit their next call when all echoes from the preceding call would have arrived. However, not every call interval is equally available. The need to reduce the high energetic costs of echolocation forces aerial-hawking bats to couple call emission to their wingbeat. We compared the wingbeat periods of 11 aerial-hawking bat species with the delays of the last-expected echoes. Acoustic flight-path tracking was employed to measure the source levels (SLs) of echolocation calls in the field. SLs were very high, extending the known range to 133 dB peak equivalent sound pressure level. We calculated the maximum detection distances for insects, larger flying objects and background targets. Wingbeat periods were derived from call intervals. Small and medium-sized bats in fact matched their maximum detection range for insects and larger flying targets to their wingbeat period. The tendency to skip calls correlated with the species' detection range for background targets. We argue that a species' call frequency is at such a pitch that the resulting detection range matches their wingbeat period.},
  eprint = {14613617},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TQFN5WZX\\Holderied and von Helversen - 2003 - Echolocation range and wingbeat period match in ae.pdf},
  keywords = {Animals,Auditory Perception,Chiroptera,Echolocation,Flight; Animal,Predatory Behavior,Time Factors,Wings; Animal},
  langid = {english},
  number = {1530},
  options = {useprefix=true},
  pmcid = {PMC1691500}
}

@article{hollerMultimodalLanguageProcessing2019,
  title = {Multimodal Language Processing in Human Communication},
  author = {Holler, Judith and Levinson, Stephen C.},
  date = {2019-08-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  pages = {639--652},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2019.05.006},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661319301299},
  urldate = {2019-08-26},
  abstract = {The natural ecology of human language is face-to-face interaction comprising the exchange of a plethora of multimodal signals. Trying to understand the psycholinguistic processing of language in its natural niche raises new issues, first and foremost the binding of multiple, temporally offset signals under tight time constraints posed by a turn-taking system. This might be expected to overload and slow our cognitive system, but the reverse is in fact the case. We propose cognitive mechanisms that may explain this phenomenon and call for a multimodal, situated psycholinguistic framework to unravel the full complexities of human language processing.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\2JPUT5J2\\Holler and Levinson - 2019 - Multimodal Language Processing in Human Communicat.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\LY3C7FWG\\S1364661319301299.html},
  keywords = {binding,cross-level prediction,multimodal gestalts,multimodal language,segregation},
  number = {8}
}

@article{hospelhornMethodAnalyzingGestural2017,
  title = {Method for {{Analyzing Gestural Communication}} in {{Musical Groups}}},
  author = {Hospelhorn, Emma and Radinsky, Josh},
  date = {2017-10-03},
  journaltitle = {Discourse Processes},
  volume = {54},
  pages = {504--523},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2015.1137183},
  url = {https://doi.org/10.1080/0163853X.2015.1137183},
  urldate = {2019-09-20},
  abstract = {Musical performances provide a rich context for studying complex spatial and embodied modes of group learning. This article proposes a framework for analyzing gesture in musical performances to highlight the ways musicians' movements reflect and promote their emerging and changing conceptions of a piece of music. The constructs of expressive musical gesture (at the individual level of analysis) and group expressive musical gesture (at the collective level) are used to analyze a series of five sequential performances of a musical passage by a string quartet during rehearsal. The analysis identifies three functions of embodied gesture for score interpretation: (1) gestures served as a tool for group interpretation in passages that had previously been pointed to by verbal exchanges, (2) gestures served to fine-tune the location and enactment of dynamic markings in the score, and (3) group expressive gestures in the final “take” of the rehearsal incorporated group expressive gestures from other takes, constituting a negotiated set of score interpretations.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8ATUUZ2N\\0163853X.2015.html},
  number = {7}
}

@article{hostetterVisibleEmbodimentGestures2008,
  title = {Visible Embodiment: {{Gestures}} as Simulated Action},
  shorttitle = {Visible Embodiment},
  author = {Hostetter, Autumn B. and Alibali, Martha W.},
  date = {2008-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {15},
  pages = {495--514},
  issn = {1531-5320},
  doi = {10.3758/PBR.15.3.495},
  url = {https://doi.org/10.3758/PBR.15.3.495},
  urldate = {2019-08-26},
  abstract = {Spontaneous gestures that accompany speech are related to both verbal and spatial processes. We argue that gestures emerge from perceptual and motor simulations that underlie embodied language and mental imagery. We first review current thinking about embodied cognition, embodied language, and embodied mental imagery. We then provide evidence that gestures stem from spatial representations and mental images. We then propose the gestures-as-simulated-action framework to explain how gestures might arise from an embodied cognitive system. Finally, we compare this framework with other current models of gesture production, and we briefly outline predictions that derive from the framework.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4VGTSXDV\\Hostetter and Alibali - 2008 - Visible embodiment Gestures as simulated action.pdf},
  keywords = {Lexical Access,Mental Image,Mental Rotation,Motor Imagery,Speech Production},
  langid = {english},
  number = {3}
}

@article{hubscherGesturalProsodicDevelopment2019,
  title = {Gestural and {{Prosodic Development Act}} as {{Sister Systems}} and {{Jointly Pave}} the {{Way}} for {{Children}}’s {{Sociopragmatic Development}}},
  author = {Hübscher, Iris and Prieto, Pilar},
  date = {2019-06-12},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front Psychol},
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.01259},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6581748/},
  urldate = {2019-09-06},
  abstract = {Children might combine gesture and prosody to express a pragmatic meaning such as a request, information focus, uncertainty or politeness, before they can convey these meanings in speech. However, little is known about the developmental trajectories of gestural and prosodic patterns and how they relate to a child’s growing understanding and propositional use of these sociopragmatic meanings. Do gesture and prosody act as sister systems in pragmatic development? Do children acquire these components of language before they are able to express themselves through spoken language, thus acting as forerunners in children’s pragmatic development? This review article assesses empirical evidence that demonstrates that gesture and prosody act as intimately related systems and, importantly, pave the way for pragmatic acquisition at different developmental stages. The review goes on to explore how the integration of gesture and prosody with semantics and syntax can impact language acquisition and how multimodal interventions can be used effectively in educational settings. Our review findings support the importance of simultaneously assessing both the prosodic and the gestural components of language in the fields of language development, language learning, and language intervention.},
  eprint = {31244716},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\T7A5B36S\\Hübscher and Prieto - 2019 - Gestural and Prosodic Development Act as Sister Sy.pdf},
  pmcid = {PMC6581748}
}

@article{ingberTensegrityMechanotransduction2008,
  title = {Tensegrity and Mechanotransduction},
  author = {Ingber, D. W.},
  date = {2008-07},
  journaltitle = {Journal of Bodywork and Movement Therapies},
  volume = {12},
  pages = {198--200},
  issn = {13608592},
  doi = {10.1016/j.jbmt.2008.04.038},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1360859208000788},
  urldate = {2019-08-09},
  abstract = {Anyone who is skilled in the art of physical therapy knows that the mechanical properties, behavior and movement of our bodies are as important for human health as chemicals and genes. However, only recently have scientists and physicians begun to appreciate the key role that mechanical forces play in biological control at the molecular and cellular levels. This article provides a brief overview of a lecture presented at the 1st International Fascia Research Congress that convened at Harvard Medical School in Boston, MA on October 4, 2007. (see figure 1) In this lecture, I described what we have learned over the past thirty years as a result of our research focused on the molecular mechanisms by which cells sense mechanical forces and convert them into changes in intracellular biochemistry and gene expression – a process called “mechanotransduction”. This work has revealed that molecules, cells, tissues, organs, and our entire bodies use “tensegrity” architecture to mechanically stabilize their shape, and to seamlessly integrate structure and function at all size scales. Through use of this tension-dependent building system, mechanical forces applied at the macroscale produce changes in biochemistry and gene expression within individual living cells. This structurebased system provides a mechanistic basis to explain how application of physical therapies might influence cell and tissue physiology.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DK6AVJRE\\Ingber - 2008 - Tensegrity and mechanotransduction.pdf},
  langid = {english},
  number = {3}
}

@unpublished{itoMediumFacilitatesPerception2015,
  title = {Medium {{Facilitates}} the Perception of Affordances for Touch},
  author = {Ito, K. and Sawada, M. and Mishima, Mamoru and Mishima, H. and Takiyama, M. and Kikuchi, Y.;},
  date = {2015-07-18},
  eventtitle = {{{XVIII International Conference}} on {{Perception}}-{{Action}}},
  venue = {{Minneapolis}}
}

@article{iversonHandMouthBrain2005,
  title = {Hand, Mouth and Brain: {{The}} Dynamic Emergence of Speech and Gesture},
  author = {Iverson, Jana M and Thelen, Esther},
  date = {2005},
  journaltitle = {Journal of Consciousness Studies},
  pages = {22},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NIX9C9GZ\\Iverson and Thelen - 2005 - Journal of Consciousness Studies.pdf},
  langid = {english}
}

@article{iversonResilienceGestureTalk2001,
  title = {The Resilience of Gesture in Talk: Gesture in Blind Speakers and Listeners},
  shorttitle = {The Resilience of Gesture in Talk},
  author = {Iverson, Jana M. and Goldin‐Meadow, Susan},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {416--422},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00183},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00183},
  urldate = {2019-05-05},
  abstract = {Spontaneous gesture frequently accompanies speech. The question is why. In these studies, we tested two non-mutually exclusive possibilities. First, speakers may gesture simply because they see others gesture and learn from this model to move their hands as they talk. We tested this hypothesis by examining spontaneous communication in congenitally blind children and adolescents. Second, speakers may gesture because they recognize that gestures can be useful to the listener. We tested this hypothesis by examining whether speakers gesture even when communicating with a blind listener who is unable to profit from the information that the hands convey. We found that congenitally blind speakers, who had never seen gestures, nevertheless gestured as they spoke, conveying the same information and producing the same range of gesture forms as sighted speakers. Moreover, blind speakers gestured even when interacting with another blind individual who could not have benefited from the information contained in those gestures. These findings underscore the robustness of gesture in talk and suggest that the gestures that co-occur with speech may serve a function for the speaker as well as for the listener.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BBKPR8QQ\\1467-7687.html},
  langid = {english},
  number = {4}
}

@article{iversonResilienceGestureTalk2001a,
  title = {The Resilience of Gesture in Talk: Gesture in Blind Speakers and Listeners},
  shorttitle = {The Resilience of Gesture in Talk},
  author = {Iverson, Jana M. and Goldin‐Meadow, Susan},
  date = {2001},
  journaltitle = {Developmental Science},
  volume = {4},
  pages = {416--422},
  issn = {1467-7687},
  doi = {10.1111/1467-7687.00183},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-7687.00183},
  urldate = {2019-10-14},
  abstract = {Spontaneous gesture frequently accompanies speech. The question is why. In these studies, we tested two non-mutually exclusive possibilities. First, speakers may gesture simply because they see others gesture and learn from this model to move their hands as they talk. We tested this hypothesis by examining spontaneous communication in congenitally blind children and adolescents. Second, speakers may gesture because they recognize that gestures can be useful to the listener. We tested this hypothesis by examining whether speakers gesture even when communicating with a blind listener who is unable to profit from the information that the hands convey. We found that congenitally blind speakers, who had never seen gestures, nevertheless gestured as they spoke, conveying the same information and producing the same range of gesture forms as sighted speakers. Moreover, blind speakers gestured even when interacting with another blind individual who could not have benefited from the information contained in those gestures. These findings underscore the robustness of gesture in talk and suggest that the gestures that co-occur with speech may serve a function for the speaker as well as for the listener.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\G6DTJTEE\\1467-7687.html},
  langid = {english},
  number = {4}
}

@article{iversonWhyPeopleGesture1998,
  title = {Why People Gesture When They Speak},
  author = {Iverson, Jana M. and Goldin-Meadow, Susan},
  date = {1998-11},
  journaltitle = {Nature},
  volume = {396},
  pages = {228--228},
  issn = {1476-4687},
  doi = {10.1038/24300},
  url = {https://www.nature.com/articles/24300},
  urldate = {2019-11-30},
  abstract = {People use gestures when they talk, but is this behaviour learned from watching others move their hands when talking? Individuals who are blind from birth never see such gestures and so have no model for gesturing. But here we show that congenitally blind speakers gesture despite their lack of a visual model, even when they speak to a blind listener. Gestures therefore require neither a model nor an observant partner.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\69RUKT7D\\24300.html},
  langid = {english},
  number = {6708}
}

@article{jarvisEvolutionVocalLearning2019,
  title = {Evolution of Vocal Learning and Spoken Language},
  author = {Jarvis, Erich D.},
  date = {2019-10-04},
  journaltitle = {Science},
  volume = {366},
  pages = {50--54},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax0287},
  url = {https://science.sciencemag.org/content/366/6461/50},
  urldate = {2019-10-18},
  abstract = {Although language, and therefore spoken language or speech, is often considered unique to humans, the past several decades have seen a surge in nonhuman animal studies that inform us about human spoken language. Here, I present a modern, evolution-based synthesis of these studies, from behavioral to molecular levels of analyses. Among the key concepts drawn are that components of spoken language are continuous between species, and that the vocal learning component is the most specialized and rarest and evolved by brain pathway duplication from an ancient motor learning pathway. These concepts have important implications for understanding brain mechanisms and disorders of spoken language.},
  eprint = {31604300},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\GGTIKRCC\\50.html},
  langid = {english},
  number = {6461}
}

@online{JointActionInteractive,
  title = {Joint {{Action}}, {{Interactive Alignment}}, and {{Dialog}} - {{Garrod}} - 2009 - {{Topics}} in {{Cognitive Science}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1756-8765.2009.01020.x},
  urldate = {2019-12-18},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HR5LMI5K\\j.1756-8765.2009.01020.html}
}

@incollection{kelsoActionperceptionPatternFormation1990,
  title = {Action-Perception as a Pattern Formation Process},
  booktitle = {Attention and Performance 13:  {{Motor}} Representation and Control},
  author = {Kelso, S. J. A. and Del Colle, J. D. and Schöner, G.},
  date = {1990},
  pages = {139--169},
  publisher = {{Lawrence Erlbaum Associates, Inc}},
  location = {{Hillsdale, NJ, US}},
  abstract = {aim to identify collective variables (or order parameters) and their dynamics (stability, loss of stability, hysteresis . . . ) for perception-action patterns / to do so we extend earlier results on phase transitions in human bimanual coordination to a perception-action task, synchronizing or syncopating finger flexion with an auditory metronome whose frequency is varied in steps (.25 Hz) over a wide range (1.0 Hz to 3.5 Hz) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Y6BQ6G7X\\1990-97330-005.html},
  isbn = {978-0-8058-0565-9},
  keywords = {Auditory Perception,Perceptual Motor Coordination,Response Parameters}
}

@article{kelsoConvergingEvidenceSupport1984,
  title = {Converging Evidence in Support of Common Dynamical Principles for Speech and Movement Coordination},
  author = {Kelso, J. A. and Tuller, B.},
  date = {1984-06},
  journaltitle = {The American Journal of Physiology},
  shortjournal = {Am. J. Physiol.},
  volume = {246},
  pages = {R928-935},
  issn = {0002-9513},
  doi = {10.1152/ajpregu.1984.246.6.R928},
  abstract = {We suggest that a principled analysis of language and action should begin with an understanding of the rate-dependent, dynamical processes that underlie their implementation. Here we present a summary of our ongoing speech production research, which reveals some striking similarities with other work on limb movements. Four design themes emerge for articulatory systems: 1) they are functionally rather than anatomically specific in the way they work; 2) they exhibit equifinality and in doing so fall under the generic category of a dynamical system called point attractor; 3) across transformations they preserve a relationally invariant topology; and 4) this, combined with their stable cyclic nature, suggests that they can function as nonlinear, limit cycle oscillators (periodic attractors). This brief inventory of regularities, though not mean to be inclusive, hints strongly that speech and other movements share a common, dynamical mode of operation.},
  eprint = {6742170},
  eprinttype = {pmid},
  issue = {6 Pt 2},
  keywords = {Animals,Brain,Humans,Locomotion,Models; Neurological,Models; Psychological,Motor Activity,Movement,Speech},
  langid = {english}
}

@incollection{kelsoDynamicPatternPerspective1983,
  title = {A “{{Dynamic Pattern}}” {{Perspective}} on the {{Control}} and {{Coordination}} of {{Movement}} | {{SpringerLink}}},
  booktitle = {The Production of Speech},
  author = {Kelso, J. A. S. and Tuller, B. and Harris, K.},
  date = {1983},
  publisher = {{Springer-Verlag}},
  location = {{Berlin}},
  url = {https://link.springer.com/chapter/10.1007/978-1-4613-8202-7_7},
  urldate = {2019-08-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NZWWH973\\978-1-4613-8202-7_7.html}
}

@article{kelsoFunctionallySpecificArticulatory1984,
  title = {Functionally Specific Articulatory Cooperation Following Jaw Perturbations during Speech: Evidence for Coordinative Structures},
  shorttitle = {Functionally Specific Articulatory Cooperation Following Jaw Perturbations during Speech},
  author = {Kelso, J. A. and Tuller, B. and Vatikiotis-Bateson, E. and Fowler, C. A.},
  date = {1984-12},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {10},
  pages = {812--832},
  issn = {0096-1523},
  abstract = {In three experiments we show that articulatory patterns in response to jaw perturbations are specific to the utterance produced. In Experiments 1 and 2, an unexpected constant force load (5.88 N) applied during upward jaw motion for final /b/ closure in the utterance /baeb/ revealed nearly immediate compensation in upper and lower lips, but not the tongue, on the first perturbation trial. The same perturbation applied during the utterance /baez/ evoked rapid and increased tongue-muscle activity for /z/ frication, but no active lip compensation. Although jaw perturbation represented a threat to both utterances, no perceptible distortion of speech occurred. In Experiment 3, the phase of the jaw perturbation was varied during the production of bilabial consonants. Remote reactions in the upper lip were observed only when the jaw was perturbed during the closing phase of motion. These findings provide evidence for flexibly assembled coordinative structures in speech production.},
  eprint = {6239907},
  eprinttype = {pmid},
  keywords = {Adult,Electromyography,Humans,Lip,Male,Mandible,Masticatory Muscles,Phonation,Phonetics,Speech,Speech Production Measurement,Voice},
  langid = {english},
  number = {6}
}

@article{kendonReflectionsGesturefirstHypothesis2017,
  title = {Reflections on the “Gesture-First” Hypothesis of Language Origins},
  author = {Kendon, Adam},
  date = {2017-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {24},
  pages = {163--170},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1117-3},
  url = {https://doi.org/10.3758/s13423-016-1117-3},
  urldate = {2019-10-14},
  abstract = {The main lines of evidence taken as support for the “gesture-first” hypothesis of language origins are briefly evaluated, and the problem that speech poses for this hypothesis is discussed. I conclude that language must have evolved in the oral–aural and kinesic modalities together, with neither modality taking precedence over the other.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\T3SYIPKP\\Kendon - 2017 - Reflections on the “gesture-first” hypothesis of l.pdf},
  keywords = {Gesture,Language origins,Primate communication,Sign language,Speech},
  langid = {english},
  number = {1}
}

@article{kienastACOUSTICALANALYSISSPECTRAL,
  title = {{{ACOUSTICAL ANALYSIS OF SPECTRAL AND TEMPORAL CHANGES IN EMOTIONAL SPEECH}}},
  author = {Kienast, M and Sendlmeier, W F},
  journaltitle = {Analysis ISCA Tutor},
  pages = {92--97},
  abstract = {In the present study, the vocal expressions of the emotions anger, happiness, fear, boredom and sadness are acoustically analyzed in relation to neutral speech. The emotional speech material produced by actors is investigated especially with regard to spectral and segmental changes which are caused by different articulatory behavior accompanying emotional arousal. The findings are interpreted in relation to temporal variations.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BIJ55YAL\\Kienast and Sendlmeier - ACOUSTICAL ANALYSIS OF SPECTRAL AND TEMPORAL CHANG.pdf},
  langid = {english}
}

@article{kirbyCompressionCommunicationCultural2015,
  title = {Compression and Communication in the Cultural Evolution of Linguistic Structure - {{ScienceDirect}}},
  author = {Kirby, S and Tamariz, M. and Cornish, H. and Smith, K.},
  date = {2015},
  journaltitle = {Cognition},
  volume = {141},
  pages = {87--102},
  doi = {https://doi.org/10.1016/j.cognition.2015.03.016},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027715000815},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YR8VRAVS\\S0010027715000815.html}
}

@inproceedings{kitaMovementPhasesSigns1998,
  title = {Movement Phases in Signs and Co-Speech Gestures, and Their Transcription by Human Coders},
  booktitle = {Gesture and {{Sign Language}} in {{Human}}-{{Computer Interaction}}},
  author = {Kita, Sotaro and van Gijn, Ingeborg and van der Hulst, Harry},
  editor = {Wachsmuth, Ipke and Fröhlich, Martin},
  date = {1998},
  pages = {23--35},
  publisher = {{Springer Berlin Heidelberg}},
  abstract = {The previous literature has suggested that the hand movement in co-speech gestures and signs consists of a series of phases with qualitatively different dynamic characteristics. In this paper, we propose a syntagmatic rule system for movement phases that applies to both co-speech gestures and signs. Descriptive criteria for the rule system were developed for the analysis video-recorded continuous production of signs and gesture. It involves segmenting a stream of body movement into phases and identifying different phase types. Two human coders used the criteria to analyze signs and cospeech gestures that are produced in natural discourse. It was found that the criteria yielded good inter-coder reliability. These criteria can be used for the technology of automatic recognition of signs and co-speech gestures in order to segment continuous production and identify the potentially meaningbearing phase.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\D6R8R2SP\\Kita et al. - 1998 - Movement phases in signs and co-speech gestures, a.pdf},
  isbn = {978-3-540-69782-4},
  keywords = {American Sign,Expressive Phase,Movement Phase,Phase Type,Sign Language},
  langid = {english},
  options = {useprefix=true},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{kitaPointingWhereLanguage2003,
  title = {Pointing:  {{Where}} Language, Culture, and Cognition Meet},
  shorttitle = {Pointing},
  author = {Kita, S.},
  date = {2003},
  publisher = {{Lawrence Erlbaum Associates Publishers}},
  location = {{Mahwah, NJ, US}},
  abstract = {This volume examines pointing gestures from a multidisciplinary viewpoint. Pointing has captured the interest of scholars from different disciplines who study communication, however, ideas and findings have been scattered across diverse journals and researchers are often not aware of results in other disciplines. The aim of this volume is to provide an arena for the interdisciplinary exchange of information on pointing. This volume will be of interest to researchers in linguistics, semiotics, psychology, anthropology, and primatology. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5URMXNHN\\2003-00985-000.html},
  isbn = {978-0-8058-4014-8},
  keywords = {Experimentation,Gestures,Interdisciplinary Research},
  pagetotal = {vii, 339},
  series = {Pointing:  {{Where}} Language, Culture, and Cognition Meet}
}

@article{klingeIncreasedAmygdalaActivation2010,
  title = {Increased Amygdala Activation to Emotional Auditory Stimuli in the Blind},
  author = {Klinge, Corinna and Röder, Brigitte and Büchel, Christian},
  date = {2010-06},
  journaltitle = {Brain},
  volume = {133},
  pages = {1729--1736},
  issn = {1460-2156, 0006-8950},
  doi = {10.1093/brain/awq102},
  url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/awq102},
  urldate = {2019-11-22},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SSDBMUSK\\Klinge et al. - 2010 - Increased amygdala activation to emotional auditor.pdf},
  langid = {english},
  number = {6}
}

@article{klingeIncreasedAmygdalaActivation2010a,
  title = {Increased Amygdala Activation to Emotional Auditory Stimuli in the Blind},
  author = {Klinge, Corinna and Röder, Brigitte and Büchel, Christian},
  date = {2010-06-01},
  journaltitle = {Brain},
  shortjournal = {Brain},
  volume = {133},
  pages = {1729--1736},
  issn = {0006-8950},
  doi = {10.1093/brain/awq102},
  url = {https://academic.oup.com/brain/article/133/6/1729/355156},
  urldate = {2019-11-22},
  abstract = {Abstract.  Emotional signals are of pivotal relevance in social interactions. Neuroimaging and lesion studies have established an important role of the amygdala},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PEZPIEYL\\Klinge et al. - 2010 - Increased amygdala activation to emotional auditor.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8VNW26X9\\355156.html},
  langid = {english},
  number = {6}
}

@article{koenigRespiratoryElectroglottographicMeasures2019,
  title = {Respiratory and Electroglottographic Measures of Normal and Loud Speech across Vowels},
  author = {Koenig, Laura L. and Fuchs, Susanne},
  date = {2019-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {145},
  pages = {1931--1931},
  issn = {0001-4966},
  doi = {10.1121/1.5102030},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.5102030},
  urldate = {2019-08-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VVDTB7XM\\1.html},
  number = {3}
}

@article{krahmerEffectsVisualBeats2007,
  title = {The Effects of Visual Beats on Prosodic Prominence: {{Acoustic}} Analyses, Auditory Perception and Visual Perception},
  shorttitle = {The Effects of Visual Beats on Prosodic Prominence},
  author = {Krahmer, Emiel and Swerts, Marc},
  date = {2007-10},
  journaltitle = {Journal of Memory and Language},
  volume = {57},
  pages = {396--414},
  issn = {0749596X},
  doi = {10.1016/j.jml.2007.06.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X07000708},
  urldate = {2019-04-18},
  abstract = {Speakers employ acoustic cues (pitch accents) to indicate that a word is important, but may also use visual cues (beat gestures, head nods, eyebrow movements) for this purpose. Even though these acoustic and visual cues are related, the exact nature of this relationship is far from well understood. We investigate whether producing a visual beat leads to changes in how acoustic prominence is realized in speech, and whether it leads to changes in how prominence is perceived by observers. For Experiment I (‘‘making beats’’) we use an original experimental paradigm in which speakers are instructed to realize a target sentence with different distributions of acoustic and visual cues for prominence. Acoustic analyses reveal that the production of a visual beat indeed has an effect on the acoustic realization of the co-occurring speech, in particular on duration and the higher formants (F2 and F3), independent of the kind of visual beat and of the presence and position of pitch accents. In Experiment II (‘‘hearing beats’’), it is found that visual beats have a significant effect on the perceived prominence of the target words. When a speaker produces a beat gesture, an eyebrow movement or a head nod, the accompanying word is produced with relatively more spoken emphasis. In Experiment III (‘‘seeing beats’’), finally, it is found that when participants see a speaker realize a visual beat on a word, they perceive it as more prominent than when they do not see the beat gesture.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\AU3AWR57\\Krahmer and Swerts - 2007 - The effects of visual beats on prosodic prominence.pdf},
  langid = {english},
  number = {3}
}

@incollection{kraussLexicalGesturesLexical2000,
  title = {Lexical Gestures and Lexical {{accessL}} a Process},
  booktitle = {Language and {{Gesture}}},
  author = {Krauss, R. M. and Chen, Y. and Gottesman, R. F.},
  date = {2000},
  url = {https://scholar.googleusercontent.com/scholar.bib?q=info:0fpr8C9_kDMJ:scholar.google.com/&output=citation&scisdr=CgUQcZkfEO3Yy3o0M_I:AAGBfm0AAAAAXZMxK_KEmWpHM-VpMySbsZU2wKqbdaIZ&scisig=AAGBfm0AAAAAXZMxK7vMU-afRHI6nivhKGEXFplyqMZq&scisf=4&ct=citation&cd=-1&hl=en},
  urldate = {2019-10-01},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ECPAAISQ\\scholar.html}
}

@article{krivokapicGesturalCoordinationProsodic2014,
  title = {Gestural Coordination at Prosodic Boundaries and Its Role for Prosodic Structure and Speech Planning Processes},
  author = {Krivokapić, Jelena},
  date = {2014-12-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0397},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4240964/},
  urldate = {2020-01-26},
  abstract = {Prosodic structure is a grammatical component that serves multiple functions in the production, comprehension and acquisition of language. Prosodic boundaries are critical for the understanding of the nature of the prosodic structure of language, and important progress has been made in the past decades in illuminating their properties. We first review recent prosodic boundary research from the point of view of gestural coordination. We then go on to tie in this work to questions of speech planning and manual and head movement. We conclude with an outline of a new direction of research which is needed for a full understanding of prosodic boundaries and their role in the speech production process.},
  eprint = {25385775},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3ZRITNK5\\Krivokapić - 2014 - Gestural coordination at prosodic boundaries and i.pdf},
  number = {1658},
  pmcid = {PMC4240964}
}

@article{krivokapicKinematicStudyProsodic2017,
  title = {A {{Kinematic Study}} of {{Prosodic Structure}} in {{Articulatory}} and {{Manual Gestures}}: {{Results}} from a {{Novel Method}} of {{Data Collection}}},
  shorttitle = {A {{Kinematic Study}} of {{Prosodic Structure}} in {{Articulatory}} and {{Manual Gestures}}},
  author = {Krivokapić, Jelena and Tiede, Mark K. and Tyrone, Martha E.},
  date = {2017},
  journaltitle = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {8},
  pages = {3},
  issn = {1868-6354},
  doi = {10.5334/labphon.75},
  url = {http://www.journal-labphon.org/articles/10.5334/labphon.75/},
  urldate = {2020-01-15},
  abstract = {The primary goal of this work is to examine prosodic structure as expressed concurrently through articulatory and manual gestures. Specifically, we investigated the effects of phrase-level prominence (Experiment 1) and of prosodic boundaries (Experiments 2 and 3) on the kinematic properties of oral constriction and manual gestures. The hypothesis guiding this work is that prosodic structure will be similarly expressed in both modalities. To test this, we have developed a novel method of data collection that simultaneously records speech audio, vocal tract gestures (using electromagnetic articulometry) and manual gestures (using motion capture). This method allows us, for the first time, to investigate kinematic properties of body movement and vocal tract gestures simultaneously, which in turn allows us to examine the relationship between speech and body gestures with great precision. A second goal of the paper is thus to establish the validity of this method. Results from two speakers show that manual and oral gestures lengthen under prominence and at prosodic boundaries, indicating that the effects of prosodic structure extend beyond the vocal tract to include body movement.1},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QZ4WY9ZY\\Krivokapić et al. - 2017 - A Kinematic Study of Prosodic Structure in Articul.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ZCZIUWFM\\labphon.75.html},
  keywords = {electro- magnetic articulometry,EMA,gestures,motion capture,Prosodic boundaries,prosodic prominence,speech production,Vicon},
  langid = {english},
  number = {1}
}

@inproceedings{krivokapicSpeechManualGesture2016,
  title = {Speech and Manual Gesture Coordination in a Pointing Task},
  author = {Krivokapic, Jelena and Tiede, Mark K. and Tyrone, Martha E. and Goldenberg, Dolly},
  date = {2016},
  doi = {10.21437/SpeechProsody.2016-255},
  abstract = {This study explores the coordination between manual pointing gestures and gestures of the vocal tract. Using a novel methodology that allows for concurrent collection of audio, kinematic body and speech articulator trajectories, we ask 1) which particular gesture (vowel gesture, consonant gesture, or tone gesture) the pointing gesture is coordinated with, and 2) with which landmarks the two gestures are coordinated (for example, whether the pointing gesture is coordinated to the speech gesture by the onset or maximum displacement). Preliminary results indicate coordination of the intonation gesture and the pointing gesture.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\JBUHQ7XB\\Krivokapic et al. - 2016 - Speech and manual gesture coordination in a pointi.pdf},
  keywords = {Audio Media,Displacement mapping,Gesture recognition,Onset (audio),Pointing device,Psychologic Displacement,recurrent childhood visual pathway glioma,Tract (literature),vowels}
}

@inproceedings{kucherenkoAnalyzingInputOutput2019,
  title = {Analyzing {{Input}} and {{Output Representations}} for {{Speech}}-{{Driven Gesture Generation}}},
  booktitle = {Proceedings of the 19th {{ACM International Conference}} on {{Intelligent Virtual Agents}}  - {{IVA}} '19},
  author = {Kucherenko, Taras and Hasegawa, Dai and Henter, Gustav Eje and Kaneko, Naoshi and Kjellström, Hedvig},
  date = {2019},
  pages = {97--104},
  publisher = {{ACM Press}},
  location = {{Paris, France}},
  doi = {10.1145/3308532.3329472},
  url = {http://dl.acm.org/citation.cfm?doid=3308532.3329472},
  urldate = {2019-10-01},
  eventtitle = {The 19th {{ACM International Conference}}},
  isbn = {978-1-4503-6672-4},
  langid = {english}
}

@incollection{kuglerConceptCoordinativeStructures1980,
  title = {On the Concept of Coordinative Structures as Dissipative Structures: {{I}}. {{Theoretical}} Lines of Convergence},
  shorttitle = {1 {{On}} the {{Concept}} of {{Coordinative Structures}} as {{Dissipative Structures}}},
  booktitle = {Advances in {{Psychology}}},
  author = {Kugler, P. N. and Scott Kelso, J. A. and Turvey, M. T.},
  editor = {Stelmach, George E. and Requin, Jean},
  date = {1980-01-01},
  volume = {1},
  pages = {3--47},
  publisher = {{North-Holland}},
  doi = {10.1016/S0166-4115(08)61936-6},
  url = {http://www.sciencedirect.com/science/article/pii/S0166411508619366},
  urldate = {2019-08-08},
  abstract = {A model construct for coordination and control is pursued according to three related guidelines: (1) that it directly address Bernstein's problem of how to explain the regulation of the many biokinematic degrees of freedom with minimal recourse to an “intelligent regulator”; (2) that it be miserly on the number of explanatory principles, sui generis; and (3) that it be consistent with established strictures of non-equilibrium thermodynamics, that is, physical principles that inform biological design. Argument is given that a group of muscles constrained to act as a unit, a coordinative structure, is a member of the class of thermodynamic engines qua dissipative structures and that this membership gives a principled basis for understanding the characteristics of coordination and control.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LEMVCFRN\\S0166411508619366.html},
  series = {Tutorials in {{Motor Behavior}}}
}

@incollection{ladefagodLinguisticAspectsRespiratory1968,
  title = {Linguistic Aspects of Respiratory Phenomena},
  booktitle = {Sound {{Production}} in {{Man}} ({{Ed}}. {{A}}. Bouhuys)},
  author = {Ladefagod, P.},
  date = {1968},
  pages = {141--151},
  publisher = {{New York Academy of Sciences}},
  location = {{New York}}
}

@article{lancasterRespiratoryMuscleActivity1995,
  title = {Respiratory Muscle Activity in Relation to Vocalization in Flying Bats.},
  author = {Lancaster, W. C. and Henson, O. W. and Keating, A. W.},
  date = {1995-01-01},
  journaltitle = {Journal of Experimental Biology},
  volume = {198},
  pages = {175--191},
  issn = {0022-0949, 1477-9145},
  url = {https://jeb.biologists.org/content/198/1/175},
  urldate = {2020-01-27},
  abstract = {Skip to Next Section
The structure of the thoracic and abdominal walls of Pteronotus parnellii (Microchiroptera: Mormoopidae) was described with respect to their function in respiration and vocalization. We monitored electromyographic activity of respiratory and flight muscles in relation to echolocative vocalization. In flight, signals were telemetered with a small FM transmitter modified to summate the low-frequency myopotentials with biosonar signals from a ceramic-crystal microphone. Recordings were also made from the same bats confined to a small cage. Vocalizations were used as the parameter by which all muscle activities were correlated. A discrete burst of activity in the lateral abdominal wall muscles accompanied each vocalization. Diaphragmatic myopotentials occurred between groups of calls and did not coincide with activity of the abdominal wall or with vocalizations. Flight muscles were not active in resting bats. During flight, vocalizations and the abdominal muscle activity that accompanied them coincided with myopotentials of the pectoralis and serratus ventralis muscles. We propose that contractions of the lateral abdominal wall provide the primary power for the production of intense biosonar vocalization in flying and in stationary bats. In flight, synchronization of vocalization with activity of the pectoralis and serratus ventralis jointly contribute to the pressurization of the thoraco-abdominal cavity. This utilization of pressure that is normally generated in flight facilitates respiration and allows for the production of intense vocalizations with little additional energetic expenditure.},
  eprint = {7891034},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\S6EMXYR3\\Lancaster et al. - 1995 - Respiratory muscle activity in relation to vocaliz.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\7Q8YBGKR\\175.html},
  langid = {english},
  number = {1}
}

@article{larssonBipedalStepsDevelopment2019,
  title = {Bipedal {{Steps}} in the {{Development}} of {{Rhythmic Behavior}} in {{Humans}}},
  author = {Larsson, Matz and Richter, Joachim and Ravignani, Andrea},
  date = {2019-01-01},
  journaltitle = {Music \& Science},
  shortjournal = {Music \& Science},
  volume = {2},
  pages = {2059204319892617},
  issn = {2059-2043},
  doi = {10.1177/2059204319892617},
  url = {https://doi.org/10.1177/2059204319892617},
  urldate = {2020-01-03},
  abstract = {We contrast two related hypotheses of the evolution of dance: H1: Maternal bipedal walking influenced the fetal experience of sound and associated movement patterns; H2: The human transition to bipedal gait produced more isochronous/predictable locomotion sound resulting in early music-like behavior associated with the acoustic advantages conferred by moving bipedally in pace. The cadence of walking is around 120 beats per minute, similar to the tempo of dance and music. Human walking displays long-term constancies. Dyads often subconsciously synchronize steps. The major amplitude component of the step is a distinctly produced beat. Human locomotion influences, and interacts with, emotions, and passive listening to music activates brain motor areas. Across dance-genres the footwork is most often performed in time to the musical beat. Brain development is largely shaped by early sensory experience, with hearing developed from week 18 of gestation. Newborns reacts to sounds, melodies, and rhythmic poems to which they have been exposed in utero. If the sound and vibrations produced by footfalls of a walking mother are transmitted to the fetus in coordination with the cadence of the motion, a connection between isochronous sound and rhythmical movement may be developed. Rhythmical sounds of the human mother locomotion differ substantially from that of nonhuman primates, while the maternal heartbeat heard is likely to have a similar isochronous character across primates, suggesting a relatively more influential role of footfall in the development of rhythmic/musical abilities in humans. Associations of gait, music, and dance are numerous. The apparent absence of musical and rhythmic abilities in nonhuman primates, which display little bipedal locomotion, corroborates that bipedal gait may be linked to the development of rhythmic abilities in humans. Bipedal stimuli in utero may primarily boost the ontogenetic development. The acoustical advantage hypothesis proposes a mechanism in the phylogenetic development.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\K8Z2Y2G9\\Larsson et al. - 2019 - Bipedal Steps in the Development of Rhythmic Behav.pdf},
  keywords = {Bipedal gait,evolution of dance,hominids,intra-uterine development,music},
  langid = {english}
}

@article{lenthPackageLsmeans2017,
  title = {Package ‘lsmeans'},
  author = {Lenth, R. and Lenth, M. R.},
  date = {2017},
  journaltitle = {The American Statistician},
  volume = {34},
  pages = {216--221},
  number = {4}
}

@article{leonardTemporalRelationBeat2011,
  title = {The Temporal Relation between Beat Gestures and Speech},
  author = {Leonard, Thomas and Cummins, Fred},
  date = {2011-12-01},
  journaltitle = {Language and Cognitive Processes},
  volume = {26},
  pages = {1457--1471},
  issn = {0169-0965},
  doi = {10.1080/01690965.2010.500218},
  url = {https://doi.org/10.1080/01690965.2010.500218},
  urldate = {2019-04-23},
  abstract = {The temporal relation between beat gestures and accompanying speech is examined in two experiments. In the first, we find that subjects are very quick to spot altered timing between gesture and speech if the gesture is later than normal, but are considerably less sensitive to alterations that result in an earlier gesture. This suggests an asymmetry in the expectation on the part of listeners/watchers and raises immediate questions about which elements within the speech are being perceived as linked to which elements in the gestural series. We therefore examine the variability between several kinematic landmarks in a beat gesture, and three potential anchor points in the accompanying speech. We find the least variable relationship obtains between the point of maximum extension of the gesture and the accompanying pitch accent. Together, these findings contribute to our understanding of both the production and perception of beat gestures along with speech, and support an account of speech communication as a strongly embodied activity.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\RKA5ALLQ\\Leonard and Cummins - 2011 - The temporal relation between beat gestures and sp.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\6TTZCMG9\\01690965.2010.html},
  keywords = {Coordination,Embodiment,Gesture},
  number = {10}
}

@article{levinsonOriginHumanMultimodal2014,
  title = {The Origin of Human Multi-Modal Communication},
  author = {Levinson, Stephen C. and Holler, Judith},
  date = {2014-09-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0302},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4123681/},
  urldate = {2020-01-26},
  abstract = {One reason for the apparent gulf between animal and human communication systems is that the focus has been on the presence or the absence of language as a complex expressive system built on speech. But language normally occurs embedded within an interactional exchange of multi-modal signals. If this larger perspective takes central focus, then it becomes apparent that human communication has a layered structure, where the layers may be plausibly assigned different phylogenetic and evolutionary origins—especially in the light of recent thoughts on the emergence of voluntary breathing and spoken language. This perspective helps us to appreciate the different roles that the different modalities play in human communication, as well as how they function as one integrated system despite their different roles and origins. It also offers possibilities for reconciling the ‘gesture-first hypothesis’ with that of gesture and speech having evolved together, hand in hand—or hand in mouth, rather—as one system.},
  eprint = {25092670},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\2UD3JMPX\\Levinson and Holler - 2014 - The origin of human multi-modal communication.pdf},
  number = {1651},
  pmcid = {PMC4123681}
}

@article{loehrTemporalStructuralPragmatic2012,
  title = {Temporal, Structural, and Pragmatic Synchrony between Intonation and Gesture},
  author = {Loehr, Daniel P.},
  date = {2012},
  journaltitle = {Laboratory Phonology},
  volume = {3},
  pages = {71--89},
  issn = {1868-6346},
  doi = {10.1515/lp-2012-0006},
  url = {https://www.degruyter.com/view/j/lp.2012.3.issue-1/lp-2012-0006/lp-2012-0006.xml},
  urldate = {2019-04-23},
  abstract = {This paper explores the interaction between intonation and gesture, noting temporal, structural, and pragmatic synchrony. Videotapes of subjects conversing freely were annotated for intonation according to ToBI (Beckman and Elam 1997), and for gesture according to Kendon (1980) and McNeill (1992). The time-stamped annotations were analyzed statistically, as well as visually in the Anvil tool (Kipp 2001), which allows time-aligned viewing of videos with their annotations. Alignments were investigated between three levels of intonational units and four levels of gestural units. The intonational units were, from smallest to largest, pitch accents, intermediate phrases, and intonational phrases. The gestural units, again from smallest to largest, were apices of strokes, gesture phases, gesture phrases, and gesture units. Of these possible combinations, two pairs aligned. Apices clearly aligned with pitch accents, and gesture phrases tended to align with intermediate phrases. The existence of intermediate phrases in English has been the subject of some debate (Ladd 2008), and this paper suggests that a probable gestural correlate to intermediate phrases provides independent evidence for their existence. In addition, intonation and gesture were found to perform simultaneous complementary pragmatic functions. This temporal, structural, and pragmatic synchrony between the two channels reinforces the claim that speech and gesture are two surface forms of the same underlying and emerging cognitive content.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FKELJLTZ\\Loehr - 2012 - Temporal, structural, and pragmatic synchrony betw.pdf},
  number = {1}
}

@article{magnussonBodyLanguageAdults2008,
  title = {The {{Body Language}} of {{Adults Who Are Blind}}},
  author = {Magnusson, Anna-Karin and Karlsson, Gunnar},
  date = {2008-06-01},
  journaltitle = {Scandinavian Journal of Disability Research},
  volume = {10},
  pages = {71--89},
  issn = {1501-7419},
  doi = {10.1080/15017410701685927},
  url = {https://doi.org/10.1080/15017410701685927},
  urldate = {2019-11-30},
  abstract = {The body expressions of adults who are blind have been relatively unexplored. The aim of this study was therefore to deepen the understanding of different forms of body expression, or “body language”, in adults who are blind. More specifically the study aimed at answering the following questions: What forms of body expression do adults who are blind display? What can the conditions for some different forms of body expression be? What importance can individual, social and cultural factors have for different forms of body expression? Data consisted of video-taped interviews with five congenitally blind, two adventitiously blind and two sighted individuals. The data were analysed in a hermeneutical and phenomenological sense. The results consisted of a typology of 19 different forms of body expression. All in all, we found that the congenitally blind participants expressed themselves mainly in a functional and concrete manner. They also seemed to have limited experiences with abstract, symbolic body expressions. The conditions and the importance of different factors for different body expressions are discussed.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\S93QF2EA\\Magnusson and Karlsson - 2008 - The Body Language of Adults Who Are Blind.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\JR8FZJIT\\15017410701685927.html},
  number = {2}
}

@article{magnussonNonverbalConversationRegulatingSignals2006,
  title = {Nonverbal {{Conversation}}-{{Regulating Signals}} of the {{Blind Adult}}},
  author = {Magnusson, Anna-Karin},
  date = {2006-12-01},
  journaltitle = {Communication Studies},
  volume = {57},
  pages = {421--433},
  issn = {1051-0974},
  doi = {10.1080/10510970600946004},
  url = {https://doi.org/10.1080/10510970600946004},
  urldate = {2019-11-30},
  abstract = {The purpose of this study is to describe nonverbal conversation-regulating signals among the blind adult and to describe how these signals are manifested through body movements/positions and paralinguistic sounds/silences. Data consist of videotaped conversations between blind-blind pairs and blind-sighted pairs. The data are analyzed in a hermeneutical/phenomenological sense. The analysis is also inspired by Conversational Analysis. The analysis resulted in descriptions of four major signal types, i.e. the listener's back-channeling signals, the speaker's turn-holding signals, the listener's starting signals, and the speaker's turn-yielding signals. One central conclusion is that earlier experience of vision does not seem to be a prerequisite for showing a variety of different types of nonverbal conversation-regulating signals in a systematic and distinct manner.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\SDPMBNKS\\Magnusson - 2006 - Nonverbal Conversation-Regulating Signals of the B.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\UGW4LCVA\\10510970600946004.html},
  keywords = {Blindness,Conversation analysis,Conversation-regulating signals,Hermeneutics,Nonverbal,Phenomenology},
  number = {4}
}

@article{magnussonNonverbalConversationRegulatingSignals2006a,
  title = {Nonverbal {{Conversation}}-{{Regulating Signals}} of the {{Blind Adult}}},
  author = {Magnusson, Anna-Karin},
  date = {2006-12-01},
  journaltitle = {Communication Studies},
  volume = {57},
  pages = {421--433},
  issn = {1051-0974},
  doi = {10.1080/10510970600946004},
  url = {https://doi.org/10.1080/10510970600946004},
  urldate = {2019-11-30},
  abstract = {The purpose of this study is to describe nonverbal conversation-regulating signals among the blind adult and to describe how these signals are manifested through body movements/positions and paralinguistic sounds/silences. Data consist of videotaped conversations between blind-blind pairs and blind-sighted pairs. The data are analyzed in a hermeneutical/phenomenological sense. The analysis is also inspired by Conversational Analysis. The analysis resulted in descriptions of four major signal types, i.e. the listener's back-channeling signals, the speaker's turn-holding signals, the listener's starting signals, and the speaker's turn-yielding signals. One central conclusion is that earlier experience of vision does not seem to be a prerequisite for showing a variety of different types of nonverbal conversation-regulating signals in a systematic and distinct manner.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\UF965FMB\\Magnusson - 2006 - Nonverbal Conversation-Regulating Signals of the B.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\43GRBUV9\\10510970600946004.html},
  keywords = {Blindness,Conversation analysis,Conversation-regulating signals,Hermeneutics,Nonverbal,Phenomenology},
  number = {4}
}

@article{mcclavePitchManualGestures1998,
  title = {Pitch and {{Manual Gestures}}},
  author = {McClave, E.},
  date = {1998},
  journaltitle = {Journal of Psycholinguistic Research},
  volume = {27},
  pages = {69--89},
  doi = {https://doi.org/10.1023/A:1023274823974},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DM2EK6C7\\10.html},
  number = {2}
}

@article{mcgurkHearingLipsSeeing1976,
  title = {Hearing Lips and Seeing Voices},
  author = {McGurk, H. and MacDonald, J.},
  date = {1976-12-23/0030},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {264},
  pages = {746--748},
  issn = {0028-0836},
  doi = {10.1038/264746a0},
  eprint = {1012311},
  eprinttype = {pmid},
  keywords = {Adolescent,Adult,Age Factors,Auditory Perception,Child,Child; Preschool,Female,Humans,Illusions,Male,Speech,Visual Perception},
  langid = {english},
  number = {5588}
}

@book{mcneillGestureThought2005,
  title = {Gesture and {{Thought}}},
  author = {McNeill, David},
  date = {2005},
  publisher = {{The University of Chicago Press}},
  location = {{Chicago}},
  url = {https://www.press.uchicago.edu/ucp/books/book/chicago/G/bo3633713.html},
  urldate = {2019-04-16},
  abstract = {Gesturing is such an integral yet unconscious part of communication that we are mostly oblivious to it. But if you observe anyone in conversation, you are likely to see his or her fingers, hands, and arms in some form of spontaneous motion. Why? David McNeill, a pioneer in the ongoing study of the relationship between gesture and language, set about answering this question over twenty-five years ago. In Gesture and Thought he brings together years of this research, arguing that gesturing, an act which has been popularly understood as an accessory to speech, is actually a dialectical component of language. Gesture and Thought expands on McNeill’s acclaimed classic Hand and Mind. While that earlier work demonstrated what gestures reveal about thought, here gestures are shown to be active participants in both speaking and thinking. Expanding on an approach introduced by Lev Vygotsky in the 1930s, McNeill posits that gestures are key ingredients in an “imagery-language dialectic” that fuels both speech and thought. Gestures are both the “imagery” and components of “language.” The smallest element of this dialectic is the “growth point,” a snapshot~of an utterance at its beginning psychological stage. Utilizing several innovative experiments he created and administered with subjects spanning several different age, gender, and language groups, McNeill shows how growth points organize themselves into utterances and extend to discourse at the moment of speaking.An ambitious project in the ongoing study of the relationship of human communication and thought, Gesture and Thought is a work of such consequence that it will influence all subsequent theory on the subject.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\S7LGLV93\\bo3633713.html},
  pagetotal = {328}
}

@book{mcneillHandMindWhat1992,
  title = {Hand and {{Mind}}: {{What Gestures Reveal}} about {{Thought}}},
  shorttitle = {Hand and {{Mind}}},
  author = {McNeill, David},
  date = {1992},
  publisher = {{University Of Chicago Press}},
  location = {{Chicago}},
  abstract = {Will be shipped from US. Used books may not include companion materials, may have some shelf wear, may contain highlighting/notes, may not include CDs or access codes. 100\% money back guarantee.}
}

@incollection{mcneillIWManWho2010,
  title = {{{IW}} - “{{The Man Who Lost His Body}}”},
  booktitle = {Handbook of {{Phenomenology}} and {{Cognitive Science}}},
  author = {McNeill, David and Quaeghebeur, Liesbet and Duncan, Susan},
  editor = {Schmicking, Daniel and Gallagher, Shaun},
  date = {2010},
  pages = {519--543},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-90-481-2646-0_27},
  url = {https://doi.org/10.1007/978-90-481-2646-0_27},
  urldate = {2019-04-16},
  abstract = {Mr. Ian Waterman, sometimes referred to as ‘IW’, suffered at age 19 a sudden, total deafferentation of his body from the neck down - the near total loss of all the touch, proprioception, and limb spatial position senses that tell you, without looking, where your body is and what it is doing. The loss followed a never-diagnosed fever that is believed to have set off an auto-immune reaction. The immediate behavioral effect was immobility, even though IW’s motor system was unaffected and there was no paralysis. The problem was not lack of movement per se but lack of control. Upon awakening after 3 days, IW nightmarishly found that he had no control over what his body did - he was unable to sit up, walk, feed himself or manipulate objects; none of the ordinary actions of everyday life, let alone the complex actions required for his vocation. To imagine what deafferentation is like, try this experiment suggested by Shaun Gallagher: sit down at a table (something IW could not have done at first) and place your hands below the surface; open and close one hand, close the other and extend a finger; put the open hand over the closed hand, and so forth. You know at all times what your hands are doing and where they are but IW would not know any of this - he would know that he had willed his hands to move but, without vision, would have no idea of what they are doing or where they are located.},
  isbn = {978-90-481-2646-0},
  keywords = {Bodily Expression,Growth Point,Idea Unit,Instrumental Action,Linguistic Meaning},
  langid = {english}
}

@book{mcneillLanguageGesture2000,
  title = {Language and {{Gesture}}},
  author = {McNeill, David},
  date = {2000-08-03},
  publisher = {{Cambridge University Press}},
  abstract = {This landmark study examines the role of gestures in relation to speech and thought. Leading scholars, including psychologists, linguists and anthropologists, offer state-of-the-art analyses to demonstrate that gestures are not merely an embellishment of speech but are integral parts of language itself. Language and Gesture offers a wide range of theoretical approaches, with emphasis not simply on behavioural descriptions but also on the underlying processes. The book has strong cross-linguistic and cross-cultural components, examining gestures by speakers of Mayan, Australian, East Asian as well as English and other European languages. The content is diverse including chapters on gestures during aphasia and severe stuttering, the first emergence of speech-gesture combinations of children, and a section on sign language. In a rapidly growing field of study this volume opens up the agenda for research into a new approach to understanding language, thought and society.},
  eprint = {DRBcMQuSrf8C},
  eprinttype = {googlebooks},
  isbn = {978-0-521-77761-2},
  keywords = {Language Arts & Disciplines / Linguistics / General,Language Arts & Disciplines / Linguistics / Sociolinguistics,Language Arts & Disciplines / Speech},
  langid = {english},
  pagetotal = {424}
}

@article{meltzoffNewbornInfantsImitate1983,
  title = {Newborn Infants Imitate Adult Facial Gestures},
  author = {Meltzoff, A. N. and Moore, M. K.},
  date = {1983},
  journaltitle = {Child Development},
  volume = {54},
  pages = {702--709},
  url = {https://www.jstor.org/stable/1130058?casa_token=-qnMcXXYbLMAAAAA:6Mkw9bqGR5zBtzxZlpZAn5LUZICPlk2Fy2TQxOCLR9HCPgRnK1I2qbICBzd8JKoQINNFy9QKXxaXrWYrnS14K--oSO0Efd-VYtm0IBQZXmri9ZGYNz8B&seq=1#metadata_info_tab_contents},
  urldate = {2019-12-07},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4KFMMS5P\\1130058.html},
  number = {3}
}

@article{mendoza-dentonSemioticLayeringGesture2011,
  title = {Semiotic {{Layering}} through {{Gesture}} and {{Intonation}}: {{A Case Study}} of {{Complementary}} and {{Supplementary Multimodality}} in {{Political Speech}}},
  shorttitle = {Semiotic {{Layering}} through {{Gesture}} and {{Intonation}}},
  author = {Mendoza-Denton, Norma and Jannedy, Stefanie},
  date = {2011-09-01},
  journaltitle = {Journal of English Linguistics},
  shortjournal = {Journal of English Linguistics},
  volume = {39},
  pages = {265--299},
  issn = {0075-4242},
  doi = {10.1177/0075424211405941},
  url = {https://doi.org/10.1177/0075424211405941},
  urldate = {2020-01-26},
  abstract = {Face-to-face communication is multimodal. In face-to-face interaction scholars can observe the interplay of several “semiotic layers,” modalities of information such as syntax, discourse structure, gesture, and intonation. The authors explore the role of gesture in structuring and aligning information in spoken discourse through a study of (1) the complementary co-occurrence of gestural apices and intonational pitch accents and (2) the supplementary co-occurrence of metaphorical gestures and elements in discourse. In the naturally occurring political speech situation the authors examine, metaphorical spatialization through gesture is key in indexing contextual relationships among the speaker, the politicians or government, and other external forces. The use of gestures simultaneously aligns with intonation and metaphorically manipulates political entities in space. Discourse context and social meaning are thus constructed together through the spoken and gestural channels and are supported through fine-grained structural alignment between intonation and gesture.},
  keywords = {embodiment,gesture,intonation,multimodality,political speech,public sphere,semiotics,spoken discourse},
  langid = {english},
  number = {3}
}

@article{michelettaMulticomponentMultimodalLipsmacking2013,
  title = {Multicomponent and {{Multimodal Lipsmacking}} in {{Crested Macaques}} ( {{{\emph{Macaca}}}}{\emph{ Nigra}} ): {{Lipsmacking Behavior}} in {{Crested Macaques}}},
  shorttitle = {Multicomponent and {{Multimodal Lipsmacking}} in {{Crested Macaques}} ( {{{\emph{Macaca}}}}{\emph{ Nigra}} )},
  author = {Micheletta, Jérôme and Engelhardt, Antje and Matthews, Lee and Agil, Muhammad and Waller, Bridget M.},
  date = {2013-07},
  journaltitle = {American Journal of Primatology},
  volume = {75},
  pages = {763--773},
  issn = {02752565},
  doi = {10.1002/ajp.22105},
  url = {http://doi.wiley.com/10.1002/ajp.22105},
  urldate = {2019-09-17},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DFIKF4E2\\Micheletta et al. - 2013 - Multicomponent and Multimodal Lipsmacking in Crest.pdf},
  langid = {english},
  number = {7}
}

@article{morillonMotorOriginTemporal2017,
  title = {Motor Origin of Temporal Predictions in Auditory Attention},
  author = {Morillon, Benjamin and Baillet, Sylvain},
  date = {2017-10-17},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {114},
  pages = {E8913-E8921},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1705373114},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1705373114},
  urldate = {2019-09-17},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FV8UVQLQ\\Morillon and Baillet - 2017 - Motor origin of temporal predictions in auditory a.pdf},
  langid = {english},
  number = {42}
}

@article{motamediEvolvingArtificialSign2019,
  title = {Evolving Artificial Sign Languages in the Lab: {{From}} Improvised Gesture to Systematic Sign},
  shorttitle = {Evolving Artificial Sign Languages in the Lab},
  author = {Motamedi, Yasamin and Schouwstra, Marieke and Smith, Kenny and Culbertson, Jennifer and Kirby, Simon},
  date = {2019-11-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {192},
  pages = {103964},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.05.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027719301234},
  urldate = {2019-12-03},
  abstract = {Recent work on emerging sign languages provides evidence for how key properties of linguistic systems are created. Here we use laboratory experiments to investigate the contribution of two specific mechanisms—interaction and transmission—to the emergence of a manual communication system in silent gesturers. We show that the combined effects of these mechanisms, rather than either alone, maintain communicative efficiency, and lead to a gradual increase of regularity and systematic structure. The gestures initially produced by participants are unsystematic and resemble pantomime, but come to develop key language-like properties similar to those documented in newly emerging sign systems.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9QYZU2GS\\Motamedi et al. - 2019 - Evolving artificial sign languages in the lab Fro.pdf},
  keywords = {Interaction,Iterated learning,Language evolution,Sign language,Silent gesture,Transmission},
  langid = {english}
}

@inproceedings{mueenExtractingOptimalPerformance2016,
  title = {Extracting {{Optimal Performance}} from {{Dynamic Time Warping}}},
  booktitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Mueen, Abdullah and Keogh, Eamonn},
  date = {2016},
  pages = {2129--2130},
  publisher = {{ACM}},
  location = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2945383},
  url = {http://doi.acm.org/10.1145/2939672.2945383},
  urldate = {2019-06-24},
  abstract = {Dynamic Time Warping (DTW) is a distance measure that compares two time series after optimally aligning them. DTW is being used for decades in thousands of academic and industrial projects despite the very expensive computational complexity, O(n2). These applications include data mining, image processing, signal processing, robotics and computer graphics among many others. In spite of all this research effort, there are many myths and misunderstanding about DTW in the literature, for example "it is too slow to be useful" or "the warping window size does not matter much." In this tutorial, we correct these misunderstandings and we summarize the research efforts in optimizing both the efficiency and effectiveness of both the basic DTW algorithm, and of the higher-level algorithms that exploit DTW such as similarity search, clustering and classification. We will discuss variants of DTW such as constrained DTW, multidimensional DTW and asynchronous DTW, and optimization techniques such as lower bounding, early abandoning, run-length encoding, bounded approximation and hardware optimization. We will discuss a multitude of application areas including physiological monitoring, social media mining, activity recognition and animal sound processing. The optimization techniques are generalizable to other domains on various data types and problems.},
  isbn = {978-1-4503-4232-2},
  keywords = {approximation,dynamic time warping,lower bounds,pruning,time series},
  series = {{{KDD}} '16}
}

@article{mullerGestureSignCataclysmic2018,
  title = {Gesture and {{Sign}}: {{Cataclysmic Break}} or {{Dynamic Relations}}?},
  shorttitle = {Gesture and {{Sign}}},
  author = {Müller, Cornelia},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {9},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2018.01651},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.01651/full},
  urldate = {2019-08-26},
  abstract = {Abstract The goal of the article is to offer a framework against which relations between gesture and sign can be systematically explored beyond the current literature. It does so by a) reconstructing the history of the discussion in the field of gesture studies, focusing on three leading positions (Kendon, McNeill, Goldin-Meadow); and b) by formulating a position to illustrate how this can be achieved. The paper concludes by emphasizing the need for systematic cross-linguistic research on multimodal use of language in its signed and spoken forms.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KM4GGRE6\\Müller - 2018 - Gesture and Sign Cataclysmic Break or Dynamic Rel.pdf},
  keywords = {Conventionalization processes,Emblems,Gesture and sign,McNeill's gesture-sign continua,Multimodality of language use,Recurrent gestures,Silent gestures,Singular gestures},
  langid = {english}
}

@incollection{navarrettaBigDataMultimodal2019,
  title = {Big {{Data}} and {{Multimodal Communication}}: {{A Perspective View}}},
  booktitle = {Innovations in {{Big Data Mining}} and {{Embedded Knowledge}}, {{A}}. {{Esposito}} et al. (Eds.),},
  author = {Navarretta, Costanza and Oemig, Lucretia},
  date = {2019},
  pages = {167--184},
  publisher = {{Springer Nature}},
  location = {{Switzerland}},
  url = {https://doi.org/10.1007/978-3-030-15939-9_9}
}

@incollection{ohalaRespiratoryActivitySpeech1990,
  title = {Respiratory {{Activity}} in Speech},
  booktitle = {Speech Production and Speech Modeling ({{Eds}}. {{W}}. {{J}}. {{Hardcastle}} and {{A}}. {{Marchal}})},
  author = {Ohala, J. J.},
  date = {1990},
  pages = {22--53},
  publisher = {{Kluwer}},
  location = {{Dordrecht}}
}

@article{oleszkiewiczVoicebasedAssessmentsTrustworthiness2017,
  title = {Voice-Based Assessments of Trustworthiness, Competence, and Warmth in Blind and Sighted Adults},
  author = {Oleszkiewicz, Anna and Pisanski, Katarzyna and Lachowicz-Tabaczek, Kinga and Sorokowska, Agnieszka},
  date = {2017-06-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {24},
  pages = {856--862},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1146-y},
  url = {https://doi.org/10.3758/s13423-016-1146-y},
  urldate = {2019-11-30},
  abstract = {The study of voice perception in congenitally blind individuals allows researchers rare insight into how a lifetime of visual deprivation affects the development of voice perception. Previous studies have suggested that blind adults outperform their sighted counterparts in low-level auditory tasks testing spatial localization and pitch discrimination, as well as in verbal speech processing; however, blind persons generally show no advantage in nonverbal voice recognition or discrimination tasks. The present study is the first to examine whether visual experience influences the development of social stereotypes that are formed on the basis of nonverbal vocal characteristics (i.e., voice pitch). Groups of 27 congenitally or early-blind adults and 23 sighted controls assessed the trustworthiness, competence, and warmth of men and women speaking a series of vowels, whose voice pitches had been experimentally raised or lowered. Blind and sighted listeners judged both men’s and women’s voices with lowered pitch as being more competent and trustworthy than voices with raised pitch. In contrast, raised-pitch voices were judged as being warmer than were lowered-pitch voices, but only for women’s voices. Crucially, blind and sighted persons did not differ in their voice-based assessments of competence or warmth, or in their certainty of these assessments, whereas the association between low pitch and trustworthiness in women’s voices was weaker among blind than sighted participants. This latter result suggests that blind persons may rely less heavily on nonverbal cues to trustworthiness compared to sighted persons. Ultimately, our findings suggest that robust perceptual associations that systematically link voice pitch to the social and personal dimensions of a speaker can develop without visual input.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8ERSAHDE\\Oleszkiewicz et al. - 2017 - Voice-based assessments of trustworthiness, compet.pdf},
  keywords = {Blind,Nonverbal communication,Sightedness,Social perception,Voice perception},
  langid = {english},
  number = {3}
}

@article{ortegaSystematicMappingsSemantic2019,
  title = {Systematic Mappings between Semantic Categories and Types of Iconic Representations in the Manual Modality: {{A}} Normed Database of Silent Gesture},
  shorttitle = {Systematic Mappings between Semantic Categories and Types of Iconic Representations in the Manual Modality},
  author = {Ortega, Gerardo and Özyürek, Aslı},
  date = {2019-02-20},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01204-6},
  url = {https://doi.org/10.3758/s13428-019-01204-6},
  urldate = {2019-12-04},
  abstract = {An unprecedented number of empirical studies have shown that iconic gestures—those that mimic the sensorimotor attributes of a referent—contribute significantly to language acquisition, perception, and processing. However, there has been a lack of normed studies describing generalizable principles in gesture production and in comprehension of the mappings of different types of iconic strategies (i.e., modes of representation; Müller, 2013). In Study 1 we elicited silent gestures in order to explore the implementation of different types of iconic representation (i.e., acting, representing, drawing, and personification) to express concepts across five semantic domains. In Study 2 we investigated the degree of meaning transparency (i.e., iconicity ratings) of the gestures elicited in Study 1. We found systematicity in the gestural forms of 109 concepts across all participants, with different types of iconicity aligning with specific semantic domains: Acting was favored for actions and manipulable objects, drawing for nonmanipulable objects, and personification for animate entities. Interpretation of gesture–meaning transparency was modulated by the interaction between mode of representation and semantic domain, with some couplings being more transparent than others: Acting yielded higher ratings for actions, representing for object-related concepts, personification for animate entities, and drawing for nonmanipulable entities. This study provides mapping principles that may extend to all forms of manual communication (gesture and sign). This database includes a list of the most systematic silent gestures in the group of participants, a notation of the form of each gesture based on four features (hand configuration, orientation, placement, and movement), each gesture’s mode of representation, iconicity ratings, and professionally filmed videos that can be used for experimental and clinical endeavors.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XJWS3QGB\\Ortega and Özyürek - 2019 - Systematic mappings between semantic categories an.pdf},
  keywords = {Iconicity,Modes of representation,Normed database,Perception of iconicity,Silent gesture},
  langid = {english}
}

@article{ozcaliskanDoesLanguageShape2016,
  title = {Does Language Shape Silent Gesture?},
  author = {Özçalışkan, Şeyda and Lucero, Ché and Goldin-Meadow, Susan},
  date = {2016-03-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {148},
  pages = {10--18},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2015.12.001},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027715301153},
  urldate = {2019-08-26},
  abstract = {Languages differ in how they organize events, particularly in the types of semantic elements they express and the arrangement of those elements within a sentence. Here we ask whether these cross-linguistic differences have an impact on how events are represented nonverbally; more specifically, on how events are represented in gestures produced without speech (silent gesture), compared to gestures produced with speech (co-speech gesture). We observed speech and gesture in 40 adult native speakers of English and Turkish (N=20/per language) asked to describe physical motion events (e.g., running down a path)—a domain known to elicit distinct patterns of speech and co-speech gesture in English- and Turkish-speakers. Replicating previous work (Kita \& Özyürek, 2003), we found an effect of language on gesture when it was produced with speech—co-speech gestures produced by English-speakers differed from co-speech gestures produced by Turkish-speakers. However, we found no effect of language on gesture when it was produced on its own—silent gestures produced by English-speakers were identical in how motion elements were packaged and ordered to silent gestures produced by Turkish-speakers. The findings provide evidence for a natural semantic organization that humans impose on motion events when they convey those events without language.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\YBUH3TR5\\Özçalışkan et al. - 2016 - Does language shape silent gesture.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BSUWUPFT\\S0010027715301153.html},
  keywords = {Cross-linguistic differences,Gesture,Language and cognition,Motion events}
}

@book{paoloLinguisticBodiesContinuity2018,
  title = {Linguistic {{Bodies}}: {{The Continuity}} between {{Life}} and {{Language}}},
  shorttitle = {Linguistic {{Bodies}}},
  author = {Paolo, Ezequiel A. Di and Cuffari, Elena Clare and Jaegher, Hanne De},
  date = {2018-11-06},
  publisher = {{MIT Press}},
  abstract = {A novel theoretical framework for an embodied, non-representational approach to language that extends and deepens enactive theory, bridging the gap between sensorimotor skills and language.Linguistic Bodies offers a fully embodied and fully social treatment of human language without positing mental representations. The authors present the first coherent, overarching theory that connects dynamical explanations of action and perception with language. Arguing from the assumption of a deep continuity between life and mind, they show that this continuity extends to language. Expanding and deepening enactive theory, they offer a constitutive account of language and the co-emergent phenomena of personhood, reflexivity, social normativity, and ideality. Language, they argue, is not something we add to a range of existing cognitive capacities but a new way of being embodied. Each of us is a linguistic body in a community of other linguistic bodies. The book describes three distinct yet entangled kinds of human embodiment, organic, sensorimotor, and intersubjective; it traces the emergence of linguistic sensitivities and introduces the novel concept of linguistic bodies; and it explores the implications of living as linguistic bodies in perpetual becoming, applying the concept of linguistic bodies to questions of language acquisition, parenting, autism, grammar, symbol, narrative, and gesture, and to such ethical concerns as microaggression, institutional speech, and pedagogy.},
  eprint = {_rVyDwAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-0-262-03816-4},
  keywords = {Language Arts & Disciplines / Linguistics / General,Psychology / Cognitive Psychology & Cognition},
  langid = {english},
  pagetotal = {428}
}

@article{parrellSpatiotemporalCouplingSpeech2014,
  title = {Spatiotemporal Coupling between Speech and Manual Motor Actions},
  author = {Parrell, Benjamin and Goldstein, Louis and Lee, Sungbok and Byrd, Dani},
  date = {2014-01-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {42},
  pages = {1--11},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2013.11.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447013000776},
  urldate = {2019-04-16},
  abstract = {Much evidence has been found for pervasive links between the manual and speech motor systems, including evidence from infant development, deictic pointing, and repetitive tapping and speaking tasks. We expand on the last of these paradigms to look at intra- and cross-modal effects of emphatic stress, as well as the effects of coordination in the absence of explicit rhythm. In this study, subjects repeatedly tapped their finger and synchronously repeated a single spoken syllable. On each trial, subjects placed an emphatic stress on one finger tap or one spoken syllable. Results show that both movement duration and magnitude are affected by emphatic stress regardless of whether that stress is in the same domain (e.g., effects on the oral articulators when a spoken repetition is stressed) or across domains (e.g., effects on the oral articulators when a tap is stressed). Though the size of the effects differs between intra-and cross-domain emphases, the implementation of stress affects both motor domains, indicating a tight connection. This close coupling is seen even in the absence of stress, though it is highlighted under stress. The results of this study support the idea that implementation of prosody is not domain-specific but relies on general aspects of the motor system.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\UZIY6KZ5\\Parrell et al. - 2014 - Spatiotemporal coupling between speech and manual .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BRJQDEAI\\S0095447013000776.html}
}

@article{partanCommunicationGoesMultimodal1999,
  title = {Communication {{Goes Multimodal}}},
  author = {Partan, Sarah and Marler, Peter},
  date = {1999-02-26},
  journaltitle = {Science},
  volume = {283},
  pages = {1272--1273},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.283.5406.1272},
  url = {https://science.sciencemag.org/content/283/5406/1272},
  urldate = {2019-11-15},
  abstract = {{$<$}p{$>$} Communication depends on the simultaneous receipt of multiple sensory stimuli. The Perspective by Partan and Marler in this week9s issue postulates a new classification system for multimodal sensory signals. Combinations of sensory signals are classified according to the behavioral responses they elicit. {$<$}/p{$>$}},
  eprint = {10084931},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EZ48YMV6\\1272.html},
  langid = {english},
  number = {5406}
}

@article{paxtonArgumentDisruptsInterpersonal2013,
  title = {Argument Disrupts Interpersonal Synchrony},
  author = {Paxton, A. and Dale, Rick},
  date = {2013-11-01},
  journaltitle = {Quarterly Journal of Experimental Psychology},
  shortjournal = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  pages = {2092--2102},
  issn = {1747-0218},
  doi = {10.1080/17470218.2013.853089},
  url = {https://doi.org/10.1080/17470218.2013.853089},
  urldate = {2019-11-30},
  abstract = {Research on interpersonal convergence and synchrony characterizes the way in which interacting individuals come to have more similar affect, behaviour, and cognition over time. Although its dynamics have been explored in many settings, convergence during conflict has been almost entirely overlooked. We present a simple but ecologically valid study comparing how different situational contexts that highlight affiliation and argument impact interpersonal convergence of body movement and to what degree emotional states affect convergence in both conversational settings. Using linear mixed-effect models, we found that in-phase bodily synchrony decreases significantly during argument. However, affective changes did not significantly predict changes in levels of interpersonal synchrony, suggesting that differences in affect valences between affiliation and argument cannot solely explain our results.},
  langid = {english},
  number = {11}
}

@article{paxtonInterpersonalMovementSynchrony2017,
  title = {Interpersonal Movement Synchrony Responds to High- and Low-{{Level}} Conversational Constraints},
  author = {Paxton, A. and Dale, R.},
  date = {2017},
  journaltitle = {Frontiers in Psychology},
  doi = {https://doi.org/10.3389/fpsyg.2017.01135},
  url = {https://www.jair.org/index.php/jair/article/view/10536},
  urldate = {2019-06-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\B2NMR4A4\\10536.html}
}

@article{paxtonMultimodalNetworksInterpersonal,
  title = {Multimodal {{Networks}} of {{Interpersonal Interaction}} and {{Conversational Contexts}}},
  author = {Paxton, Alexandra and Dale, Rick},
  pages = {7},
  abstract = {In interpersonal interaction, the terms synchrony or alignment refer to the way in which communication channels like speech or body movement become intertwined over time, both across interlocutors and within a single individual. A recent trend in alignment research has targeted multimodal alignment, exploring how various communication channels affect one another over time (e.g., Louwerse et al., 2012). While existing research has made significant progress in mapping multimodal alignment during task-based or positively valenced interactions, little is known about the dynamics of multimodal alignment during conflict. We visualize multimodal alignment during naturalistic affiliative and argumentative interactions as networks based on analyses of body movement and speech. Broadly, we find that conversational contexts strongly impact the ways in which interlocutors’ movement and speech systems self-organize interpersonally and intrapersonally.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5BD477BP\\Paxton and Dale - Multimodal Networks of Interpersonal Interaction a.pdf},
  langid = {english}
}

@article{pearsonGestureSonicEvent2013,
  title = {Gesture and the {{Sonic Event}} in {{Karnatak Music}}},
  author = {Pearson, Lara},
  date = {2013-10-24},
  journaltitle = {Empirical Musicology Review},
  volume = {8},
  pages = {2--14},
  issn = {1559-5749},
  doi = {10.18061/emr.v8i1.3918},
  url = {http://emusicology.org/article/view/3918},
  urldate = {2019-09-20},
  abstract = {This paper presents an analysis of the relationship between gesture and music in the context of a Karnatak vocal lesson recorded in Tamil Nadu, South India in September 2011. The study aims to examine instances of correspondence between gesture and sonic event that occur during the lesson. Through this analysis the paper aims to contribute to the wider debate on the factors that determine gesture. Shape and trajectory are used in this study as means of describing and comparing gestures. The teacher’s hand movements are tracked and traced rendering the gestures as static shapes in still images, and developing lines in moving images. The correspondences found between gestures and sonic features are discussed in relation to the physical movement required to produce the music. In addition, the circumstances in which correspondence is not found are analyzed and the extent to which the dynamic form of gesture is also influenced by the phrase as a whole is emphasized.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TCRJQNPC\\Pearson - 2013 - Gesture and the Sonic Event in Karnatak Music.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\X4RGCFSX\\3918.html},
  keywords = {gesture,movement,shape,South Indian music},
  langid = {english},
  number = {1}
}

@book{perez-pereiraLanguageDevelopmentSocial1999,
  title = {Language {{Development}} and {{Social Interaction}} in {{Blind Children}}},
  author = {Perez-Pereira, M. and Conti-Ramsden, G.},
  date = {1999},
  publisher = {{Psychology Press}},
  location = {{New York}}
}

@incollection{perrierMotorEquivalenceSpeech2015,
  title = {Motor {{Equivalence}} in {{Speech Production}}},
  booktitle = {The {{Handbook}} of {{Speech Production}}},
  author = {Perrier, Pascal and Fuchs, Susanne},
  date = {2015},
  pages = {223--247},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118584156.ch11},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118584156.ch11},
  urldate = {2019-08-08},
  abstract = {In this chapter, the concepts of “motor equivalence” and “degrees of freedom” are first described and illustrated with a few examples of motor tasks in general and of speech production tasks in particular. After this, methods used to investigate experimentally motor equivalence phenomena in speech production are presented. These are mainly paradigms that perturb the perception-action loop during on-going speech, either by limiting the degrees of freedom of the speech motor system, or by changing the physical conditions of speech production or by modifying the feedback information. Examples are provided for each of these approaches. Implications of these studies for a better understanding of speech production and its interactions with speech perception are presented in a final section of the chapter. Implications are mainly related to characterizing the mechanisms underlying interarticulatory coordination and to the analysis of speech production goals.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MVB5X94U\\Perrier and Fuchs - 2015 - Motor Equivalence in Speech Production.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\AA9TEB2L\\9781118584156.html},
  isbn = {978-1-118-58415-6},
  keywords = {auditory feedback perturbations,motor equivalence,motor goals,motor perturbations,speech coordination,speech motor control},
  langid = {english}
}

@article{petroneRelationsSubglottalPressure2017,
  title = {Relations among Subglottal Pressure, Breathing, and Acoustic Parameters of Sentence-Level Prominence in {{German}}},
  author = {Petrone, Caterina and Fuchs, Susanne and Koenig, Laura L.},
  date = {2017-03-01},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {141},
  pages = {1715--1725},
  issn = {0001-4966},
  doi = {10.1121/1.4976073},
  url = {https://asa.scitation.org/doi/abs/10.1121/1.4976073},
  urldate = {2019-05-05},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\73SSTAQS\\Petrone et al. - 2017 - Relations among subglottal pressure, breathing, an.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BCEL8HVU\\1.html},
  number = {3}
}

@book{pikovskySynchronizationUniversalConcept2001,
  title = {Synchronization: {{A Universal Concept}} in {{Nonlinear Sciences}}},
  author = {Pikovsky, A and Kurths, J and Rosenblum, M},
  date = {2001},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, Mass}},
  url = {https://books.google.nl/books?hl=en&lr=&id=FuIv845q3QUC&oi=fnd&pg=PP1&dq=pikovsky+synchronization&ots=RM-tIgHgU7&sig=PHOyMBVgtLIao8rdkP5bCTVKwo0#v=onepage&q=pikovsky%20synchronization&f=false},
  urldate = {2019-07-11},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8IGPB5M9\\books.html}
}

@software{pinheiroNlmeLinearNonlinear2019,
  title = {Nlme: {{Linear}} and Nonlinear Mixed Effects Models},
  author = {Pinheiro, J. and Bates, D. and DebRoy, S. and Sarkar, D. and R Team, R. C.},
  date = {2019},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\F38LDCK6\\Cayuela - Modelos lineales mixtos en R.pdf}
}

@article{pisanskiCanBlindPersons2016,
  title = {Can Blind Persons Accurately Assess Body Size from the Voice?},
  author = {Pisanski, Katarzyna and Oleszkiewicz, Anna and Sorokowska, Agnieszka},
  date = {2016-04-30},
  journaltitle = {Biology Letters},
  shortjournal = {Biology Letters},
  volume = {12},
  pages = {20160063},
  doi = {10.1098/rsbl.2016.0063},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rsbl.2016.0063},
  urldate = {2019-11-30},
  abstract = {Vocal tract resonances provide reliable information about a speaker's body size that human listeners use for biosocial judgements as well as speech recognition. Although humans can accurately assess men's relative body size from the voice alone, how this ability is acquired remains unknown. In this study, we test the prediction that accurate voice-based size estimation is possible without prior audiovisual experience linking low frequencies to large bodies. Ninety-one healthy congenitally or early blind, late blind and sighted adults (aged 20–65) participated in the study. On the basis of vowel sounds alone, participants assessed the relative body sizes of male pairs of varying heights. Accuracy of voice-based body size assessments significantly exceeded chance and did not differ among participants who were sighted, or congenitally blind or who had lost their sight later in life. Accuracy increased significantly with relative differences in physical height between men, suggesting that both blind and sighted participants used reliable vocal cues to size (i.e. vocal tract resonances). Our findings demonstrate that prior visual experience is not necessary for accurate body size estimation. This capacity, integral to both nonverbal communication and speech perception, may be present at birth or may generalize from broader cross-modal correspondences.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Z6GEIHL2\\Pisanski et al. - 2016 - Can blind persons accurately assess body size from.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\FR5XWK37\\rsbl.2016.html},
  number = {4}
}

@article{pisanskiReturnOzVoice2014,
  title = {Return to {{Oz}}: Voice Pitch Facilitates Assessments of Men's Body Size},
  shorttitle = {Return to {{Oz}}},
  author = {Pisanski, Katarzyna and Fraccaro, Paul J. and Tigue, Cara C. and O'Connor, Jillian J. M. and Feinberg, David R.},
  date = {2014-08},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {40},
  pages = {1316--1331},
  issn = {1939-1277},
  doi = {10.1037/a0036956},
  abstract = {Listeners associate low voice pitch (fundamental frequency and/or harmonics) and formants (vocal-tract resonances) with large body size. Although formants reliably predict size within sexes, pitch does not reliably predict size in groups of same-sex adults. Voice pitch has therefore long been hypothesized to confound within-sex size assessment. Here we performed a knockout test of this hypothesis using whispered and 3-formant sine-wave speech devoid of pitch. Listeners estimated the relative size of men with above-chance accuracy from voiced, whispered, and sine-wave speech. Critically, although men's pitch and physical height were unrelated, the accuracy of listeners' size assessments increased in the presence rather than absence of pitch. Size assessments based on relatively low pitch yielded particularly high accuracy (70\%-80\%). Results of Experiment 2 revealed that amplitude, noise, and signal degradation of unvoiced speech could not explain this effect; listeners readily perceived formant shifts in manipulated whispered speech. Rather, in Experiment 3, we show that the denser harmonic spectrum provided by low pitch allowed for better resolution of formants, aiding formant-based size assessment. These findings demonstrate that pitch does not confuse body size assessment as has been previously suggested, but instead facilitates accurate size assessment by providing a carrier signal for vocal-tract resonances.},
  eprint = {24933617},
  eprinttype = {pmid},
  keywords = {Adolescent,Adult,Body Size,Female,Humans,Male,Pitch Perception,Speech Perception,Voice,Young Adult},
  langid = {english},
  number = {4}
}

@article{pisanskiVoiceModulationWindow2016,
  title = {Voice Modulation: {{A}} Window into the Origins of Guman Vocal Control?},
  shorttitle = {Voice {{Modulation}}},
  author = {Pisanski, Katarzyna and Cartei, Valentina and McGettigan, Carolyn and Raine, Jordan and Reby, David},
  date = {2016-04-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {20},
  pages = {304--318},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2016.01.002},
  url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(16)00020-6},
  urldate = {2019-10-17},
  eprint = {26857619},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CIL5832Q\\S1364-6613(16)00020-6.html},
  keywords = {formant scaling,fundamental frequency,nonverbal vocal communication,source–filter theory,speech evolution},
  langid = {english},
  number = {4}
}

@online{PitchAccentTrajectories,
  title = {Pitch {{Accent Trajectories}} across {{Different Conditions}} of {{Visibility}} and {{Information Structure}} - {{Evidence}} from {{Spontaneous Dyadic Interaction}}},
  url = {https://pub.uni-bielefeld.de/record/2936369},
  urldate = {2019-08-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3RXUK7T3\\2936369.html}
}

@inproceedings{pouwAcousticSpecificationUpper2019,
  title = {Acoustic Specification of Upper Limb Movement in Voicing},
  booktitle = {Proceedings of the 6th Meeting of {{Gesture}} and {{Speech}} in {{Interaction}}},
  author = {Pouw, W. and Paxton, A. and Harrison, S. J. and Dixon, J. A.},
  date = {2019},
  pages = {75--80},
  publisher = {{Universitaetsbibliothek Paderborn}},
  location = {{Paderborn, Germany}},
  doi = {10.17619/UNIPB/1-812},
  url = {https://psyarxiv.com/5rcdu/},
  urldate = {2019-05-05},
  abstract = {Hand gestures communicate complex information to listeners through the visual information created by movement. In a recent study, however, we found that there are also direct biomechanical effects of high-impetus upper limb movement on voice acoustics. Here we explored whether listeners could detect information about movement in voice acoustics of another person. In this exploratory study, participants listened to a recording of a vocalizer who was simultaneously producing low- (wrist movement) or high- (arm movement) impetus movements at three different tempos. Listeners were asked to synchronize their own movement (wrist or arm movement) with that of the vocalizer. Listeners coupled with the frequency of the vocalizer arm (but not wrist) movements, and showed phase-coupling with vocalizer arm (but not wrist) movements. However, we found that this synchronization occurred regardless of whether the listener was moving their wrist or arm. This study shows that, in principle, there is acoustic specification of arm movements in voicing, but not wrist movements. These results, if replicated, provide novel insight into the possible interpersonal functions of gesture acoustics, which may lie in communicating bodily states. The second part of the paper is a pre-registration for the confirmatory study that will assess the research question in a larger sample with more diverse and naturalistic stimuli.},
  eventtitle = {{{GESPIN}} 6},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\B75VYI5G\\Pouw et al. - 2019 - Acoustic specification of upper limb movement in v.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BUY5LQSE\\5rcdu.html}
}

@article{pouwEnergyFlowsGesturespeech2019,
  title = {Energy Flows in Gesture-Speech Physics: {{Exploratory}} Findings and Pre-Registration of Confirmatory Analysis},
  shorttitle = {Energy Flows in Gesture-Speech Physics},
  author = {Pouw, W. and Harrison, S. J. and Esteve-Gibert, N. and Dixon, J. A.},
  date = {2019},
  doi = {10.31234/osf.io/c7456},
  url = {https://psyarxiv.com/c7456/},
  urldate = {2019-10-13},
  abstract = {A well-known phenomenon of multimodal language is the synchronous coupling of prosodic contours in speech with salient kinematic changes in co-speech hand-gesture motions. Invariably, such coupling has been rendered by psychologists to require a dedicated neural-cognitive mechanism preplanning speech and gesture trajectories. Recently, in a continuous vocalization task, it was found that acoustic peaks unintentionally appear in vocalizations when gesture motions reach peaks in physical impetus, suggesting a biomechanical basis for gesture-speech synchrony (Pouw, Harrison, \& Dixon, 2019). However, from this rudimentary study it is still difficult to draw strong conclusions about gesture-speech dynamics in (more) complex speech and the precise biomechanical nature of these effects. Here we assess how the timing of physical impetus of a gesture relates to its effect on acoustic parameters of mono-syllabic consonant-vowel (CV) vocalization(/pa/). Furthermore, we assess how chest-wall kinematics is affected by gesturing, and whether this modulates the effect of gestures on acoustics. In the current exploratory analysis, we analyze a subset (N = 4) of an already collected dataset (N = 36), which serves as the basis for a pre-registration of the confirmatory analyses yet to be completed. Here we provide exploratory evidence that gestures affect acoustics (amplitude envelope and F0) as well as chest-wall kinematics during mono-syllabic vocalizations. These effects are more extreme when a gesture’s peak impetus occurs closer to the center of the vowel vocalization event. If the current findings can be replicated in confirmatory fashion, there is a more compelling case to be made that gesture-speech physics is important facet of multimodal synchrony.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\J4JHP6YV\\Pouw et al. - 2019 - Energy flows in gesture-speech physics Explorator.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\RTTD5PQQ\\c7456.html}
}

@unpublished{pouwEnergyFlowsGesturespeech2019a,
  title = {Energy Flows in Gesture-Speech Physics: {{The}} Respiratory-Vocal System and Its Coupling with Hand Gestures},
  author = {Pouw, W. and Esteve-Gibert, N. and Harrison, S. H. and Dixon, J. A.},
  date = {2019},
  url = {https://psyarxiv.com/rnpav},
  urldate = {2019-12-18},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XS8MQDHG\\rnpav.html}
}

@article{pouwEntrainmentModulationGesture2019,
  title = {Entrainment and Modulation of Gesture–Speech Synchrony under Delayed Auditory Feedback},
  author = {Pouw, W. and Dixon, J. A.},
  date = {2019},
  journaltitle = {Cognitive Science},
  volume = {43},
  pages = {e12721},
  issn = {1551-6709},
  doi = {10.1111/cogs.12721},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12721},
  urldate = {2019-04-16},
  abstract = {Gesture–speech synchrony re-stabilizes when hand movement or speech is disrupted by a delayed feedback manipulation, suggesting strong bidirectional coupling between gesture and speech. Yet it has also been argued from case studies in perceptual–motor pathology that hand gestures are a special kind of action that does not require closed-loop re-afferent feedback to maintain synchrony with speech. In the current pre-registered within-subject study, we used motion tracking to conceptually replicate McNeill's () classic study on gesture–speech synchrony under normal and 150 ms delayed auditory feedback of speech conditions (NO DAF vs. DAF). Consistent with, and extending McNeill's original results, we obtain evidence that (a) gesture-speech synchrony is more stable under DAF versus NO DAF (i.e., increased coupling effect), (b) that gesture and speech variably entrain to the external auditory delay as indicated by a consistent shift in gesture-speech synchrony offsets (i.e., entrainment effect), and (c) that the coupling effect and the entrainment effect are co-dependent. We suggest, therefore, that gesture–speech synchrony provides a way for the cognitive system to stabilize rhythmic activity under interfering conditions.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\5VSDHX34\\Pouw and Dixon - 2019 - Entrainment and Modulation of Gesture–Speech Synch.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8B7CACLJ\\cogs.html},
  keywords = {Cross-wavelet analysis,Delayed auditory feedback,Hand-gesture,Speech,Synchrony},
  langid = {english},
  number = {3}
}

@article{pouwGestureNetworksIntroducing2019,
  title = {Gesture Networks: {{Introducing}} Dynamic Time Warping and Network Analyses for the Kinematic Study of Gesture Ensembles},
  author = {Pouw, W. and Dixon, J. A.},
  date = {2019},
  journaltitle = {Discourse Processes},
  doi = {10.1080/0163853X.2019.1678967},
  url = {https://psyarxiv.com/hbnt2},
  urldate = {2019-08-07},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\JYA5DTB6\\hbnt2.html}
}

@article{pouwGesturespeechPhysicsBiomechanical2019,
  title = {Gesture-Speech Physics: {{The}} Biomechanical Basis of the Emergence of Gesture-Speech Synchrony},
  author = {Pouw, W. and Harrison, S. H. and Dixon, J. A.},
  date = {2019},
  journaltitle = {Journal of Experimental Psychology: General},
  doi = {10.1037/xge0000646},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\455PZW9L\\default.html}
}

@software{pouwMaterialsTutorialGespin20192019,
  title = {Materials {{Tutorial Gespin2019}} - {{Using}} Video-Based Motion Tracking to Quantify Speech-Gesture Synchrony},
  author = {Pouw, W. and Trujillo, J. P.},
  date = {2019},
  url = {10.17605/OSF.IO/RXB8J}
}

@article{pouwPhysicalBasisGesturespeech2018,
  title = {The Physical Basis of Gesture-Speech Synchrony: {{Exploratory}} Study and Pre-Registration},
  author = {Pouw, W. and Harrison, S. and Dixon, J.},
  date = {2018},
  journaltitle = {PsyArXiv},
  url = {https://psyarxiv.com/9fzsv},
  urldate = {2019-08-09},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QQU48XH5\\9fzsv.html}
}

@article{pouwQuantificationGesturespeechSynchrony2019,
  title = {The Quantification of Gesture-Speech Synchrony: {{A}} Tutorial and Validation of Multi-Modal Data Acquisition Using Device-Based and Video-Based Motion Tracking},
  author = {Pouw, W. and Trujillo, J. and Dixon, J.A.},
  date = {2019},
  journaltitle = {Behavior Research Methods},
  doi = {https://doi.org/10.3758/s13428-019-01271-9},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\TAEEFQA8\\jm3hk.html}
}

@inproceedings{pouwQuantifyingGesturespeechSynchrony2019,
  title = {Quantifying Gesture-Speech Synchrony},
  booktitle = {Proceedings of the 6th Meeting of {{Gesture}} and {{Speech}} in {{Interaction}}},
  author = {Pouw, W. and Dixon, J. A.},
  date = {2019},
  pages = {68--74},
  publisher = {{Universitaetsbibliothek Paderborn}},
  location = {{Paderborn}},
  doi = {10.17619/UNIPB/1-812},
  eventtitle = {{{GESPIN}} 6}
}

@unpublished{pouwSocialResonanceAcoustic2019,
  title = {Social {{Resonance}}: {{Acoustic Information}} about {{Upper Limb Movement}} in {{Voicing}}},
  author = {Pouw, W. and Paxton, A. and Harrison, S. J. and Dixon, J. A.},
  date = {2019},
  url = {https://psyarxiv.com/ny39e},
  urldate = {2019-06-18},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\CJLYXHKE\\ny39e.html}
}

@article{pouwStabilizingSpeechProduction2018,
  title = {Stabilizing {{Speech Production}} through {{Gesture}}-{{Speech Coordination}}},
  author = {Pouw, W. and De Jonge-Hoekstra, L. and Dixon, J.},
  date = {2018-12-17T16:48:41.054Z},
  doi = {10.31234/osf.io/arzne},
  url = {https://psyarxiv.com/arzne/},
  urldate = {2019-09-20},
  abstract = {Hand-gestures are seamlessly coordinated with speech. Yet, there is only anecdotal support for gestures’ functional role in speech production. Here we explore temporal aspects of speech production when people use hand gesture. We performed exploratory analyses with a naturalistic German-speaking sample from The Bielefeld Speech and Gesture Alignment Corpus (SaGA), which consisted of 67 minutes of narration data and over 500 gesture events (N = 6). We found that the rhythmic timing of speech (defined as the mean and standard deviations of speech onset intervals) is highly correlated with the likelihood of gesturing. Furthermore, we utilized deep learning methods to track gesture motion, and extracted the amplitude envelope of speech, so as to gauge the degree of (continuous) gesture-speech synchrony. We then performed a continuous time-series analysis (recurrence quantification analysis; RQA) to index how temporal properties of speech change when gesture and speech are more or less synchronized. Our analyses revealed that when gesture and speech were more synchronized, the temporal structure of speech was more ordered and less complex, as indexed by classic measures of dynamic temporal stability (e.g., Entropy, Ratio of \%Determinism/Recurrence). We suggest that a fundamental gesture-speech relation is rooted in entrainment, which yields stability in the temporal structure of speech.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\USEMMUQM\\Pouw et al. - 2018 - Stabilizing Speech Production through Gesture-Spee.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\5LEH8GKW\\arzne.html}
}

@article{priceMotionDoesMatter2006,
  title = {Motion Does Matter: An Examination of Speech-Based Text Entry on the Move},
  shorttitle = {Motion Does Matter},
  author = {Price, Kathleen J. and Lin, Min and Feng, Jinjuan and Goldman, Rich and Sears, Andrew and Jacko, Julie A.},
  date = {2006-03-01},
  journaltitle = {Universal Access in the Information Society},
  shortjournal = {Univ Access Inf Soc},
  volume = {4},
  pages = {246--257},
  issn = {1615-5297},
  doi = {10.1007/s10209-005-0006-8},
  url = {https://doi.org/10.1007/s10209-005-0006-8},
  urldate = {2019-10-17},
  abstract = {Desktop interaction solutions are often inappropriate for mobile devices due to small screen size and portability needs. Speech recognition can improve interactions by providing a relatively hands-free solution that can be used in various situations. While mobile systems are designed to be transportable, few have examined the effects of motion on mobile interactions. This paper investigates the effect of motion on automatic speech recognition (ASR) input for mobile devices. Speech recognition error rates (RER) have been examined with subjects walking or seated, while performing text input tasks and the effect of ASR enrollment conditions on RER. The obtained results suggest changes in user training of ASR systems for mobile and seated usage.},
  keywords = {Automatic speech recognition,Mobile device,Motion,Speech recognition errors,Speech-based data entry},
  langid = {english},
  number = {3}
}

@book{prietoDevelopmentProsodyFirst2018,
  title = {The {{Development}} of {{Prosody}} in {{First Language Acquisition}} ({{Trends}} in {{Language Acquisition Research}})},
  author = {Prieto, P. and Esteve-Gibert, N.},
  date = {2018},
  volume = {23},
  publisher = {{John Benjamins}},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EB53FMQX\\ref=sr_1_1.html},
  series = {Trends in {{Language Acquisition Research}}}
}

@article{prieurOriginsGesturesLanguage2019,
  title = {The Origins of Gestures and Language: History, Current Advances and Proposed Theories},
  shorttitle = {The Origins of Gestures and Language},
  author = {Prieur, Jacques and Barbu, Stéphanie and Blois-Heulin, Catherine and Lemasson, Alban},
  date = {2019-12-18},
  journaltitle = {Biological Reviews of the Cambridge Philosophical Society},
  shortjournal = {Biol Rev Camb Philos Soc},
  issn = {1469-185X},
  doi = {10.1111/brv.12576},
  abstract = {Investigating in depth the mechanisms underlying human and non-human primate intentional communication systems (involving gestures, vocalisations, facial expressions and eye behaviours) can shed light on the evolutionary roots of language. Reports on non-human primates, particularly great apes, suggest that gestural communication would have been a crucial prerequisite for the emergence of language, mainly based on the evidence of large communication repertoires and their associated multifaceted nature of intentionality that are key properties of language. Such research fuels important debates on the origins of gestures and language. We review here three non-mutually exclusive processes that can explain mainly great apes' gestural acquisition and development: phylogenetic ritualisation, ontogenetic ritualisation, and learning via social negotiation. We hypothesise the following scenario for the evolutionary origins of gestures: gestures would have appeared gradually through evolution via signal ritualisation following the principle of derived activities, with the key involvement of emotional expression and processing. The increasing level of complexity of socioecological lifestyles and associated daily manipulative activities might then have enabled the acquisition and development of different interactional strategies throughout the life cycle. Many studies support a multimodal origin of language. However, we stress that the origins of language are not only multimodal, but more broadly multicausal. We propose a multicausal theory of language origins which better explains current findings. It postulates that primates' communicative signalling is a complex trait continually shaped by a cost-benefit trade-off of signal production and processing of interactants in relation to four closely interlinked categories of evolutionary and life cycle factors: species, individual and context-related characteristics as well as behaviour and its characteristics. We conclude by suggesting directions for future research to improve our understanding of the evolutionary roots of gestures and language.},
  eprint = {31854102},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7AZS82NH\\Prieur et al. - 2019 - The origins of gestures and language history, cur.pdf},
  keywords = {communication behaviours,emotional and intentional signalling,language evolution,multifactoriality,multimodality},
  langid = {english}
}

@article{prieurOriginsGesturesLanguage2019a,
  title = {The Origins of Gestures and Language: History, Current Advances and Proposed Theories},
  shorttitle = {The Origins of Gestures and Language},
  author = {Prieur, Jacques and Barbu, Stéphanie and Blois-Heulin, Catherine and Lemasson, Alban},
  date = {2019-12-18},
  journaltitle = {Biological Reviews of the Cambridge Philosophical Society},
  shortjournal = {Biol Rev Camb Philos Soc},
  issn = {1469-185X},
  doi = {10.1111/brv.12576},
  abstract = {Investigating in depth the mechanisms underlying human and non-human primate intentional communication systems (involving gestures, vocalisations, facial expressions and eye behaviours) can shed light on the evolutionary roots of language. Reports on non-human primates, particularly great apes, suggest that gestural communication would have been a crucial prerequisite for the emergence of language, mainly based on the evidence of large communication repertoires and their associated multifaceted nature of intentionality that are key properties of language. Such research fuels important debates on the origins of gestures and language. We review here three non-mutually exclusive processes that can explain mainly great apes' gestural acquisition and development: phylogenetic ritualisation, ontogenetic ritualisation, and learning via social negotiation. We hypothesise the following scenario for the evolutionary origins of gestures: gestures would have appeared gradually through evolution via signal ritualisation following the principle of derived activities, with the key involvement of emotional expression and processing. The increasing level of complexity of socioecological lifestyles and associated daily manipulative activities might then have enabled the acquisition and development of different interactional strategies throughout the life cycle. Many studies support a multimodal origin of language. However, we stress that the origins of language are not only multimodal, but more broadly multicausal. We propose a multicausal theory of language origins which better explains current findings. It postulates that primates' communicative signalling is a complex trait continually shaped by a cost-benefit trade-off of signal production and processing of interactants in relation to four closely interlinked categories of evolutionary and life cycle factors: species, individual and context-related characteristics as well as behaviour and its characteristics. We conclude by suggesting directions for future research to improve our understanding of the evolutionary roots of gestures and language.},
  eprint = {31854102},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XDV6ZT32\\Prieur et al. - 2019 - The origins of gestures and language history, cur.pdf},
  keywords = {communication behaviours,emotional and intentional signalling,language evolution,multifactoriality,multimodality},
  langid = {english}
}

@article{raineHumanRoarsCommunicate2019,
  title = {Human Roars Communicate Upper-Body Strength More Effectively than Do Screams or Aggressive and Distressed Speech},
  author = {Raine, Jordan and Pisanski, Katarzyna and Bond, Rod and Simner, Julia and Reby, David},
  date = {2019-03-04},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  pages = {e0213034},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0213034},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0213034},
  urldate = {2019-10-17},
  abstract = {Despite widespread evidence that nonverbal components of human speech (e.g., voice pitch) communicate information about physical attributes of vocalizers and that listeners can judge traits such as strength and body size from speech, few studies have examined the communicative functions of human nonverbal vocalizations (such as roars, screams, grunts and laughs). Critically, no previous study has yet to examine the acoustic correlates of strength in nonverbal vocalisations, including roars, nor identified reliable vocal cues to strength in human speech. In addition to being less acoustically constrained than articulated speech, agonistic nonverbal vocalizations function primarily to express motivation and emotion, such as threat, and may therefore communicate strength and body size more effectively than speech. Here, we investigated acoustic cues to strength and size in roars compared to screams and speech sentences produced in both aggressive and distress contexts. Using playback experiments, we then tested whether listeners can reliably infer a vocalizer’s actual strength and height from roars, screams, and valenced speech equivalents, and which acoustic features predicted listeners’ judgments. While there were no consistent acoustic cues to strength in any vocal stimuli, listeners accurately judged inter-individual differences in strength, and did so most effectively from aggressive voice stimuli (roars and aggressive speech). In addition, listeners more accurately judged strength from roars than from aggressive speech. In contrast, listeners’ judgments of height were most accurate for speech stimuli. These results support the prediction that vocalizers maximize impressions of physical strength in aggressive compared to distress contexts, and that inter-individual variation in strength may only be honestly communicated in vocalizations that function to communicate threat, particularly roars. Thus, in continuity with nonhuman mammals, the acoustic structure of human aggressive roars may have been selected to communicate, and to some extent exaggerate, functional cues to physical formidability.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MPXMFGRN\\Raine et al. - 2019 - Human roars communicate upper-body strength more e.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\BJ2YTZ3P\\article.html},
  keywords = {Acoustics,Bioacoustics,Hand strength,Physiological parameters,Speech,Speech signal processing,Verbal communication,Vocalization},
  langid = {english},
  number = {3}
}

@article{ramusCorrelatesLinguisticRhythm2000,
  title = {Correlates of Linguistic Rhythm in the Speech Signal},
  author = {Ramus, Franck and Nespor, Marina and Mehler, Jacques},
  date = {2000-04-14},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {75},
  pages = {AD3-AD30},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(00)00101-3},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027700001013},
  urldate = {2019-09-28},
  abstract = {Spoken languages have been classified by linguists according to their rhythmic properties, and psycholinguists have relied on this classification to account for infants’ capacity to discriminate languages. Although researchers have measured many speech signal properties, they have failed to identify reliable acoustic characteristics for language classes. This paper presents instrumental measurements based on a consonant/vowel segmentation for eight languages. The measurements suggest that intuitive rhythm types reflect specific phonological properties, which in turn are signaled by the acoustic/phonetic properties of speech. The data support the notion of rhythm classes and also allow the simulation of infant language discrimination, consistent with the hypothesis that newborns rely on a coarse segmentation of speech. A hypothesis is proposed regarding the role of rhythm perception in language acquisition.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PL9745Z6\\Ramus et al. - 2000 - Correlates of linguistic rhythm in the speech sign.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8NJPFQUW\\S0010027700001013.html},
  keywords = {Language acquisition,Language discrimination,Phonological bootstrapping,Prosody,Speech rhythm,Syllable structure},
  number = {1}
}

@article{ravignaniEvolutionSpeechRhythm2018,
  title = {Evolution of Speech Rhythm: A Cross-Species Perspective},
  shorttitle = {Evolution of Speech Rhythm},
  author = {Ravignani, Andrea and Dalla Bella, Simone and Falk, Simone and Kello, Chris and Noriega, Florencia and Kotz, Sonja},
  date = {2018},
  doi = {10.7287/peerj.preprints.27539v1},
  url = {https://peerj.com/preprints/27539},
  urldate = {2019-05-07},
  abstract = {Cognition and communication, at the core of human speech rhythm, do not leave a fossil record. However, if the purpose is to understand the origin and evolution of speech rhythm, alternative methods are available. A powerful tool is comparative approach: studying the presence or absence of cognitive/behavioral traits in other species, drawing conclusions on which traits are shared between species, and which are recent human inventions. Here we apply this approach to traits related to human speech rhythm. Many species exhibit temporal structure in their vocalizations but little is known about the range of rhythmic structures perceived and produced, their biological and developmental bases, and communicative functions. We review the literatures on human and non-human studies of rhythm in speech and animal vocalizations to survey similarities and differences. We report important links between vocal perception and motor coordination, and the differentiation of rhythm based on hierarchical temporal structure. We extend this review to quantitative techniques useful for computing rhythmic structure in acoustic sequences and hence facilitating cross-species research. While still far from a full comparative cross-species perspective of speech rhythm, we are closer to fitting missing pieces of the puzzle.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EYVX4IU5\\Ravignani et al. - Evolution of speech rhythm a cross-species perspe.pdf},
  langid = {english}
}

@article{ravignaniInteractiveRhythmsSpecies2019,
  title = {Interactive Rhythms across Species: The Evolutionary Biology of Animal Chorusing and Turn-Taking},
  shorttitle = {Interactive Rhythms across Species},
  author = {Ravignani, Andrea and Verga, Laura and Greenfield, Michael D.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  volume = {1453},
  pages = {12--21},
  issn = {1749-6632},
  doi = {10.1111/nyas.14230},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/abs/10.1111/nyas.14230},
  urldate = {2019-10-13},
  abstract = {The study of human language is progressively moving toward comparative and interactive frameworks, extending the concept of turn-taking to animal communication. While such an endeavor will help us understand the interactive origins of language, any theoretical account for cross-species turn-taking should consider three key points. First, animal turn-taking must incorporate biological studies on animal chorusing, namely how different species coordinate their signals over time. Second, while concepts employed in human communication and turn-taking, such as intentionality, are still debated in animal behavior, lower level mechanisms with clear neurobiological bases can explain much of animal interactive behavior. Third, social behavior, interactivity, and cooperation can be orthogonal, and the alternation of animal signals need not be cooperative. Considering turn-taking a subset of chorusing in the rhythmic dimension may avoid overinterpretation and enhance the comparability of future empirical work.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZITD6ZMI\\Ravignani et al. - 2019 - Interactive rhythms across species the evolutiona.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ID8J4AYB\\nyas.html},
  keywords = {bioacoustics,cooperation,interaction,language evolution,speech rhythm,synchrony},
  langid = {english},
  number = {1}
}

@article{ravignaniRhythmSpeechAnimal2019,
  title = {Rhythm in Speech and Animal Vocalizations: A Cross‐species Perspective},
  author = {Ravignani, A and Dalla Bella, S and Falk, S and Kello, C. T. and Noriega, F. and Kotz, S.},
  date = {2019},
  journaltitle = {Annals of the New York Academy of Sciences},
  doi = {10.1111/nyas.14166},
  url = {https://nyaspubs.onlinelibrary.wiley.com/doi/epdf/10.1111/nyas.14166},
  urldate = {2019-08-30},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VGFKQD2P\\nyas.html}
}

@article{ravivLargerCommunitiesCreate2019,
  title = {Larger Communities Create More Systematic Languages},
  author = {Raviv, Limor and Meyer, Antje and Lev-Ari, Shiri},
  date = {2019-07-24},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {286},
  pages = {20191262},
  doi = {10.1098/rspb.2019.1262},
  url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspb.2019.1262},
  urldate = {2019-12-11},
  abstract = {Understanding worldwide patterns of language diversity has long been a goal for evolutionary scientists, linguists and philosophers. Research over the past decade has suggested that linguistic diversity may result from differences in the social environments in which languages evolve. Specifically, recent work found that languages spoken in larger communities typically have more systematic grammatical structures. However, in the real world, community size is confounded with other social factors such as network structure and the number of second languages learners in the community, and it is often assumed that linguistic simplification is driven by these factors instead. Here, we show that in contrast to previous assumptions, community size has a unique and important influence on linguistic structure. We experimentally examine the live formation of new languages created in the laboratory by small and larger groups, and find that larger groups of interacting participants develop more systematic languages over time, and do so faster and more consistently than small groups. Small groups also vary more in their linguistic behaviours, suggesting that small communities are more vulnerable to drift. These results show that community size predicts patterns of language diversity, and suggest that an increase in community size might have contributed to language evolution.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QA5QVPTR\\rspb.2019.html},
  number = {1907}
}

@movie{rawlenceManWhoLost1997,
  title = {The {{Man Who Lost His Body}}},
  editor = {Rawlence, Chris},
  year = {1997, 16th October},
  publisher = {{BBC Horizon}},
  editortype = {director},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3RE6SZCW\\the-man-who-lost-his-body.html},
  langid = {english}
}

@book{redfordHandbookSpeechProduction2015,
  title = {The Handbook of Speech Production},
  author = {Redford, Melissa A.},
  date = {2015},
  publisher = {{Wiley Blackwell}},
  location = {{West Sussex}}
}

@article{rendallLiftingCurtainWizard2007,
  title = {Lifting the Curtain on the {{Wizard}} of {{Oz}}: Biased Voice-Based Impressions of Speaker Size},
  shorttitle = {Lifting the Curtain on the {{Wizard}} of {{Oz}}},
  author = {Rendall, Drew and Vokey, John R. and Nemeth, Christie},
  date = {2007-10},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {33},
  pages = {1208--1219},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.33.5.1208},
  abstract = {The consistent, but often wrong, impressions people form of the size of unseen speakers are not random but rather point to a consistent misattribution bias, one that the advertising, broadcasting, and entertainment industries also routinely exploit. The authors report 3 experiments examining the perceptual basis of this bias. The results indicate that, under controlled experimental conditions, listeners can make relative size distinctions between male speakers using reliable cues carried in voice formant frequencies (resonant frequencies, or timbre) but that this ability can be perturbed by discordant voice fundamental frequency (F-sub-0, or pitch) differences between speakers. The authors introduce 3 accounts for the perceptual pull that voice F-sub-0 can exert on our routine (mis)attributions of speaker size and consider the role that voice F-sub-0 plays in additional voice-based attributions that may or may not be reliable but that have clear size connotations.},
  eprint = {17924818},
  eprinttype = {pmid},
  keywords = {Adult,Amplifiers; Electronic,Cues,Female,Humans,Literature,Male,Speech Perception,Voice Quality},
  langid = {english},
  number = {5}
}

@article{reppSensorimotorSynchronizationReview2005,
  title = {Sensorimotor Synchronization: {{A}} Review of the Tapping Literature},
  shorttitle = {Sensorimotor Synchronization},
  author = {Repp, Bruno H.},
  date = {2005-12-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {12},
  pages = {969--992},
  issn = {1531-5320},
  doi = {10.3758/BF03206433},
  url = {https://doi.org/10.3758/BF03206433},
  urldate = {2019-04-02},
  abstract = {Sensorimotor synchronization (SMS), the rhythmic coordination of perception and action, occurs in many contexts, but most conspicuously in music performance and dance. In the laboratory, it is most often studied in the form of finger tapping to a sequence of auditory stimuli. This review summarizes theories and empirical findings obtained with the tapping task. Its eight sections deal with the role of intention, rate limits, the negative mean asynchrony, variability, models of error correction, perturbation studies, neural correlates of SMS, and SMS in musical contexts. The central theoretical issue is considered to be how best to characterize the perceptual information and the internal processes that enable people to achieve and maintain SMS. Recent research suggests that SMS is controlled jointly by two error correction processes (phase correction and period correction) that differ in their degrees of cognitive control and may be associated with different brain circuits. They exemplify the general distinction between subconscious mechanisms of action regulation and conscious processes involved in perceptual judgment and action planning.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EYLWBHB4\\Repp - 2005 - Sensorimotor synchronization A review of the tapp.pdf},
  keywords = {Auditory Sequence,Experimental Brain Research,Music Perception,Phase Correction,Target Tone},
  langid = {english},
  number = {6}
}

@online{RespirationWingBeatUltrasonic,
  title = {Respiration, {{Wing}}-{{Beat}} and {{Ultrasonic Pulse Emission}} in an {{Echo}}-{{Locating Bat}} | {{Journal}} of {{Experimental Biology}}},
  url = {https://jeb.biologists.org/content/56/1/37},
  urldate = {2020-01-23},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QSGQKIY8\\37.html}
}

@online{RespiratoryMuscleActivity,
  title = {Respiratory Muscle Activity in Relation to Vocalization in Flying Bats. | {{Journal}} of {{Experimental Biology}}},
  url = {https://jeb.biologists.org/content/198/1/175},
  urldate = {2020-01-23},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7PLEZBXS\\175.html}
}

@software{richardsonPolhemusApplicationsExample2009,
  title = {Polhemus Applications and Example Code},
  author = {Richardson, M.},
  date = {2009},
  url = {http://xkiwilabs.com/softwa re-toolboxes/}
}

@article{robertsSocialEcologicalComplexity,
  title = {Social and Ecological Complexity Is Associated with Gestural Repertoire Size of Wild Chimpanzees},
  author = {Roberts, Sam George Bradley and Roberts, Anna Ilona},
  journaltitle = {Integrative Zoology},
  volume = {n/a},
  issn = {1749-4877},
  doi = {10.1111/1749-4877.12423},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1749-4877.12423},
  urldate = {2019-11-30},
  abstract = {Increasing our understanding of primate gestural communication can provide new insights into language evolution. A key question in primate communication is the association between the social relationships of primates and their repertoire of gestures. Such analyses can reveal how primates use their repertoire of gestural communication to maintain their networks of family and friends, much as humans use language to maintain their social networks. In this study we examined the association between the repertoire of gestures (overall, manual and bodily gestures, gestures of different modalities) and social bonds (presence of reciprocated grooming), coordinated behaviours (travel, resting, co-feeding), and the complexity of ecology (e.g. noise, illumination) and sociality (party size, audience), in wild East African chimpanzees (Pan troglodytes schweinfurthii). A larger repertoire size of manual, visual gestures was associated with the presence of a relationship based on reciprocated grooming and increases in social complexity. A smaller repertoire of manual tactile gestures occurred when relationship was based on reciprocated grooming. A smaller repertoire of bodily gestures occurred between partners who jointly travelled for longer. Whereas gesture repertoire size was associated with social complexity, complex ecology also influenced repertoire size. The evolution of a large repertoire of manual, visual gestures may have been a key factor that enabled for larger social groups to emerge during evolution. Thus, the evolution of the larger brains in hominins may have co-occurred with an increase in the cognitive complexity underpinning gestural communication and this in turn may have enabled hominins to live in more complex social groups. This article is protected by copyright. All rights reserved.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ANUHUNCE\\1749-4877.html},
  keywords = {chimpanzee,ecology,gesture,repertoire size,social network analysis,sociality},
  langid = {english},
  number = {n/a}
}

@article{roch-levecqProductionBasicEmotions2006,
  title = {Production of Basic Emotions by Children with Congenital Blindness: {{Evidence}} for the Embodiment of Theory of Mind},
  shorttitle = {Production of Basic Emotions by Children with Congenital Blindness},
  author = {Roch‐Levecq, Anne-Catherine},
  date = {2006},
  journaltitle = {British Journal of Developmental Psychology},
  volume = {24},
  pages = {507--528},
  issn = {2044-835X},
  doi = {10.1348/026151005X50663},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/026151005X50663},
  urldate = {2019-11-22},
  abstract = {Children with congenital blindness are delayed in understanding other people's minds. The present study examined whether this delay was related to a more primitive form of inter-subjectivity by which infants draw correspondence between parental mirroring of the infant's display and proprioceptive sensations. Twenty children with congenital blindness and 20 typically-developing sighted children aged between 4 and 12 years were administered a series of tasks examining false belief and emotion understanding and production. The blind children scored lower on the false belief tasks and did not convey emotions facially to adult observers as accurately as sighted participants. The adults' ratings of the children's expressions were correlated with the children's scores on the false belief tasks. It is suggested that understanding people's minds might be anchored in primitive embodied forms of relatedness.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VMDL76VZ\\Roch‐Levecq - 2006 - Production of basic emotions by children with cong.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\HJA4M7WY\\026151005X50663.html},
  langid = {english},
  number = {3}
}

@article{rochet-capellanSpeechFocusPosition2008,
  title = {The Speech Focus Position Effect on Jaw–Finger Coordination in a Pointing Task},
  author = {Rochet-Capellan, A. and Laboissière, R. and Galván, A. and Jean-Luc, S.},
  date = {2008-12-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {51},
  pages = {1507--1521},
  doi = {10.1044/1092-4388(2008/07-0173)},
  url = {https://jslhr.pubs.asha.org/doi/full/10.1044/1092-4388%282008/07-0173%29},
  urldate = {2019-04-18},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\22HUYGHV\\Rochet-Capellan Amélie et al. - 2008 - The Speech Focus Position Effect on Jaw–Finger Coo.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\6CD9L55U\\07-0173).html},
  number = {6}
}

@article{rochet-capellanTakeBreathTake2014,
  title = {Take a Breath and Take the Turn: How Breathing Meets Turns in Spontaneous Dialogue},
  shorttitle = {Take a Breath and Take the Turn},
  author = {Rochet-Capellan, A. and Fuchs, S.},
  date = {2014-12-19},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  volume = {369},
  issn = {0962-8436},
  doi = {10.1098/rstb.2013.0399},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4240966/},
  urldate = {2019-11-16},
  abstract = {Physiological rhythms are sensitive to social interactions and could contribute to defining social rhythms. Nevertheless, our knowledge of the implications of breathing in conversational turn exchanges remains limited. In this paper, we addressed the idea that breathing may contribute to timing and coordination between dialogue partners. The relationships between turns and breathing were analysed in unconstrained face-to-face conversations involving female speakers. No overall relationship between breathing and turn-taking rates was observed, as breathing rate was specific to the subjects' activity in dialogue (listening versus taking the turn versus holding the turn). A general inter-personal coordination of breathing over the whole conversation was not evident. However, specific coordinative patterns were observed in shorter time-windows when participants engaged in taking turns. The type of turn-taking had an effect on the respective coordination in breathing. Most of the smooth and interrupted turns were taken just after an inhalation, with specific profiles of alignment to partner breathing. Unsuccessful attempts to take the turn were initiated late in the exhalation phase and with no clear inter-personal coordination. Finally, breathing profiles at turn-taking were different than those at turn-holding. The results support the idea that breathing is actively involved in turn-taking and turn-holding.},
  eprint = {25385777},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\GWEBTNTH\\Rochet-Capellan and Fuchs - 2014 - Take a breath and take the turn how breathing mee.pdf},
  number = {1658},
  pmcid = {PMC4240966}
}

@article{roschWaveletCompGuidedTour2014,
  title = {{{WaveletComp}} 1.1: {{A}} Guided Tour through the {{R}} Package},
  author = {Rosch, Angi and Schmidbauer, Harald},
  date = {2014},
  pages = {59},
  abstract = {WaveletComp is an R package for continuous wavelet-based analysis of univariate and bivariate time series. Wavelet functions are implemented in WaveletComp such that a wide range of intermediate and final results are easily accessible. The null hypothesis that there is no (joint) periodicity in the series is tested via p-values obtained from simulation, where the model to be simulated can be chosen from a wide variety of options. The reconstruction, and thus filtering, of a given series from its wavelet decomposition, subject to a range of possible constraints, is also possible. WaveletComp provides extended plotting functionality — which objects should be added to a plot (for example, the ridge of wavelet power, contour lines indicating significant periodicity, arrows indicating the leading/lagging series), which kind and degree of smoothing is desired in wavelet coherence plots, which color palette to use, how to define the layout of the time axis (using POSIXct conventions), and others. Technically, we have developed vector- and matrix-based implementations of algorithms to reduce computation time. Easy and intuitive handling was given high priority.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\FXBWTB7H\\Rosch and Schmidbauer - WaveletComp 1.1 A guided tour through the R packa.pdf},
  langid = {english}
}

@online{ruiterProductionGestureSpeech2000,
  title = {The Production of Gesture and Speech},
  author = {de Ruiter, Jan Peter},
  date = {2000-08},
  journaltitle = {Language and Gesture},
  doi = {10.1017/CBO9780511620850.018},
  url = {/core/books/language-and-gesture/production-of-gesture-and-speech/7703D35DC0D8F631AD0E7525AB363841},
  urldate = {2019-05-04},
  abstract = {{$<$}div class="abstract" data-abstract-type="normal"{$><$}p{$>$}Introduction{$<$}/p{$><$}p{$>$}Research topics in the field of speech-related gesture that have received considerable attention are the function of gesture, its synchronization with speech, and its semiotic properties. While the findings of these studies often have interesting implications for theories about the processing of gesture in the human brain, few studies have addressed this issue in the framework of information processing.{$<$}/p{$><$}p{$>$}In this chapter, I will present a general processing architecture for gesture production. It can be used as a starting point for investigating the processes and representations involved in gesture and speech. For convenience, I will use the term ‘model'when referring to ‘processing architecture’ throughout this chapter.{$<$}/p{$><$}p{$>$}Since the use of information-processing models is not believed by every gesture researcher to be an appropriate way of investigating gesture (see, e.g., McNeill 1992), I will first argue that information-processing models are essential theoretical tools for understanding the processing involved in gesture and speech. I will then proceed to formulate a new model for the production of gesture and speech, called the Sketch Model. It is an extension of Levelt's (1989) model for speech production. The modifications and additions to Levelt's model are discussed in detail. At the end of the section, the working of the Sketch Model is demonstrated, using a number of illustrative gesture/speech fragments as examples.{$<$}/p{$><$}p{$>$}Subsequently, I will compare the Sketch Model with both McNeill's (1992) growth-point theory and with the information-processing model by Krauss, Chen \& Gottesman (this volume). While the Sketch Model and the model by Krauss et al. are formulated within the same framework, they are based on fundamentally different assumptions.{$<$}/p{$><$}/div{$>$}},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\9FX5U9BF\\7703D35DC0D8F631AD0E7525AB363841.html},
  langid = {english}
}

@article{rusiewiczEffectsProsodyPosition2013,
  title = {Effects of Prosody and Position on the Timing of Deictic Gestures},
  author = {Rusiewicz, H. L. and Susan, S. and Iverson, J. and Szuminsky, N.},
  date = {2013-04-01},
  journaltitle = {Journal of Speech, Language, and Hearing Research},
  shortjournal = {Journal of Speech, Language, and Hearing Research},
  volume = {56},
  pages = {458--470},
  doi = {10.1044/1092-4388(2012/11-0283)},
  url = {https://jslhr.pubs.asha.org/doi/full/10.1044/1092-4388%282012/11-0283%29},
  urldate = {2019-04-18},
  abstract = {Purpose
      In this study, the authors investigated the hypothesis that the perceived tight temporal
         synchrony of speech and gesture is evidence of an integrated spoken language and manual
         gesture communication system. It was hypothesized that experimental manipulations
         of the spoken response would affect the timing of deictic gestures.
      
      
      Method
      The authors manipulated syllable position and contrastive stress in compound words
         in multiword utterances by using a repeated-measures design to investigate the degree
         of synchronization of speech and pointing gestures produced by 15 American English
         speakers. Acoustic measures were compared with the gesture movement recorded via capacitance.
      
      
      Results
      Although most participants began a gesture before the target word, the temporal parameters
         of the gesture changed as a function of syllable position and prosody. Syllables with
         contrastive stress in the 2nd position of compound words were the longest in duration
         and also most consistently affected the timing of gestures, as measured by several
         dependent measures.
      
      
      Conclusion
      Increasing the stress of a syllable significantly affected the timing of a corresponding
         gesture, notably for syllables in the 2nd position of words that would not typically
         be stressed. The findings highlight the need to consider the interaction of gestures
         and spoken language production from a motor-based perspective of coordination.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EUJRIGW6\\Rusiewicz Heather Leavy et al. - 2013 - Effects of Prosody and Position on the Timing of D.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ER26DFTE\\11-0283).html},
  number = {2}
}

@incollection{rusiewiczSetTime2018,
  title = {Set in Time},
  booktitle = {The {{Development}} of Prosody in First Language Acquisition},
  author = {Rusiewicz, H. L. and Esteve-Gibert, N.},
  date = {2018},
  pages = {103}
}

@article{samuelPerceptualLearningSpeech2009,
  title = {Perceptual Learning for Speech},
  author = {Samuel, Arthur G. and Kraljic, Tanya},
  date = {2009-08},
  journaltitle = {Attention, Perception, \& Psychophysics},
  volume = {71},
  pages = {1207--1218},
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/APP.71.6.1207},
  url = {http://www.springerlink.com/index/10.3758/APP.71.6.1207},
  urldate = {2019-05-07},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BJFJMQS5\\Samuel and Kraljic - 2009 - Perceptual learning for speech.pdf},
  langid = {english},
  number = {6}
}

@article{satoAllAspectsLearning2019,
  title = {Do All Aspects of Learning Benefit from Iconicity? {{Evidence}} from Motion Capture},
  shorttitle = {Do All Aspects of Learning Benefit from Iconicity?},
  author = {Sato, Asha and Schouwstra, Marieke and Flaherty, Molly and Kirby, Simon},
  date = {2019},
  journaltitle = {Language and Cognition},
  pages = {1--20},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2019.37},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/do-all-aspects-of-learning-benefit-from-iconicity-evidence-from-motion-capture/55EDF990ED0E81A85100F8F01988B7C2},
  urldate = {2019-11-05},
  abstract = {Recent work suggests that not all aspects of learning benefit from an iconicity advantage (Ortega, 2017). We present the results of an artificial sign language learning experiment testing the hypothesis that iconicity may help learners to learn mappings between forms and meanings, whilst having a negative impact on learning specific features of the form. We used a 3D camera (Microsoft Kinect) to capture participants’ gestures and quantify the accuracy with which they reproduce the target gestures in two conditions. In the iconic condition, participants were shown an artificial sign language consisting of congruent gesture–meaning pairs. In the arbitrary condition, the language consisted of non-congruent gesture–meaning pairs. We quantified the accuracy of participants’ gestures using dynamic time warping (Celebi et. al., 2013). Our results show that participants in the iconic condition learn mappings more successfully than participants in the arbitrary condition, but there is no difference in the accuracy with which participants reproduce the forms. While our work confirms that iconicity helps to establish form–meaning mappings, our study did not give conclusive evidence about the effect of iconicity on production; we suggest that iconicity may only have an impact on learning forms when these are complex.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Q6J7QLYR\\Sato et al. - Do all aspects of learning benefit from iconicity.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\4466NDHE\\55EDF990ED0E81A85100F8F01988B7C2.html},
  keywords = {Artificial Sign Language learning,iconicity,motion capture},
  langid = {english}
}

@article{sauterCrossculturalRecognitionBasic2010,
  title = {Cross-Cultural Recognition of Basic Emotions through Nonverbal Emotional Vocalizations},
  author = {Sauter, Disa A. and Eisner, Frank and Ekman, Paul and Scott, Sophie K.},
  date = {2010-02-09},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {107},
  pages = {2408--2412},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0908239106},
  url = {https://www.pnas.org/content/107/6/2408},
  urldate = {2019-10-22},
  abstract = {Emotional signals are crucial for sharing important information, with conspecifics, for example, to warn humans of danger. Humans use a range of different cues to communicate to others how they feel, including facial, vocal, and gestural signals. We examined the recognition of nonverbal emotional vocalizations, such as screams and laughs, across two dramatically different cultural groups. Western participants were compared to individuals from remote, culturally isolated Namibian villages. Vocalizations communicating the so-called “basic emotions” (anger, disgust, fear, joy, sadness, and surprise) were bidirectionally recognized. In contrast, a set of additional emotions was only recognized within, but not across, cultural boundaries. Our findings indicate that a number of primarily negative emotions have vocalizations that can be recognized across cultures, while most positive emotions are communicated with culture-specific signals.},
  eprint = {20133790},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\L8ZW7D4H\\Sauter et al. - 2010 - Cross-cultural recognition of basic emotions throu.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\6PBF8UUR\\2408.html},
  keywords = {affect,communication,universality,vocal signals},
  langid = {english},
  number = {6}
}

@article{schererVocalCommunicationEmotion2003,
  title = {Vocal Communication of Emotion: {{A}} Review of Research Paradigms},
  shorttitle = {Vocal Communication of Emotion},
  author = {Scherer, Klaus R},
  date = {2003-04-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {40},
  pages = {227--256},
  issn = {0167-6393},
  doi = {10.1016/S0167-6393(02)00084-5},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639302000845},
  urldate = {2019-10-22},
  abstract = {The current state of research on emotion effects on voice and speech is reviewed and issues for future research efforts are discussed. In particular, it is suggested to use the Brunswikian lens model as a base for research on the vocal communication of emotion. This approach allows one to model the complete process, including both encoding (expression), transmission, and decoding (impression) of vocal emotion communication. Special emphasis is placed on the conceptualization and operationalization of the major elements of the model (i.e., the speaker’s emotional state, the listener’s attribution, and the mediating acoustic cues). In addition, the advantages and disadvantages of research paradigms for the induction or observation of emotional expression in voice and speech and the experimental manipulation of vocal cues are discussed, using pertinent examples drawn from past and present research.
Zusammenfassung
Der Aufsatz gibt einen umfassenden Überblick über den Forschungsstand zum Thema der Beeinflussung von Stimme und Sprechweise durch Emotionen des Sprechers. Allgemein wird vorgeschlagen, die Forschung zur vokalen Kommunikation der Emotionen am Brunswik’schen Linsenmodell zu orientieren. Dieser Ansatz erlaubt den gesamten Kommunikationsprozess zu modellieren, von der Enkodierung (Ausdruck), über die Transmission (Übertragung), bis zur Dekodierung (Eindruck). Besondere Aufmerksamkeit gilt den Problemen der Konzeptualisierung und Operationalisierung der zentralen Elemente des Modells (z.B., dem Emotionszustand des Sprechers, den Inferenzprozessen des Hörers, und den zugrundeliegenden vokalen Hinweisreizen). Anhand ausgewählter Beispiele empirischer Untersuchungen werden die Vor- und Nachteile verschiedener Forschungsparadigmen zur Induktion und Beobachtung des emotionalen Stimmausdrucks sowie zur experimentellen Manipulation vokaler Hinweisreize diskutiert.
Résumé
L’état actuel de la recherche sur l’effet des émotions d’un locuteur sur la voix et la parole est décrit et des approches prometteuses pour le futur identifiées. En particulier, le modèle de perception de Brunswik (dit “de la lentille” est proposé) comme paradigme pour la recherche sur la communication vocale des émotions. Ce modèle permet la modélisation du processus complet, de l’encodage (expression) par la transmission au décodage (impression). La conceptualisation et l’opérationalization des éléments centraux du modèle (l’état émotionnel du locuteur, l’inférence de cet état par l’auditeur, et les indices auditifs) sont discuté en détail. De plus, en analysant des exemples de la recherche dans le domaine, les avantages et désavantages de différentes méthodes pour l’induction et l’observation de l’expression émotionnelle dans la voix et la parole et pour la manipulation expérimentale de différents indices vocaux sont évoqués.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XSGGSSSV\\Scherer - 2003 - Vocal communication of emotion A review of resear.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\8ZBAMLDD\\S0167639302000845.html},
  keywords = {Acoustic markers of emotion,Emotion induction,Emotion simulation,Evaluation of emotion effects on voice and speech,Expression of emotion,Perception/decoding,Speaker moods and attitudes,Speech technology,Stress effects on voice,Theories of emotion,Vocal communication},
  langid = {english},
  number = {1}
}

@article{schmidtBodilySynchronizationUnderlying2014,
  title = {Bodily Synchronization Underlying Joke Telling},
  author = {Schmidt, R. C. and Nie, Lin and Franco, Alison and Richardson, Michael J.},
  date = {2014},
  journaltitle = {Frontiers in Human Neuroscience},
  shortjournal = {Front. Hum. Neurosci.},
  volume = {8},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2014.00633},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2014.00633/full},
  urldate = {2019-05-07},
  abstract = {Advances in video and time series analysis have greatly enhanced our ability to study the bodily synchronization that occurs in natural interactions. Past research has demonstrated that the behavioral synchronization involved in social interactions is similar to dynamical synchronization found generically in nature. The present study investigated how the bodily synchronization in a joke telling task is spread across different nested temporal scales. Pairs of participants enacted knock-knock jokes and times series of their bodily activity were recorded. Coherence and relative phase analyses were used to evaluate the synchronization of bodily rhythms for the whole trial as well as at the subsidiary time scales of the whole joke, the setup of the punch line, the two-person exchange and the utterance. The analyses revealed greater than chance entrainment of the joke teller’s and joke responder’s movements at all time scales and that the relative phasing of the teller’s movements led those of the responder at the longer time scales. Moreover, this entrainment was greater when visual information about the partner’s movements was present but was decreased particularly at the shorter time scales when explicit gesturing in telling the joke was performed. In short, the results demonstrate that a complex interpersonal bodily “dance” occurs during structured conversation interactions and that this “dance” is constructed from a set of rhythms associated with the nested behavioral structure of the interaction.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\2WA6HV7G\\Schmidt et al. - 2014 - Bodily synchronization underlying joke telling.pdf},
  keywords = {Motor movements,sensorimotor synchronization,social coordination,social interaction,Spectral decomposition},
  langid = {english}
}

@article{sellAdaptationsHumansAssessing2010,
  title = {Adaptations in Humans for Assessing Physical Strength from the Voice},
  author = {Sell, Aaron and Bryant, Gregory A. and Cosmides, Leda and Tooby, John and Sznycer, Daniel and von Rueden, Christopher and Krauss, Andre and Gurven, Michael},
  date = {2010-11-22},
  journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
  shortjournal = {Proceedings of the Royal Society B: Biological Sciences},
  volume = {277},
  pages = {3509--3518},
  doi = {10.1098/rspb.2010.0769},
  url = {https://royalsocietypublishing.org/doi/full/10.1098/rspb.2010.0769},
  urldate = {2019-10-17},
  abstract = {Recent research has shown that humans, like many other animals, have a specialization for assessing fighting ability from visual cues. Because it is probable that the voice contains cues of strength and formidability that are not available visually, we predicted that selection has also equipped humans with the ability to estimate physical strength from the voice. We found that subjects accurately assessed upper-body strength in voices taken from eight samples across four distinct populations and language groups: the Tsimane of Bolivia, Andean herder-horticulturalists and United States and Romanian college students. Regardless of whether raters were told to assess height, weight, strength or fighting ability, they produced similar ratings that tracked upper-body strength independent of height and weight. Male voices were more accurately assessed than female voices, which is consistent with ethnographic data showing a greater tendency among males to engage in violent aggression. Raters extracted information about strength from the voice that was not supplied from visual cues, and were accurate with both familiar and unfamiliar languages. These results provide, to our knowledge, the first direct evidence that both men and women can accurately assess men's physical strength from the voice, and suggest that estimates of strength are used to assess fighting ability.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\K4KVXIR4\\Sell et al. - 2010 - Adaptations in humans for assessing physical stren.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ARPVBSQY\\rspb.2010.html},
  number = {1699},
  options = {useprefix=true}
}

@article{senghasChildrenCreatingCore2004,
  title = {Children Creating Core Properties of Language: {{Evidence}} from an Emerging Sign Language in Nicaragua},
  shorttitle = {Children {{Creating Core Properties}} of {{Language}}},
  author = {Senghas, Ann and Kita, Sotaro and Özyürek, Asli},
  date = {2004-09-17},
  journaltitle = {Science},
  volume = {305},
  pages = {1779--1782},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1100199},
  url = {https://science.sciencemag.org/content/305/5691/1779},
  urldate = {2019-11-30},
  abstract = {A new sign language has been created by deaf Nicaraguans over the past 25 years, providing an opportunity to observe the inception of universal hallmarks of language. We found that in their initial creation of the language, children analyzed complex events into basic elements and sequenced these elements into hierarchically structured expressions according to principles not observed in gestures accompanying speech in the surrounding language. Successive cohorts of learners extended this procedure, transforming Nicaraguan signing from its early gestural form into a linguistic system. We propose that this early segmentation and recombination reflect mechanisms with which children learn, and thereby perpetuate, language. Thus, children naturally possess learning abilities capable of giving language its fundamental structure.
A sign language developed by deaf children consists of discrete units similar to those of spoken language, perhaps reflecting the fundamental organization of the brain's language centers.
A sign language developed by deaf children consists of discrete units similar to those of spoken language, perhaps reflecting the fundamental organization of the brain's language centers.},
  eprint = {15375269},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\8ZZMFZSD\\Senghas et al. - 2004 - Children Creating Core Properties of Language Evi.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\9BBN28AM\\1779.html},
  langid = {english},
  number = {5691}
}

@online{SequenceMemoryConstraints,
  title = {Sequence {{Memory Constraints Give Rise}} to {{Language}}-{{Like Structure}} through {{Iterated Learning}}},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0168532},
  urldate = {2020-01-21},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\VIJ5DP3M\\article.html}
}

@article{sharkeyHandGesturesVisually2000,
  title = {Hand {{Gestures}} of {{Visually Impaired}} and {{Sighted Interactants}}},
  author = {Sharkey, William F. and Asamoto, Paula and Tokunaga, Christine and Haraguchi, Gail and McFaddon-Robar, Tammy},
  date = {2000-09-01},
  journaltitle = {Journal of Visual Impairment \& Blindness},
  shortjournal = {Journal of Visual Impairment \& Blindness},
  volume = {94},
  pages = {549--563},
  issn = {0145-482X},
  doi = {10.1177/0145482X0009400902},
  url = {https://doi.org/10.1177/0145482X0009400902},
  urldate = {2019-11-30},
  abstract = {This study investigated the types of gestures used, the frequency of the gestures, and the total time engaged in gestural communication by 11 visually impaired-sighted dyads; 12 sighted-sighted dyads; and 8 visually impaired-visually impaired dyads. Regardless of the type of dyad, the persons who were visually impaired used more adaptors and used gestures, emblems, and illustrators less often than did those who were sighted.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KGADQWEP\\Sharkey et al. - 2000 - Hand Gestures of Visually Impaired and Sighted Int.pdf},
  langid = {english},
  number = {9}
}

@article{sharkeyTurnTakingResources1990,
  title = {Turn‐taking Resources Employed by Congenitally Blind Conversers},
  author = {Sharkey, William F. and Stafford, Laura},
  date = {1990-06-01},
  journaltitle = {Communication Studies},
  volume = {41},
  pages = {161--182},
  issn = {1051-0974},
  doi = {10.1080/10510979009368299},
  url = {https://doi.org/10.1080/10510979009368299},
  urldate = {2019-11-30},
  abstract = {It has been suggested that blind persons lack appropriate communicative social skills. One aspect of social skills is the ability to regulate interaction smoothly. The study examined turn‐taking resources utilized by congenitally blind persons. Conversational Analysis was employed to discover the turn‐taking resources used by six congenitally blind individuals in three dyads (i.e., one male, one female and one mixed dyad). The results were compared with past research on turn‐taking resources utilized by sighted conversers. Overall, the participants utilized the majority of focal resources reported in research on sighted individuals. However, non‐vocal resources deviated from those found in previous research on sighted conversers. Specifically, tactile resources were not used; self‐adaptors, gestures and posture shifts were seldom used; mechanistic movements of the head and atypical use of facial orientation were discovered. Possible implications of these finding are discussed.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\G6XQJHDE\\10510979009368299.html},
  number = {2}
}

@inproceedings{shattuck-hufnagelDimensionalizingCospeechGestures2019,
  title = {Dimensionalizing Co-Speech Gestures},
  booktitle = {Proceedings of the {{International Congress}} of {{Phonetic Sciences}} 2019},
  author = {Shattuck-Hufnagel, S. and Prieto, P.},
  date = {2019},
  pages = {5},
  location = {{Melbourne, Australia}},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\IHR6ZBZX\\Shattuck-Hufnagel and Prieto - Dimensionalizing co-speech gestures.pdf},
  langid = {english}
}

@article{shattuck-hufnagelProsodicCharacteristicsNonreferential2018,
  title = {The Prosodic Characteristics of Non-Referential Co-Speech Gestures in a Sample of Academic-Lecture-Style Speech},
  author = {Shattuck-Hufnagel, S. and Ren, Ada},
  date = {2018},
  journaltitle = {Frontiers in Psychology},
  volume = {9},
  doi = {https://doi.org/10.3389/fpsyg.2018.01514},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HTSWA3AA\\fpsyg.2018.html},
  number = {1514}
}

@book{sheets-johnstonePrimacyMovement2011,
  title = {The {{Primacy}} of {{Movement}}},
  author = {Sheets-Johnstone, M.},
  date = {2011},
  publisher = {{John Benjamins}},
  location = {{Amsterdam}}
}

@article{shockleyArticulatoryConstraintsInterpersonal2007,
  title = {Articulatory Constraints on Interpersonal Postural Coordination},
  author = {Shockley, Kevin and Baker, Aimee A. and Richardson, Michael J. and Fowler, Carol A.},
  date = {2007},
  journaltitle = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  pages = {201--208},
  issn = {1939-1277(Electronic),0096-1523(Print)},
  doi = {10.1037/0096-1523.33.1.201},
  abstract = {Cooperative conversation has been shown to foster interpersonal postural coordination. The authors investigated whether such coordination is mediated by the influence of articulation on postural sway. In Experiment 1, talkers produced words in synchrony or in alternation, as the authors varied speaking rate and word similarity. Greater shared postural activity was found for the faster speaking rate. In Experiment 2, the authors demonstrated that shared postural activity also increases when individuals speak the same words or speak words that have similar stress patterns. However, this increase in shared postural activity is present only when participants' data are compared with those of their partner, who was present during the task, but not when compared with the data of a member of a different pair speaking the same word sequences as those of the original partner. The authors' findings suggest that interpersonal postural coordination observed during conversation is mediated by convergent speaking patterns. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KNU5IU2G\\2007-01135-014.html},
  keywords = {Articulation (Speech),Conversation,Motor Coordination,Posture},
  number = {1}
}

@article{shockleyMutualInterpersonalPostural2003,
  title = {Mutual Interpersonal Postural Constraints Are Involved in Cooperative Conversation},
  author = {Shockley, Kevin and Santana, Marie-Vee and Fowler, Carol A.},
  date = {2003-04},
  journaltitle = {Journal of Experimental Psychology. Human Perception and Performance},
  shortjournal = {J Exp Psychol Hum Percept Perform},
  volume = {29},
  pages = {326--332},
  issn = {0096-1523},
  doi = {10.1037/0096-1523.29.2.326},
  abstract = {The research was designed to evaluate interpersonal coordination during conversation with a new measurement tool. The experiment uses an analysis based on recurrence strategies, known as cross recurrence quantification, to evaluate the shared activity between 2 postural time series in reconstructed phase space. Pairs of participants were found to share more locations in phase space (greater recurrence) in conditions where they were conversing with one another to solve a puzzle task than in conditions in which they convened with others. The trajectories of pairs of participants also showed less divergence when they conversed with each other than when they conversed with others well. This is offered as objective evidence of interpersonal coordination of postural sway in the context of a cooperative verbal task.},
  eprint = {12760618},
  eprinttype = {pmid},
  keywords = {Adult,Analysis of Variance,Communication,Cooperative Behavior,Data Collection,Evaluation Studies as Topic,Humans,Interpersonal Relations,Kinesics,Posture,Problem Solving,Verbal Behavior},
  langid = {english},
  number = {2}
}

@software{sievertPlotlyCreateInteractive2019,
  title = {Plotly: {{Create Interactive Web Graphics}} via 'Plotly.Js'},
  shorttitle = {Plotly},
  author = {Sievert, Carson and Parmer, Chris and Hocking, Toby and Chamberlain, Scott and Ram, Karthik and Corvellec, Marianne and Despouy, Pedro and Inc, Plotly Technologies},
  date = {2019-04-10},
  url = {https://CRAN.R-project.org/package=plotly},
  urldate = {2019-04-23},
  abstract = {Create interactive web graphics from 'ggplot2' graphs and/or a custom interface to the (MIT-licensed) JavaScript library 'plotly.js' inspired by the grammar of graphics.},
  keywords = {WebTechnologies},
  version = {4.9.0}
}

@inproceedings{silvaEffectEndpointsDynamic2016,
  title = {On the {{Effect}} of {{Endpoints}} on {{Dynamic Time Warping}}},
  author = {Silva, D. F. and Batista, G. A. E. P. A. and Keogh, E.},
  date = {2016},
  pages = {10},
  location = {{San Francisco}},
  abstract = {While there exist a plethora of classification algorithms for most data types, there is an increasing acceptance that the unique properties of time series mean that the combination of nearest neighbor classifiers and Dynamic Time Warping (DTW) is very competitive across a host of domains, from medicine to astronomy to environmental sensors. While there has been significant progress in improving the efficiency and effectiveness of DTW in recent years, in this work we demonstrate that an underappreciated issue can significantly degrade the accuracy of DTW in real-world deployments. This issue has probably escaped the attention of the very active time series research community because of its reliance on static highly contrived benchmark datasets, rather than real world dynamic datasets where the problem tends to manifest itself. In essence, the issue is that DTW’s eponymous invariance to warping is only true for the main “body” of the two time series being compared. However, for the “head” and “tail” of the time series, the DTW algorithm affords no warping invariance. The effect of this is that tiny differences at the beginning or end of the time series (which may be either consequential or simply the result of poor “cropping”) will tend to contribute disproportionally to the estimated similarity, producing incorrect classifications. In this work, we show that this effect is real, and reduces the performance of the algorithm. We further show that we can fix the issue with a subtle redesign of the DTW algorithm, and that we can learn an appropriate setting for the extra parameter we introduced. We further demonstrate that our generalization is amiable to all the optimizations that make DTW tractable for large datasets.},
  eventtitle = {Proceedings of the {{22Nd ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  langid = {english}
}

@article{silvaSteadystateStressOne2007,
  title = {Steady-State Stress at One Hand Magnifies the Amplitude, Stiffness, and Non-Linearity of Oscillatory Behavior at the Other Hand},
  author = {Silva, P. and Moreno, M. and Mancini, M. and Fonseca, S. and Turvey, M. T.},
  date = {2007-12-11},
  journaltitle = {Neuroscience Letters},
  shortjournal = {Neuroscience Letters},
  volume = {429},
  pages = {64--68},
  issn = {0304-3940},
  doi = {10.1016/j.neulet.2007.09.066},
  url = {http://www.sciencedirect.com/science/article/pii/S0304394007010750},
  urldate = {2019-04-18},
  abstract = {Stress at one body segment can influence rhythmic movements of non-neighboring body segments. The nervous, circulatory, and fascia (connective tissue) systems are potential mediators of such remote effects. Assessing them begins with a detailed description of the remote effects. Precisely, how do the rhythmic movements change? In our experiment with seven participants, left-hand oscillations of held pendulums at self-selected frequencies were examined as a function of right-hand tonic forces of 0, 10 or 20\% of the maximum voluntary contraction. We evaluated the effect of the right hand's tonic force on the amplitude and frequency, and the stiffness and friction functions of the left hand's oscillations. Our results suggest that (a) amplitude and stiffness (both linear and non-linear) increased with tonic force but frequency and friction (both linear and non-linear) did not, and (b) the stiffness increases due to right hand 10 and 20\% stress were indifferent to the initial (0\%) left-hand stiffness values. Discussion took note of how the nervous system and architectural features of the body (e.g., its network of connective tissue) may produce such effects.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\24CQGZCP\\Silva et al. - 2007 - Steady-state stress at one hand magnifies the ampl.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\QKBQX6EV\\S0304394007010750.html},
  keywords = {Fascia,Remote effects,Rhythmic movements,Tonic force},
  number = {1}
}

@article{smithInteractionGlottalpulseRate2005,
  title = {The Interaction of Glottal-Pulse Rate and Vocal-Tract Length in Judgements of Speaker Size, Sex, and Age},
  author = {Smith, David R. R. and Patterson, Roy D.},
  date = {2005-11},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J. Acoust. Soc. Am.},
  volume = {118},
  pages = {3177--3186},
  issn = {0001-4966},
  doi = {10.1121/1.2047107},
  abstract = {Glottal-pulse rate (GPR) and vocal-tract length (VTL) are related to the size, sex, and age of the speaker but it is not clear how the two factors combine to influence our perception of speaker size, sex, and age. This paper describes experiments designed to measure the effect of the interaction of GPR and VTL upon judgements of speaker size, sex, and age. Vowels were scaled to represent people with a wide range of GPRs and VTLs, including many well beyond the normal range of the population, and listeners were asked to judge the size and sex/age of the speaker. The judgements of speaker size show that VTL has a strong influence upon perceived speaker size. The results for the sex and age categorization (man, woman, boy, or girl) show that, for vowels with GPR and VTL values in the normal range, judgements of speaker sex and age are influenced about equally by GPR and VTL. For vowels with abnormal combinations of low GPRs and short VTLs, the VTL information appears to decide the sex/age judgement.},
  eprint = {16334696},
  eprinttype = {pmid},
  keywords = {Acoustic Stimulation,Adult,Age Factors,Child,Female,Glottis,Humans,Male,Organ Size,Sex Characteristics,Speech Perception,Vocal Cords},
  langid = {english},
  number = {5},
  pmcid = {PMC2346770}
}

@article{stennekenSelfinducedReactiveTriggering2002,
  title = {Self-Induced versus Reactive Triggering of Synchronous Movements in a Deafferented Patient and Control Subjects},
  author = {Stenneken, P. and Aschersleben, G. and Cole, J. D. and Prinz, W.},
  date = {2002-02-01},
  journaltitle = {Psychological Research},
  shortjournal = {Psychological Research},
  volume = {66},
  pages = {40--49},
  issn = {1430-2772},
  doi = {10.1007/s004260100072},
  url = {https://doi.org/10.1007/s004260100072},
  urldate = {2019-04-02},
  abstract = {. The present study investigates the contribution of tactile-kinesthetic information to the timing of movements. The relative timing of simultaneous tapping movements of finger and foot (hand-foot asynchrony) was examined in a simple reaction time task and in discrete self-initiated taps (Experiment 1), and in externally triggered synchronization tapping (Experiment 2). We compared the performance of a deafferented participant (IW) to the performance of two control groups of different ages. The pattern of results in control groups replicates previous findings: Whereas positive hand-foot asynchronies (hand precedes foot) are observed in a simultaneous reaction to an auditory stimulus, hand-foot asynchronies are negative with discrete self-initiated as well as auditorily paced sequences of synchronized finger and foot taps. In the first case, results are explained by a simultaneous triggering of motor commands. In contrast, self-initiated and auditorily paced movements are assumed to be controlled in terms of their afferent consequences, as provided by tactile-kinesthetic information. The performance of the deafferented participant differed from that of healthy participants in some aspects. As expected on the basis of unaffected motor functions, the participant was able to generate finger and foot movements in reaction to an external signal. In spite of the lack of movement-contingent sensory feedback, the deafferented participant showed comparable timing errors in self-initiated and regularly paced tapping as observed in control participants. However, in discrete self-initiated taps IW's hand-foot asynchronies were considerably larger than in control participants, while performance did not differ from that of controls in continuous movement generation. These findings are discussed in terms of an internal generation of the movement's sensory consequences (forward-modeling).},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\MNW3S2WF\\Stenneken et al. - 2002 - Self-induced versus reactive triggering of synchro.pdf},
  keywords = {Control Participant,Motor Command,Reactive Trigger,Simple Reaction Time Task,Synchronization Task},
  langid = {english},
  number = {1}
}

@article{stennekenSelfinducedReactiveTriggering2002a,
  title = {Self-Induced versus Reactive Triggering of Synchronous Movements in a Deafferented Patient and Control Subjects},
  author = {Stenneken, P. and Aschersleben, G. and Cole, J. and Prinz, W.},
  date = {2002-02},
  journaltitle = {Psychological Research},
  volume = {66},
  pages = {40--49},
  issn = {0340-0727, 1430-2772},
  doi = {10.1007/s004260100072},
  url = {http://link.springer.com/10.1007/s004260100072},
  urldate = {2019-04-19},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\EAPIWRAU\\Stenneken et al. - 2002 - Self-induced versus reactive triggering of synchro.pdf},
  langid = {english},
  number = {1}
}

@book{stetsonMotorPhoneticsStudy1928,
  title = {Motor {{Phonetics}}: {{A Study}} of {{Speech Movements}} in {{Action}}},
  shorttitle = {Motor {{Phonetics}}},
  author = {Stetson, R. H.},
  date = {1928},
  publisher = {{Springer Netherlands}},
  url = {https://www.springer.com/gp/book/9789401521475},
  urldate = {2019-08-08},
  abstract = {Motor Phonetics...},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ZEVAZGJY\\9789401521475.html},
  isbn = {978-94-015-2147-5},
  langid = {english}
}

@article{stoltmannSyllablepointingGestureCoordination2017,
  title = {Syllable-Pointing Gesture Coordination in {{Polish}} Counting out Rhymes: {{The}} Effect of Speech Rate},
  author = {Stoltmann, K. and Fuchs, S.},
  date = {2017},
  journaltitle = {Journal of Multimodal Communication Studies},
  volume = {4},
  pages = {63--68},
  number = {1-2}
}

@article{streeckDepictingGesture2008,
  title = {Depicting by Gesture},
  author = {Streeck, Jürgen},
  date = {2008},
  journaltitle = {Gesture},
  volume = {8},
  pages = {285--301},
  issn = {1569-9773(Electronic),1568-1475(Print)},
  doi = {10.1075/gest.8.3.02str},
  abstract = {This paper deals with ways in which gestural "pictures" are made, i.e., manual depictions of phenomena in the world. The view that "iconic" gestures uniformly function by way of some resemblance between signifier and signified is rejected, giving way to an understanding of depiction by gesture as the achievement of a heterogeneous set of practices, some of which rely on relations of contiguity or indexicality to evoke commonly known objects or scenes. Others seem to be derivative of other representation methods (e.g., drawing on surfaces). The paper reviews some existing work on gestural depiction methods, offers a working heuristics, and illustrates some of its categories. It is suggested that some of the basic ways in which actions of the hands evoke the world in gesture correspond to fundamental modes of existence and activity of human hands in the world: hands depict by enacting their familiar, "real-world" capacities as users, transporters, experiencers, assemblers, molders, and shapers of things. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BVVVHGT5\\2009-01670-001.html},
  keywords = {Gestures,Hand (Anatomy),Heuristics,Iconic Memory},
  number = {3}
}

@article{sugiharaDetectingCausalityComplex2012,
  title = {Detecting {{Causality}} in {{Complex Ecosystems}}},
  author = {Sugihara, George and May, Robert and Ye, Hao and Hsieh, Chih-hao and Deyle, Ethan and Fogarty, Michael and Munch, Stephan},
  date = {2012-10-26},
  journaltitle = {Science},
  volume = {338},
  pages = {496--500},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1227079},
  url = {https://science.sciencemag.org/content/338/6106/496},
  urldate = {2020-01-16},
  abstract = {Identifying causal networks is important for effective policy and management recommendations on climate, epidemiology, financial regulation, and much else. We introduce a method, based on nonlinear state space reconstruction, that can distinguish causality from correlation. It extends to nonseparable weakly connected dynamic systems (cases not covered by the current Granger causality paradigm). The approach is illustrated both by simple models (where, in contrast to the real world, we know the underlying equations/relations and so can check the validity of our method) and by application to real ecological systems, including the controversial sardine-anchovy-temperature problem.
A new method, based on nonlinear state space reconstruction, can distinguish causality from correlation.
A new method, based on nonlinear state space reconstruction, can distinguish causality from correlation.},
  eprint = {22997134},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\XMNY2UJX\\Sugihara et al. - 2012 - Detecting Causality in Complex Ecosystems.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\CFDTXH86\\496.html},
  langid = {english},
  number = {6106}
}

@article{sundbergInfluenceBodyPosture1991,
  title = {Influence of Body Posture and Lung Volume on Subglottal Pressure Control during Singing},
  author = {Sundberg, J. and Leanderson, R. and von Euler, C. and Knutsson, E.},
  date = {1991-01-01},
  journaltitle = {Journal of Voice},
  shortjournal = {Journal of Voice},
  volume = {5},
  pages = {283--291},
  issn = {0892-1997},
  doi = {10.1016/S0892-1997(05)80057-8},
  url = {http://www.sciencedirect.com/science/article/pii/S0892199705800578},
  urldate = {2019-10-17},
  abstract = {The role of different breathing muscles during singing was investigated by synchronously recording EMG, pressure, and sound signals, using lung volume and gravity as experimental parameters. Surface EMG signals from the external and internal intercostals, the diaphragm, and the abdominal oblique muscles were recorded, while two singer subjects performed various singing tasks associated with rapid and precise changes of subglottal pressure. Esophageal and gastric pressures were measured by pressure transducers, and lung volume by means of impedance plethysmography. The results show that the breathing system efficiently compensates for drastic differences in the mechanics of the breathing apparatus, caused by differences in lung volume and gravity induced by changes of body posture.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\WAMA79AT\\S0892199705800578.html},
  keywords = {Abdominal oblique muscle,Body posture,Breathing,Diaphragm muscle,EMG,Intercostal muscles},
  langid = {english},
  number = {4},
  options = {useprefix=true}
}

@book{tomaselloOriginsHumanCommunication2008,
  title = {The Origins of Human Communication},
  author = {Tomasello, M.},
  date = {2008},
  publisher = {{MIT press}},
  location = {{Cambdride, MA}}
}

@article{tormeneMatchingIncompleteTime2009,
  title = {Matching Incomplete Time Series with Dynamic Time Warping: An Algorithm and an Application to Post-Stroke Rehabilitation},
  shorttitle = {Matching Incomplete Time Series with Dynamic Time Warping},
  author = {Tormene, Paolo and Giorgino, Toni and Quaglini, Silvana and Stefanelli, Mario},
  date = {2009-01},
  journaltitle = {Artificial Intelligence in Medicine},
  volume = {45},
  pages = {11--34},
  issn = {09333657},
  doi = {10.1016/j.artmed.2008.11.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0933365708001772},
  urldate = {2019-05-15},
  abstract = {Objective: The purpose of this study was to assess the performance of a real-time (‘‘open-end’’) version of the dynamic time warping (DTW) algorithm for the recognition of motor exercises. Given a possibly incomplete input stream of data and a reference time series, the open-end DTW algorithm computes both the size of the prefix of reference which is best matched by the input, and the dissimilarity between the matched portions. The algorithm was used to provide real-time feedback to neurological patients undergoing motor rehabilitation.
Methods and materials: We acquired a dataset of multivariate time series from a sensorized long-sleeve shirt which contains 29 strain sensors distributed on the upper limb. Seven typical rehabilitation exercises were recorded in several variations, both correctly and incorrectly executed, and at various speeds, totaling a data set of 840 time series. Nearest-neighbour classifiers were built according to the outputs of openend DTW alignments and their global counterparts on exercise pairs. The classifiers were also tested on well-known public datasets from heterogeneous domains.
Results: Nonparametric tests show that (1) on full time series the two algorithms achieve the same classification accuracy ( p-value ¼ 0:32); (2) on partial time series, classifiers based on open-end DTW have a far higher accuracy (k ¼ 0:898 versus k ¼ 0:447; p {$<$} 10À5); and (3) the prediction of the matched fraction follows closely the ground truth (root mean square {$<$} 10\%). The results hold for the motor rehabilitation and the other datasets tested, as well.
Conclusions: The open-end variant of the DTW algorithm is suitable for the classification of truncated quantitative time series, even in the presence of noise. Early recognition and accurate class prediction can be achieved, provided that enough},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\LNSWNC7R\\Tormene et al. - 2009 - Matching incomplete time series with dynamic time .pdf},
  langid = {english},
  number = {1}
}

@article{treffnerIntentionalAttentionalDynamics2002,
  title = {Intentional and Attentional Dynamics of Speech–Hand Coordination},
  author = {Treffner, P. and Peter, M.},
  date = {2002},
  journaltitle = {Human Movement Science},
  volume = {21},
  pages = {641--697},
  doi = {10.1016/S0167-9457(02)00178-1},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0167945702001781},
  urldate = {2019-08-08},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\M6EBIXEJ\\S0167945702001781.html},
  number = {5-6}
}

@article{turveyCoordination1990,
  title = {Coordination},
  author = {Turvey, M. T.},
  date = {1990},
  journaltitle = {American Psychologist},
  volume = {45},
  pages = {938--953},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.45.8.938},
  abstract = {The Russian physiologist N. Bernstein (1967) defined coordination as a problem of mastering the very many degrees of freedom involved in a particular movement—of reducing the number of independent variables to be controlled. The initial theorizing and experimentation on "Bernstein's problem" was conducted largely in terms of how a device of very many independent variables might be regulated without ascribing excessive responsibility to an executive subsystem. A second round of theory and research on Bernstein's problem is now under way. This second round is motivated by similarities between coordination and physical processes in which multiple components become collectively self-organized; it is directed at an explanation of coordination in terms of very general laws and principles. The major achievements of the first round of efforts to address Bernstein's problem are summarized, and six examples of the theory and research typifying the second round are presented. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\PXDZCKSF\\1990-30330-001.html},
  keywords = {Experimentation,Motor Coordination,Theories},
  number = {8}
}

@book{turveyLecturesPerceptionEcological2018,
  title = {Lectures on {{Perception}}: {{An Ecological Perspective}}},
  shorttitle = {Lectures on {{Perception}}},
  author = {Turvey, M. T.},
  date = {2018-11-07},
  edition = {1 edition},
  publisher = {{Routledge}},
  abstract = {Lectures on Perception: An Ecological Perspective addresses the generic principles by which each and every kind of life form―from single celled organisms (e.g., difflugia) to multi-celled organisms (e.g., primates)―perceives the circumstances of their living so that they can behave adaptively. It focuses on the fundamental ability that relates each and every organism to its surroundings, namely, the ability to perceive things in the sense of how to get about among them and what to do, or not to do, with them. The book’s core thesis breaks from the conventional interpretation of perception as a form of abduction based on innate hypotheses and acquired knowledge, and from the historical scientific focus on the perceptual abilities of animals, most especially those abilities ascribed to humankind. Specifically, it advances the thesis of perception as a matter of laws and principles at nature’s ecological scale, and gives equal theoretical consideration to the perceptual achievements of all of the classically defined ‘kingdoms’ of organisms―Archaea, Bacteria, Protoctista, Fungi, Plantae, and Animalia.},
  isbn = {978-1-138-33526-4},
  langid = {english},
  pagetotal = {446}
}

@article{turveyMediumHapticPerception2014,
  title = {The {{Medium}} of {{Haptic Perception}}: {{A Tensegrity Hypothesis}}},
  shorttitle = {The {{Medium}} of {{Haptic Perception}}},
  author = {Turvey, M. T. and Fonseca, S. T.},
  date = {2014-05},
  journaltitle = {Journal of Motor Behavior},
  volume = {46},
  pages = {143--187},
  issn = {0022-2895, 1940-1027},
  doi = {10.1080/00222895.2013.798252},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00222895.2013.798252},
  urldate = {2019-04-18},
  abstract = {For any given animal, the sources of mechanical disturbances inducing tissue deformation define environment from the perspective of the animal’s haptic perceptual system. The system’s achievements include perceiving the body, attachments to the body, and the surfaces and substances adjacent to the body. Among the perceptual systems, it stands alone in having no defined medium. There is no articulated functional equivalent to air and water, the media that make possible the energy transmissions and diffusions underpinning the other perceptual systems. To identify the haptic system’s medium the authors focus on connective tissue and the conjunction of muscular, connective tissue net, and skeletal (MCS) as the body’s proper characterization. The challenge is a biophysical formulation of MCS as a continuum that, similar to air and water, is homogeneous and isotropic. The authors hypothesized a multifractal tensegrity (MFT) with the shape and stability of the constituents of each scale, from individual cell to whole body, derivative of continuous tension and discontinuous compression. Each component tensegrity of MFT is an adjustive-receptive unit, and the array of tensions in MFT is information about MCS. The authors extend the MFT hypothesis to body-brain linkages, and to limb perception phenomena attendant to amputation, vibration, anesthesia, neuropathy, and microgravity.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\QSVVYB7C\\Turvey and Fonseca - 2014 - The Medium of Haptic Perception A Tensegrity Hypo.pdf},
  langid = {english},
  number = {3}
}

@article{valenteAdultsVisualRecognition2019,
  title = {Adults’ Visual Recognition of Actions Simulations by Finger Gestures ({{ASFGs}}) Produced by Sighted and Blind Individuals},
  author = {Valente, Dannyelle and Palama, Amaya and Malsert, Jennifer and Bolens, Guillemette and Gentaz, Edouard},
  date = {2019-03-28},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  pages = {e0214371},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0214371},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0214371},
  urldate = {2019-11-30},
  abstract = {The present study examines the visual recognition of action simulations by finger gestures (ASFGs) produced by sighted and blind individuals. In ASFGs, fingers simulate legs to represent actions such as jumping, spinning, climbing, etc. The question is to determine whether the common motor experience of one’s own body is sufficient to produce adequate ASFGs or whether the possibility to see gestures from others are also necessary to do it. Three experiments were carried out to address this question. Experiment 1 examined in 74 sighted adults the recognition of 18 types of ASFGs produced by 20 blindfolded sighted adults. Results showed that rates of correct recognition were globally very high, but varied with the type of ASFG. Experiment 2 studied in 91 other sighted adults the recognition of ASFGs produced by 10 early blind and 7 late blind adults. Results also showed a high level of recognition with a similar order of recognizability by type of ASFG. However, ASFGs produced by early blind individuals were more poorly recognized than those produced by late blind individuals. In order to match data of recognition obtained with the form that gestures are produced by individuals, two independant judges evaluated prototypical and atypical attributes of ASFG produced by blindfolded sighted, early blind and late blind individuals in Experiment 3. Results revealed the occurrence of more atypical attributes in ASFG produced by blind individuals: their ASFGs transpose more body movements from a character-viewpoint in less agreement with visual rules. The practical interest of the study relates to the relevance of including ASFGs as a new exploratory procedure in tactile devices which are more apt to convey action concepts to blind users/readers.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\W8QFSG9D\\Valente et al. - 2019 - Adults’ visual recognition of actions simulations .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\Q4LQTI9U\\article.html},
  keywords = {Adults,Blindness,Climbing,Congenital disorders,Fingers,Language,Surveys,Vision},
  langid = {english},
  number = {3}
}

@article{vervloedTeachingMeaningWords2014,
  title = {Teaching the {{Meaning}} of {{Words}} to {{Children}} with {{Visual Impairments}}},
  author = {Vervloed, Mathijs P. J. and Loijens, Nancy E. A. and Waller, Sarah E.},
  date = {2014-09-01},
  journaltitle = {Journal of Visual Impairment \& Blindness},
  shortjournal = {Journal of Visual Impairment \& Blindness},
  volume = {108},
  pages = {433--438},
  issn = {0145-482X},
  doi = {10.1177/0145482X1410800508},
  url = {https://doi.org/10.1177/0145482X1410800508},
  urldate = {2019-11-30},
  langid = {english},
  number = {5}
}

@article{wagnerExploitingSpeechgestureLinkInPress,
  title = {Exploiting the Speech-Gesture Link to Capture Fine-Grained Prosodic Prominence Impressions and Listening Strategies},
  author = {Wagner, Petra and Cwiek, A. and Samlowksi, B.},
  year = {In Press},
  journaltitle = {Journal of Phonetics},
  pages = {74},
  abstract = {In this paper, we explore the possibility to gather perceptual impressions of prosodic prominence by exploiting the strong prosody-gesture link, i.e., by having listeners transform a perceptual impression into a motor movement, namely drumming, for two domains of prominence: word-level and syllablelevel. A feasibility study reveals that such a procedure is indeed easily and speedily mastered by na¨ıve listeners, but more difficult for word-level prominences. We furthermore examine whether “drummed” annotations are comparable to those gathered with more established annotation protocols based on cumulative na¨ıve impressions and fine-grained expert ratings. These comparisons reveal high correspondences across all prominence annotation protocols, thus corroborating the general usefulness of the gestural approach. The analyses also reveal that all annotation protocols are strongly driven by structural linguistic considerations. We then use Random Forest Models to investigate the relative impact of signal and structural cues to prominence annotations. We find that expert ratings of prosodic prominence are guided comparatively more by structural concerns than those of na¨ıve annotators, that word-level annotations are influenced more by structural linguistic cues than syllable-level ones, and that “drummed” annotations are driven least by structural cues. Lastly, we isolate two main listener strategies among our group of “drummers”, namely those integrating structural and signal cues to prominence, and those being guided predominantly by signal cues.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\ETFMPUHE\\Wagner - Exploiting the speech-gesture link to capture ﬁne-.pdf},
  langid = {english}
}

@article{wagnerGestureSpeechInteraction2014,
  title = {Gesture and Speech in Interaction: {{An}} Overview},
  shorttitle = {Gesture and Speech in Interaction},
  author = {Wagner, Petra and Malisz, Zofia and Kopp, Stefan},
  date = {2014-02-01},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {57},
  pages = {209--232},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2013.09.008},
  url = {http://www.sciencedirect.com/science/article/pii/S0167639313001295},
  urldate = {2019-04-16},
  abstract = {Gestures and speech interact. They are linked in language production and perception, with their interaction contributing to felicitous communication. The multifaceted nature of these interactions has attracted considerable attention from the speech and gesture community. This article provides an overview of our current understanding of manual and head gesture form and function, of the principle functional interactions between gesture and speech aiding communication, transporting meaning and producing speech. Furthermore, we present an overview of research on temporal speech-gesture synchrony, including the special role of prosody in speech-gesture alignment. In addition, we provide a summary of tools and data available for gesture analysis, and describe speech-gesture interaction models and simulations in technical systems. This overview also serves as an introduction to a Special Issue covering a wide range of articles on these topics. We provide links to the Special Issue throughout this paper.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\I8QZH4YS\\S0167639313001295.html}
}

@article{wallotRecurrenceQuantificationAnalysis2017,
  title = {Recurrence {{Quantification Analysis}} of {{Processes}} and {{Products}} of {{Discourse}}: {{A Tutorial}} in {{R}}},
  shorttitle = {Recurrence {{Quantification Analysis}} of {{Processes}} and {{Products}} of {{Discourse}}},
  author = {Wallot, Sebastian},
  date = {2017-07-04},
  journaltitle = {Discourse Processes},
  volume = {54},
  pages = {382--405},
  issn = {0163-853X},
  doi = {10.1080/0163853X.2017.1297921},
  url = {https://doi.org/10.1080/0163853X.2017.1297921},
  urldate = {2019-09-20},
  abstract = {Processes of naturalistic reading and writing are based on complex linguistic input, stretch-out over time, and rely on an integrated performance of multiple perceptual, cognitive, and motor processes. Hence, naturalistic reading and writing performance is nonstationary and exhibits fluctuations and transitions. However, instead of being just complications for the analysis of such data, they are also informative about cognitive change, fluency, and reading or writing skill. To use and quantify such dynamics, one needs appropriate statistics that capture these aspects. In this article I introduce Recurrence Quantification Analysis (RQA) as a tool to capture such dynamic structure. After a conceptual introduction of the analysis, I present a step-by-step tutorial on how to run RQA using R. Guidance is given with regard to common issues and best practices using this time-series analysis technique. Finally, I review previous results from studies applying RQA to reading and writing and summarize current hypotheses and interpretations of RQA measures in the context of reading and writing.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\BBK6PIN3\\Wallot - 2017 - Recurrence Quantification Analysis of Processes an.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\ALHILH8H\\0163853X.2017.html},
  number = {5-6}
}

@book{webberRecurrenceQuantificationAnalysis2015,
  title = {Recurrence {{Quantification Analysis}}: {{Theory}} and {{Best Practices}}},
  author = {Webber, C. L. and Marwan, N.},
  date = {2015},
  publisher = {{Springer}},
  location = {{Cham}},
  url = {https://doi.org/10.1007/978-3-319-07155-8}
}

@software{wickhamGgplot2CreateElegant2019,
  title = {Ggplot2: {{Create Elegant Data Visualisations Using}} the {{Grammar}} of {{Graphics}}},
  shorttitle = {Ggplot2},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and RStudio},
  date = {2019-04-07},
  url = {https://CRAN.R-project.org/package=ggplot2},
  urldate = {2019-04-23},
  abstract = {A system for 'declaratively' creating graphics, based on "The Grammar of Graphics". You provide the data, tell 'ggplot2' how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.},
  keywords = {Graphics,Phylogenetics,TeachingStatistics},
  version = {3.1.1}
}

@article{wielingAnalyzingDynamicPhonetic2018,
  title = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling: {{A}} Tutorial Focusing on Articulatory Differences between {{L1}} and {{L2}} Speakers of {{English}}},
  shorttitle = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling},
  author = {Wieling, Martijn},
  date = {2018-09},
  journaltitle = {Journal of Phonetics},
  volume = {70},
  pages = {86--116},
  issn = {00954470},
  doi = {10.1016/j.wocn.2018.03.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0095447017301377},
  urldate = {2019-10-12},
  abstract = {In phonetics, many datasets are encountered which deal with dynamic data collected over time. Examples include diphthongal formant trajectories and articulator trajectories observed using electromagnetic articulography. Traditional approaches for analyzing this type of data generally aggregate data over a certain timespan, or only include measurements at a fixed time point (e.g., formant measurements at the midpoint of a vowel). In this paper, I discuss generalized additive modeling, a non-linear regression method which does not require aggregation or the pre-selection of a fixed time point. Instead, the method is able to identify general patterns over dynamically varying data, while simultaneously accounting for subject and item-related variability. An advantage of this approach is that patterns may be discovered which are hidden when data is aggregated or when a single time point is selected. A corresponding disadvantage is that these analyses are generally more time consuming and complex. This tutorial aims to overcome this disadvantage by providing a hands-on introduction to generalized additive modeling using articulatory trajectories from L1 and L2 speakers of English within the freely available R environment. All data and R code is made available to reproduce the analysis presented in this paper.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\3HBTJ6T5\\Wieling - 2018 - Analyzing dynamic phonetic data using generalized .pdf},
  langid = {english}
}

@article{wielingAnalyzingDynamicPhonetic2018a,
  title = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling: {{A}} Tutorial Focusing on Articulatory Differences between {{L1}} and {{L2}} Speakers of {{English}}},
  shorttitle = {Analyzing Dynamic Phonetic Data Using Generalized Additive Mixed Modeling},
  author = {Wieling, Martijn},
  date = {2018-09-01},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {70},
  pages = {86--116},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2018.03.002},
  url = {http://www.sciencedirect.com/science/article/pii/S0095447017301377},
  urldate = {2019-10-12},
  abstract = {In phonetics, many datasets are encountered which deal with dynamic data collected over time. Examples include diphthongal formant trajectories and articulator trajectories observed using electromagnetic articulography. Traditional approaches for analyzing this type of data generally aggregate data over a certain timespan, or only include measurements at a fixed time point (e.g., formant measurements at the midpoint of a vowel). This paper discusses generalized additive modeling, a non-linear regression method which does not require aggregation or the pre-selection of a fixed time point. Instead, the method is able to identify general patterns over dynamically varying data, while simultaneously accounting for subject and item-related variability. An advantage of this approach is that patterns may be discovered which are hidden when data is aggregated or when a single time point is selected. A corresponding disadvantage is that these analyses are generally more time consuming and complex. This tutorial aims to overcome this disadvantage by providing a hands-on introduction to generalized additive modeling using articulatory trajectories from L1 and L2 speakers of English within the freely available R environment. All data and R code is made available to reproduce the analysis presented in this paper.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\6ILUPIEI\\Wieling - 2018 - Analyzing dynamic phonetic data using generalized .pdf;C\:\\Users\\wimpo\\Zotero\\storage\\N67F9T36\\S0095447017301377.html},
  keywords = {Dynamic data,Electromagnetic articulography,Generalized additive modeling,Tutorial}
}

@article{wilsonRhythmicEntrainmentWhy2016,
  title = {Rhythmic Entrainment: {{Why}} Humans Want to, Fireflies Can't Help It, Pet Birds Try, and Sea Lions Have to Be Bribed},
  shorttitle = {Rhythmic Entrainment},
  author = {Wilson, Margaret and Cook, Peter F.},
  date = {2016-12},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {23},
  pages = {1647--1659},
  issn = {1531-5320},
  doi = {10.3758/s13423-016-1013-x},
  abstract = {Until recently, the literature on rhythmic ability took for granted that only humans are able to synchronize body movements to an external beat-to entrain. This assumption has been undercut by findings of beat-matching in various species of parrots and, more recently, in a sea lion, several species of primates, and possibly horses. This throws open the question of how widespread beat-matching ability is in the animal kingdom. Here we reassess the arguments and evidence for an absence of beat-matching in animals, and conclude that in fact no convincing case against beat-matching in animals has been made. Instead, such evidence as there is suggests that this capacity could be quite widespread. Furthermore, mutual entrainment of oscillations is a general principle of physical systems, both biological and nonbiological, suggesting that entrainment of motor systems by sensory systems may be a default rather than an oddity. The question then becomes, not why a few privileged species are able to beat-match, but why species do not always do so-why they vary in both spontaneous and learned beat-matching. We propose that when entrainment is not driven by fixed, mandatory connections between input and output (as in the case of, e.g., fireflies entraining to each others' flashes), it depends on voluntary control over, and voluntary or learned coupling of, sensory and motor systems, which can paradoxically lead to apparent failures of entrainment. Among the factors that affect whether an animal will entrain are sufficient control over the motor behavior to be entrained, sufficient perceptual sophistication to extract the entraining beat from the overall sensory environment, and the current cognitive state of the animal, including attention and motivation. The extent of entrainment in the animal kingdom potentially has widespread implications, not only for understanding the roots of human dance, but also for understanding the neural and cognitive architectures of animals.},
  eprint = {26920589},
  eprinttype = {pmid},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\DDMH7YR4\\Wilson and Cook - 2016 - Rhythmic entrainment Why humans want to, fireflie.pdf},
  keywords = {Animal cognition,Animals,Behavior; Animal,Entrainment,Humans,Motor Activity,Time Perception},
  langid = {english},
  number = {6}
}

@software{winkelmannWrasspInterfaceASSP2018,
  title = {Wrassp: {{Interface}} to the '{{ASSP}}' {{Library}}},
  shorttitle = {Wrassp},
  author = {Winkelmann, Raphael and Bombien, Lasse and Scheffers, Michel},
  date = {2018-08-31},
  url = {https://CRAN.R-project.org/package=wrassp},
  urldate = {2019-09-24},
  abstract = {A wrapper around Michel Scheffers's 'libassp' ({$<$}http://libassp.sourceforge.net/{$>$}). The 'libassp' (Advanced Speech Signal Processor) library aims at providing functionality for handling speech signal files in most common audio formats and for performing analyses common in phonetic science/speech science. This includes the calculation of formants, fundamental frequency, root mean square, auto correlation, a variety of spectral analyses, zero crossing rate, filtering etc. This wrapper provides R with a large subset of 'libassp's signal processing functions and provides them to the user in a (hopefully) user-friendly manner.},
  version = {0.1.8}
}

@article{wittenburgELANProfessionalFramework2006,
  title = {{{ELAN}}: A {{Professional Framework}} for {{Multimodality Research}}},
  author = {Wittenburg, Peter and Brugman, Hennie and Russel, Albert and Klassmann, Alex and Sloetjes, Han},
  date = {2006},
  pages = {4},
  abstract = {Utilization of computer tools in linguistic research has gained importance with the maturation of media frameworks for the handling of digital audio and video. The increased use of these tools in gesture, sign language and multimodal interaction studies has led to stronger requirements on the flexibility, the efficiency and in particular the time accuracy of annotation tools. This paper describes the efforts made to make ELAN a tool that meets these requirements, with special attention to the developments in the area of time accuracy. In subsequent sections an overview will be given of other enhancements in the latest versions of ELAN, that make it a useful tool in multimodality research.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\Q84I5QW9\\Wittenburg et al. - ELAN a Professional Framework for Multimodality R.pdf},
  langid = {english}
}

@book{woodGeneralizedAdditiveModels2017,
  title = {Generalized {{Additive Models}}: {{An Introduction}} with {{R}}, {{Second Edition}}},
  shorttitle = {Generalized {{Additive Models}}},
  author = {Wood, S. N.},
  date = {2017},
  publisher = {{Chapman and Hall/CRC}},
  url = {https://www.crcpress.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331},
  urldate = {2019-10-12},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\NZSPGFBD\\9781498728331.html},
  langid = {english}
}

@article{zelicArticulatoryConstraintsSpontaneous2015,
  title = {Articulatory Constraints on Spontaneous Entrainment between Speech and Manual Gesture},
  author = {Zelic, Gregory and Kim, Jeesun and Davis, Chris},
  date = {2015-08-01},
  journaltitle = {Human Movement Science},
  shortjournal = {Human Movement Science},
  volume = {42},
  pages = {232--245},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2015.05.009},
  url = {http://www.sciencedirect.com/science/article/pii/S0167945715000937},
  urldate = {2019-05-03},
  abstract = {The present study examined the extent to which speech and manual gestures spontaneously entrain in a non-communicative task. Participants had to repeatedly utter nonsense /CV/ syllables while continuously moving the right index finger in flexion/extension. No instructions to coordinate were given. We manipulated the type of syllable uttered (/ba/ vs. /sa/), and vocalization (phonated vs. silent speech). Assuming principles of coordination dynamics, a stronger entrainment between the fingers oscillations and the jaw motion was predicted (1) for /ba/, due to expected larger amplitude of jaw motion and (2) in phonated speech, due to the auditory feedback. Fifteen out of twenty participants showed simple ratios of speech to finger cycles (1:1, 1:2 or 2:1). In contrast with our predictions, speech–gesture entrainment was stronger when vocalizing /sa/ than /ba/, also more widely distributed on an in-phase mode. Furthermore, results revealed a spatial anchoring and an increased temporal variability in jaw motion when producing /sa/. We suggested that this indicates a greater control of the speech articulators for /sa/, making the speech performance more receptive to environmental forces, resulting in the greater entrainment observed to gesture oscillations. The speech–gesture coordination was maintained in silent speech, suggesting a somatosensory basis for their endogenous coupling.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\KVGKEBKU\\S0167945715000937.html},
  keywords = {Rhythmic coordination dynamics,Speech articulatory constraints,Spontaneous entrainment processes}
}

@article{zhangMoreWordsOnline2020,
  title = {More than Words: {{The}} Online Orchestration of Word Predictability, Prosody, Gesture, and Mouth Movements during Natural Language Comprehension},
  shorttitle = {More than Words},
  author = {Zhang, Ye and Frassinelli, Diego and Tuomainen, Jyrki and Skipper, Jeremy I and Vigliocco, Gabriella},
  date = {2020-01-09},
  journaltitle = {bioRxiv},
  doi = {10.1101/2020.01.08.896712},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.01.08.896712},
  urldate = {2020-01-13},
  abstract = {Communication naturally occurs in dynamic face-to-face environments where spoken words are embedded in linguistic discourse and accompanied by multimodal cues. Existing research supports predictive brain models where prior discourse but also prosody, mouth movements, and hand-gestures individually contribute to comprehension. In electroencephalography (EEG) studies, more predictable words show reduced negativity, peaking at approximately 400ms after onset of the word (N400). Meaningful gestures and prosody also reduce the N400, while the effects of rhythmic gestures and mouth movements remain unclear. However, these studies have only focused on individual cues while in the real world, communicative cues co-occur and potentially interact. We measured EEG elicited by words while participants watched videos of a speaker producing short naturalistic passages. For each word, we quantified the information carried by prior linguistic discourse (surprisal), prosody (mean pitch), mouth informativeness (lip-reading), and presence of meaningful and/or rhythmic gestures. Discourse predictability reduced the N400 amplitude and multimodal cues impacted and interacted with this effect. Specifically, higher pitch and meaningful gestures reduced N400 amplitude, beyond the effect of discourse, while rhythmic gestures increased the N400 amplitude. Moreover, higher pitch overall reduced N400 amplitude while informative mouth movements only showed effects when no gestures were present. These results can constrain existing predictive brain models in that they demonstrate that the brain uses cues selectively in a dynamic and contextually determined manner in real-world language comprehension.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\C44EXJ56\\Zhang et al. - 2020 - More than words The online orchestration of word .pdf},
  langid = {english}
}

@article{zhangVocalDevelopmentMorphological2018,
  title = {Vocal Development through Morphological Computation},
  author = {Zhang, Yisi S. and Ghazanfar, Asif A.},
  date = {2018-02-20},
  journaltitle = {PLOS Biology},
  shortjournal = {PLOS Biology},
  volume = {16},
  pages = {e2003933},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2003933},
  url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2003933},
  urldate = {2019-10-17},
  abstract = {The vocal behavior of infants changes dramatically during early life. Whether or not such a change results from the growth of the body during development—as opposed to solely neural changes—has rarely been investigated. In this study of vocal development in marmoset monkeys, we tested the putative causal relationship between bodily growth and vocal development. During the first two months of life, the spontaneous vocalizations of marmosets undergo (1) a gradual disappearance of context-inappropriate call types and (2) an elongation in the duration of context-appropriate contact calls. We hypothesized that both changes are the natural consequences of lung growth and do not require any changes at the neural level. To test this idea, we first present a central pattern generator model of marmoset vocal production to demonstrate that lung growth can affect the temporal and oscillatory dynamics of neural circuits via sensory feedback from the lungs. Lung growth qualitatively shifted vocal behavior in the direction observed in real marmoset monkey vocal development. We then empirically tested this hypothesis by placing the marmoset infants in a helium–oxygen (heliox) environment in which air is much lighter. This simulated a reversal in development by decreasing the effort required to respire, thus increasing the respiration rate (as though the lungs were smaller). The heliox manipulation increased the proportions of inappropriate call types and decreased the duration of contact calls, consistent with a brief reversal of vocal development. These results suggest that bodily growth alone can play a major role in shaping the development of vocal behavior.},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\4DFQJQJC\\Zhang and Ghazanfar - 2018 - Vocal development through morphological computatio.pdf;C\:\\Users\\wimpo\\Zotero\\storage\\X2PTYECE\\article.html},
  keywords = {Behavior,Lungs,Marmosets,Monkeys,Nervous system,Respiratory physiology,Syllables,Vocalization},
  langid = {english},
  number = {2}
}

@article{zhangVocalStateChange2019,
  title = {Vocal State Change through Laryngeal Development},
  author = {Zhang, Yisi S. and Takahashi, Daniel Y. and Liao, Diana A. and Ghazanfar, Asif A. and Elemans, Coen P. H.},
  date = {2019-12},
  journaltitle = {Nature Communications},
  volume = {10},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-12588-6},
  url = {http://www.nature.com/articles/s41467-019-12588-6},
  urldate = {2019-10-15},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\7X5E7J3J\\Zhang et al. - 2019 - Vocal state change through laryngeal development.pdf},
  langid = {english},
  number = {1}
}

@online{zotero-605,
  url = {https://scholar.googleusercontent.com/scholar.bib?q=info:O1Ol2n5NRkgJ:scholar.google.com/&output=citation&scisdr=CgXU6_56EO3YywLcpY8:AAGBfm0AAAAAXevZvY88goMvxdSn-QVq8kKk7wZ4cBis&scisig=AAGBfm0AAAAAXevZvSDNxFPA9egwSB-BSi4QvGv3BzXV&scisf=4&ct=citation&cd=-1&hl=en},
  urldate = {2019-12-07},
  file = {C\:\\Users\\wimpo\\Zotero\\storage\\HS2MKAST\\scholar.html}
}


@Manual{R-base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2019},
  url = {https://www.R-project.org/},
}
@Manual{R-dplyr,
  title = {dplyr: A Grammar of Data Manipulation},
  author = {Hadley Wickham and Romain François and Lionel Henry and Kirill Müller},
  year = {2019},
  note = {R package version 0.8.3},
  url = {https://CRAN.R-project.org/package=dplyr},
}
@Manual{R-foreach,
  title = {foreach: Provides Foreach Looping Construct},
  author = {{Microsoft} and Steve Weston},
  year = {2019},
  note = {R package version 1.4.7},
  url = {https://CRAN.R-project.org/package=foreach},
}
@Manual{R-gam,
  title = {gam: Generalized Additive Models},
  author = {Trevor Hastie},
  year = {2019},
  note = {R package version 1.16.1},
  url = {https://CRAN.R-project.org/package=gam},
}
@Manual{R-ggbeeswarm,
  title = {ggbeeswarm: Categorical Scatter (Violin Point) Plots},
  author = {Erik Clarke and Scott Sherrill-Mix},
  year = {2017},
  note = {R package version 0.6.0},
  url = {https://CRAN.R-project.org/package=ggbeeswarm},
}
@Book{R-ggplot2,
  author = {Hadley Wickham},
  title = {ggplot2: Elegant Graphics for Data Analysis},
  publisher = {Springer-Verlag New York},
  year = {2016},
  isbn = {978-3-319-24277-4},
  url = {https://ggplot2.tidyverse.org},
}
@Manual{R-gridExtra,
  title = {gridExtra: Miscellaneous Functions for "Grid" Graphics},
  author = {Baptiste Auguie},
  year = {2017},
  note = {R package version 2.3},
  url = {https://CRAN.R-project.org/package=gridExtra},
}
@Misc{R-itsadug,
  title = {{itsadug}: Interpreting Time Series and Autocorrelated Data Using GAMMs},
  author = {Jacolien {van Rij} and Martijn Wieling and R. Harald Baayen and Hedderik {van Rijn}},
  year = {2017},
  note = {R package version 2.3},
}
@Article{R-mgcv_a,
  title = {Fast stable restricted maximum likelihood and marginal 
likelihood estimation of semiparametric generalized linear models},
  journal = {Journal of the Royal Statistical Society (B)},
  volume = {73},
  number = {1},
  pages = {3-36},
  year = {2011},
  author = {S. N. Wood},
}
@Article{R-mgcv_b,
  title = {Smoothing parameter and model selection for general smooth models (with discussion)},
  author = {S.N. Wood and {N.} and {Pya} and B. S{"a}fken},
  journal = {Journal of the American Statistical Association},
  year = {2016},
  pages = {1548-1575},
  volume = {111},
}
@Article{R-mgcv_c,
  title = {Stable and efficient multiple smoothing parameter estimation for 
         generalized additive models},
  journal = {Journal of the American Statistical Association},
  volume = {99},
  number = {467},
  pages = {673-686},
  year = {2004},
  author = {S. N. Wood},
}
@Article{R-mgcv_d,
  title = {Thin-plate regression splines},
  journal = {Journal of the Royal Statistical Society (B)},
  volume = {65},
  number = {1},
  pages = {95-114},
  year = {2003},
  author = {S. N. Wood},
}
@Manual{R-nlme,
  title = {{nlme}: Linear and Nonlinear Mixed Effects Models},
  author = {Jose Pinheiro and Douglas Bates and Saikat DebRoy and Deepayan Sarkar and {R Core Team}},
  year = {2019},
  note = {R package version 3.1-140},
  url = {https://CRAN.R-project.org/package=nlme},
}
@Manual{R-papaja,
  author = {Frederik Aust and Marius Barth},
  title = {{papaja}: {Create} {APA} manuscripts with {R Markdown}},
  year = {2018},
  note = {R package version 0.1.0.9842},
  url = {https://github.com/crsh/papaja},
}
@Manual{R-plotfunctions,
  title = {plotfunctions: Various Functions to Facilitate Visualization of Data and
Analysis},
  author = {Jacolien {van Rij}},
  year = {2017},
  note = {R package version 1.3},
  url = {https://CRAN.R-project.org/package=plotfunctions},
}
@Manual{R-raster,
  title = {raster: Geographic Data Analysis and Modeling},
  author = {Robert J. Hijmans},
  year = {2019},
  note = {R package version 3.0-7},
  url = {https://CRAN.R-project.org/package=raster},
}
@Manual{R-scales,
  title = {scales: Scale Functions for Visualization},
  author = {Hadley Wickham and Dana Seidel},
  year = {2019},
  note = {R package version 1.1.0},
  url = {https://CRAN.R-project.org/package=scales},
}
@Article{R-sp,
  author = {Edzer J. Pebesma and Roger S. Bivand},
  title = {Classes and methods for spatial data in {R}},
  journal = {R News},
  year = {2005},
  volume = {5},
  number = {2},
  pages = {9--13},
  month = {November},
  url = {https://CRAN.R-project.org/doc/Rnews/},
}
